{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Useful Life ($\\textit{RUL}$) estimation of turbofan engines\n",
    "\n",
    "This notebook contains code for estimating the Remaining Useful Life ($\\textit{RUL}$) of turbofan engines of **new** C-MAPSS (Commercial Modular Aero-Propulsion System Simulation) dataset from NASA for aircraft engines. More details about the generation process can be found at https://www.mdpi.com/2306-5729/6/1/5. \n",
    "\n",
    "This **new** C-MAPSS dataset comprises run-to-failure data of multiple engine units located at different $\\textit{DS}$ sets. This notebook creates a model using data from all $DS$ sets.  This notebook reused the inception-based CNN network from (https://doi.org/10.36001/phmconf.2021.v13i1.3109) and its pre-processing precedures to estimate the $\\textit{RUL}$ of engines experiencing a determined flight class (FC). Particularly, this notebook uses sensor readings $X_s$ and operating conditions $w$ to estimate the $RUL$ which can follows a linear or piece-wise degradation. This second option could be activated by setting PIECE_WISE variable in True. Unfortunatly, the model needs to previously execute the following notebooks:\n",
    "\n",
    "- 1) Spliting a given dataset by Flight Class.ipynb\n",
    "- 2) Concatenating datasets by Flight Class.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import random\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from sklearn.utils import shuffle\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# EVALUATION modules\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "params = {'legend.fontsize': 8,\n",
    "          'figure.figsize': (9,6),\n",
    "         'axes.labelsize': 20,\n",
    "         'axes.titlesize':20,\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'axes.linewidth' : 2,\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "\n",
    "gpu=True\n",
    "if gpu == True:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      try:\n",
    "        tf.config.set_visible_devices(gpus[2], 'GPU')\n",
    "      except RuntimeError as e:\n",
    "            # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WINDOW_LEN = 30\n",
    "stride = 1\n",
    "FCs=3\n",
    "DOWNSAMPLING_STEP=[2,2,2]\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "PIECE_WISE=True\n",
    "#seed = 666\n",
    "#seed = 5\n",
    "#seed=229\n",
    "#seed=752\n",
    "#seed=965\n",
    "#seed=111\n",
    "#seed=777\n",
    "#seed=121\n",
    "#seed=921\n",
    "#seed=945\n",
    "#seed=742\n",
    "seed=692\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mape error\n",
    "def mape(real,predicted):\n",
    "    return np.mean(np.abs((real - predicted) / real)) * 100\n",
    "\n",
    "def mae(real,predicted):\n",
    "    return np.mean(np.abs(real - predicted))\n",
    "\n",
    "def rmse(real,predicted):\n",
    "    return np.sqrt(np.mean((real - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x, lb, ub, max_v=1.0, min_v=-1.0):\n",
    "\n",
    "    # Set-up\n",
    "    if (lb == []) & (ub == []):\n",
    "        # OPTION 1:\n",
    "        ub = x.max(0)\n",
    "        lb = x.min(0)\n",
    "    \n",
    "        # OPTION 2:\n",
    "        #ub = np.percentile(x, 99.9, axis=0, keepdims=True)\n",
    "        #lb = np.percentile(x, 0.1, axis=0, keepdims=True)\n",
    "    \n",
    "    ub.shape = (1,-1)\n",
    "    lb.shape = (1,-1)           \n",
    "    max_min = max_v - min_v\n",
    "    delta = ub-lb\n",
    "\n",
    "    # Compute\n",
    "    x_n = max_min * (x - lb) / delta + min_v\n",
    "    if 0 in delta:\n",
    "        idx = np.ravel(delta == 0)\n",
    "        x_n[:,idx] = x[:,idx] - lb[:, idx]\n",
    "\n",
    "    return x_n, lb, ub \n",
    "\n",
    "\n",
    "def split_dataset(dataset, split=4): \n",
    "    '''\n",
    "    Split 'dataset' in tree pieces: w, x_s, theta\n",
    "    '''\n",
    "    w = dataset[:,:split]\n",
    "    x_s = dataset[:,split:20]\n",
    "    theta = dataset[:,-10:]\n",
    "    tmp = theta[:,6]\n",
    "    tmp.shape = (-1,1)\n",
    "    \n",
    "    return w, x_s, tmp\n",
    "\n",
    "def extract_units_ds(id_en, ds, units):\n",
    "    '''\n",
    "    Creates a subset with only id_en units for ds\n",
    "    '''\n",
    "    \n",
    "    # Set-up\n",
    "    ds_sub = []\n",
    "    units_unique = np.unique(units)\n",
    "\n",
    "    # Process\n",
    "    for i in units_unique:\n",
    "        if i in id_en:\n",
    "            idx = np.ravel(units==i)\n",
    "            ds_sub.append(ds[idx,:])           \n",
    "    \n",
    "    return np.concatenate(ds_sub, axis=0)\n",
    "\n",
    "def create_lag_data(w, x_s, theta, y, Units, Cycles, stride=1):\n",
    "    # Set-up\n",
    "    W, X, T, Y, U, C, _W, _X = [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # Loop over units and then within the units\n",
    "    units = np.unique(Units)\n",
    "    for k in units:\n",
    "        unit = np.ravel(Units == k)\n",
    "        w_unit = w[unit,:]\n",
    "        x_s_unit = x_s[unit,:]\n",
    "        T_unit = theta[unit,:]\n",
    "        Y_unit = y[unit,:]\n",
    "        U_unit = Units[unit,:]\n",
    "        C_unit = Cycles[unit,:]\n",
    "        dim = w_unit.shape[0]\n",
    "        for i in range(dim-1): \n",
    "            X.append(x_s_unit[i + stride, :])          # X  or X_\n",
    "            W.append(w_unit[i + stride, :])            # W  or W_\n",
    "            T.append(T_unit[i + stride, :])            # T  or T_\n",
    "            Y.append(Y_unit[i + stride, :])            # T  or T_\n",
    "            U.append(U_unit[i + stride, :])            # U  or U_\n",
    "            C.append(C_unit[i + stride, :])            # C  or C_\n",
    "            _X.append(x_s_unit[i, :])                  # _X or X\n",
    "            _W.append(w_unit[i, :])                  # _X or X\n",
    "            \n",
    "    return np.array(W), np.array(X), np.array(T), np.array(Y), np.array(U), np.array(C), np.array(_X), np.array(_W)\n",
    "\n",
    "\n",
    "\n",
    "# np.array(X_out), np.array(W_out), np.array(Y_out), np.array(T_out), np.array(U_out), np.array(C_out), np.array(HI_out)\n",
    "\n",
    "def split_sequences(input_data, sequence_length, stride = 1, option = None):\n",
    "    \"\"\"\n",
    "     \n",
    "    \"\"\"\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(0,len(input_data),stride):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + sequence_length\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(input_data):\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        if option=='last':\n",
    "            seq_x = input_data[end_ix-1, :]\n",
    "        elif option=='next':\n",
    "            seq_x = input_data[end_ix, :]\n",
    "        else:\n",
    "            seq_x = input_data[i:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "# def sequence_generator(input_data, units, sequence_length=10, option=None):\n",
    "#     \"\"\"\n",
    "#      # Generates dataset with windows of sequence_length      \n",
    "#     \"\"\"  \n",
    "#     X = list()\n",
    "#     unit_num=[]\n",
    "#     for i, elem_u in enumerate(list(np.unique(units))):\n",
    "#         mask = np.ravel(units==elem_u)\n",
    "#         seq_x_u = split_sequences(input_data[mask],sequence_length, option)\n",
    "#         X.append(seq_x_u)\n",
    "#         unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "    \n",
    "#     return np.vstack(X),unit_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensor_signal(x_real,x_pred,var_names,num=1000,figsize=10):\n",
    "    input_dim = len(var_names)\n",
    "    cols = min(np.floor(input_dim**0.5).astype(int),4)\n",
    "    rows = (np.ceil(input_dim / cols)).astype(int)\n",
    "    gs   = gridspec.GridSpec(rows, cols)\n",
    "    fig  = plt.figure(figsize=(figsize, max(figsize, rows*2)))   \n",
    "    for i in range(input_dim):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        # ax.plot(np.arange(0,num),x_real[-num:,i],label=\"True\",marker='.',markeredgewidth=0.25, markersize=8)\n",
    "        # ax.plot(np.arange(0,num),x_pred[-num:,i],label=\"Pred\",marker='.',markeredgewidth=0.25, markersize=8)\n",
    "        ax.scatter(np.arange(0,num),x_real[-num:,i],label=\"True\",marker='.')\n",
    "        ax.scatter(np.arange(0,num),x_pred[-num:,i],label=\"Pred\",marker='.')\n",
    "        plt.title(var_names[i])\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predicted_true_rul(log_y_hat_test, unit_sel, Unit_test, C_test, rul_test):\n",
    "    \"\"\"\n",
    "    Plots the predicted and true remaining useful life (RUL) for a given set of test data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_y_hat_test : numpy.ndarray\n",
    "        Logarithm of the predicted RUL for each cycle of each unit in the test data.\n",
    "    unit_sel : list\n",
    "        List of units to include in the plot.\n",
    "    Unit_test : numpy.ndarray\n",
    "        Array containing the unit numbers for each cycle in the test data.\n",
    "    C_test : numpy.ndarray\n",
    "        Array containing the cycle numbers for each cycle in the test data.\n",
    "    rul_test : numpy.ndarray\n",
    "        Array containing the true RUL for each cycle in the test data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i in range(len(log_y_hat_test)):\n",
    "        fig = plt.figure(figsize=(9, 7))\n",
    "        leg = []\n",
    "        \n",
    "        # Plot predicted RUL\n",
    "        for j in unit_sel:\n",
    "            y_hat_mean, y_hat_max, y_hat_min = [], [], []\n",
    "            unit = Unit_test == j\n",
    "            c_test = np.sort(C_test[unit])-1\n",
    "            idx = np.argsort(C_test[unit])\n",
    "            y_hat_test = log_y_hat_test[i][unit]\n",
    "            y_hat_test_sorted = y_hat_test[idx]\n",
    "            for k in np.unique(c_test):\n",
    "                y_hat_mean.append(np.mean(y_hat_test_sorted[c_test == k]))\n",
    "                y_hat_max.append(np.max(y_hat_test_sorted[c_test == k]))\n",
    "                y_hat_min.append(np.min(y_hat_test_sorted[c_test == k]))\n",
    "            y_hat_mean = np.array(y_hat_mean, dtype=np.float64)\n",
    "            y_hat_max = np.array(y_hat_max, dtype=np.float64)\n",
    "            y_hat_min = np.array(y_hat_min, dtype=np.float64)\n",
    "            plt.plot(np.unique(c_test), y_hat_mean, 'o', alpha=0.7, markersize=5)\n",
    "            plt.fill_between(np.unique(c_test), y_hat_min, y_hat_max, alpha=0.3)\n",
    "        # Plot true RUL\n",
    "        plt.gca().set_prop_cycle(None)\n",
    "        for j in unit_sel:        \n",
    "            unit = Unit_test == j  \n",
    "            c_test_unique = np.unique(np.sort(C_test[unit])-1)\n",
    "            rul_test_unique = np.unique(rul_test[unit])\n",
    "            rul_test_unique=np.append(rul_test_unique,np.zeros(len(c_test_unique)-len(rul_test_unique))+np.max(rul_test[unit]))\n",
    "            plt.plot(c_test_unique, rul_test_unique[::-1], alpha=0.7)\n",
    "            leg.append('Prediction-Traj.' + str(j))           \n",
    "            leg.append('$RUL$-Traj.' + str(j))\n",
    "        plt.legend(leg, loc='upper right')\n",
    "        plt.ylabel(r'$Prediction$ & $RUL$ [cycles]')\n",
    "        plt.xlabel('Time [cycles]')\n",
    "        plt.ylim(top=90)\n",
    "        plt.ylim(bottom=-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sequence Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generator(input_data, units, cycles, sequence_length=10,stride = 1, option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        c_mask = cycles[mask]\n",
    "        x_unit = input_data[mask]\n",
    "        for j in np.unique(c_mask):\n",
    "            mask = np.ravel(c_mask==j)\n",
    "            seq_x_u = split_sequences(x_unit[mask],sequence_length, stride, option)\n",
    "            X.append(seq_x_u)\n",
    "            unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "            c_num.extend(np.ones(len(seq_x_u),dtype = int)*j)\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.array(c_num).reshape(-1,1)\n",
    "\n",
    "\n",
    "def sequence_generator_per_unit(input_data, units, cycles, sequence_length=10, stride =1,option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        x_unit = input_data[mask]\n",
    "        seq_x_u = split_sequences(x_unit,sequence_length, stride, option)\n",
    "        X.append(seq_x_u)\n",
    "        unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "        c_num.append(split_sequences(cycles[mask],sequence_length, stride, option))\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.vstack(c_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(xs,w,y,t,units,cycles,hi,freq,mean = False):\n",
    "    \n",
    "    X_out,W_out,Y_out,T_out,U_out,C_out,HI_out= [], [], [], [], [], [] , []\n",
    "    \n",
    "    # Number of Trajectories (Units)\n",
    "    uniq_units=np.unique(units)\n",
    "    \n",
    "    for k in uniq_units:\n",
    "        unit = np.ravel(units == k)\n",
    "        u_unit=units[unit,:]\n",
    "        w_unit = w[unit,:]\n",
    "        x_s_unit = xs[unit,:]\n",
    "        y_unit= y[unit,:]\n",
    "        t_unit= t[unit,:]\n",
    "        c_unit= cycles[unit,:]\n",
    "        h_unit = hi[unit]\n",
    "        \n",
    "        if mean:\n",
    "            for i in range(0,x_s_unit.shape[0],freq):\n",
    "                X_out.append(np.mean(x_s_unit[i:i+freq,:],axis=0))\n",
    "                W_out.append(np.mean(w_unit[i:i+freq,:],axis=0))\n",
    "                Y_out.append(np.mean(y_unit[i:i+freq,:],axis=0))\n",
    "                T_out.append(np.mean(t_unit[i:i+freq,:],axis=0))\n",
    "                HI_out.append(np.mean(h_unit[i:i+freq,:],axis=0))\n",
    "            \n",
    "            C_out.extend(c_unit[::freq])\n",
    "            U_out.extend(u_unit[::freq])\n",
    "        else:\n",
    "        \n",
    "            X_out.extend(x_s_unit[::freq,])\n",
    "            W_out.extend(w_unit[::freq,])\n",
    "            Y_out.extend(y_unit[::freq])\n",
    "            U_out.extend(u_unit[::freq])\n",
    "            T_out.extend(t_unit[::freq])\n",
    "            C_out.extend(c_unit[::freq])\n",
    "            HI_out.extend(h_unit[::freq])\n",
    "            \n",
    "    return  np.array(X_out).astype(np.float16),\\\n",
    "            np.array(W_out).astype(np.float16),\\\n",
    "            np.array(Y_out).astype(np.float16),\\\n",
    "            np.array(T_out).astype(np.float16),\\\n",
    "            np.array(U_out).astype(np.float16),\\\n",
    "            np.array(C_out).astype(np.float16),\\\n",
    "            np.array(HI_out).astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_healthy(data,units,cycles,cycle_len,sample_len=1):\n",
    "    X,U,C=[],[],[]\n",
    "    uniq_units=np.unique(units)\n",
    "    for i in uniq_units:\n",
    "        idx=np.ravel(units==i)\n",
    "        cy=cycles[idx]\n",
    "        x=data[idx,:]\n",
    "        u=units[idx]\n",
    "        cyidx=np.ravel(cy<=cycle_len)\n",
    "        x=x[cyidx,:]\n",
    "        u=u[cyidx]\n",
    "        cyy = cy[cyidx]\n",
    "        X.extend(x[::sample_len,:])\n",
    "        U.extend(u[::sample_len])\n",
    "        C.extend(cyy[::sample_len])\n",
    "        \n",
    "    return np.array(X), np.array(U), np.array(C)\n",
    "\n",
    "def sample_healthy_mixed(data,units,cycles,starts,cycle_len,full=False):\n",
    "    X,U,C=[],[],[]\n",
    "    uniq_units=np.unique(units)\n",
    "    for i in range(len(uniq_units)):\n",
    "        idx=np.ravel(units==uniq_units[i])\n",
    "        cy=cycles[idx]\n",
    "        x=data[idx,:]\n",
    "        u=units[idx]\n",
    "        if full:\n",
    "            cyidx=np.ravel((cy>=starts[i]))\n",
    "        else:\n",
    "            cyidx=np.ravel((cy>=starts[i])&(cy<=cycle_len+starts[i]))\n",
    "        x=x[cyidx,:]\n",
    "        u=u[cyidx]\n",
    "        cyy = cy[cyidx]\n",
    "        X.extend(x)\n",
    "        U.extend(u)\n",
    "        C.extend(cyy)\n",
    "        \n",
    "    return np.array(X), np.array(U), np.array(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def smooth_z_cycle(HI,T,units):\n",
    "    results = []\n",
    "    stds = []\n",
    "    cycle_num =[]\n",
    "    unit_num =[]\n",
    "    for j in np.unique(units):\n",
    "        idx = np.ravel(units==j)\n",
    "        HI_unit = HI[idx]\n",
    "        T_unit = T[idx].astype('float64')\n",
    "        for jj in np.unique(T_unit):\n",
    "            idxT = np.ravel(T_unit==jj)\n",
    "            HI_mu_cycle = np.mean(HI_unit[idxT])\n",
    "            std_cycle = np.std(HI_unit[idxT])\n",
    "            results.append(HI_mu_cycle)\n",
    "            stds.append(std_cycle)\n",
    "            cycle_num.append(jj)\n",
    "            unit_num.append(j)\n",
    "    return np.array(results), np.array(stds),np.array(cycle_num),np.array(unit_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUL MODEL\n",
    "\n",
    "def predictor(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_H_in=1,\n",
    "      feature_out_size=1,\n",
    "      activation='relu',\n",
    "      filter = [10,10,1],\n",
    "      filter_size = 10,\n",
    "      useH=True):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "    \n",
    "    if useH:\n",
    "      h_in = layers.Input(shape=(t,feature_H_in),name=\"H_in\")\n",
    "      # h_in = layers.Input(shape=(1,1),name=\"H_in\")\n",
    "      x = tf.concat([x_in,w_in, h_in],-1)\n",
    "    else: \n",
    "      x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    for i in filter:\n",
    "      x = layers.Conv1D(i,filter_size,1,padding='same',activation = activation)(x)\n",
    "      # x = layers.BatchNormalization()(x)\n",
    "      \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(50,activation = activation)(x)\n",
    "    y = layers.Dense(feature_out_size,activation = 'linear')(y)\n",
    "\n",
    "    if useH:\n",
    "      model = models.Model([x_in,w_in,h_in], y)\n",
    "    else:\n",
    "      model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AE MODEL FCN\n",
    "def AE(original_dimX,original_dimW,latent_dims,z_size = 1):\n",
    "    \n",
    "    inpX = layers.Input(shape=(original_dimX,))\n",
    "    inpW = layers.Input(shape=(original_dimW,))\n",
    "    x = tf.concat([inpX,inpW],axis = -1)\n",
    "        \n",
    "\n",
    "    for i in latent_dims:\n",
    "        x=tf.keras.layers.Dense(i,activation='relu')(x)\n",
    "        \n",
    "    z = tf.keras.layers.Dense(z_size,name=\"Z\")(x)\n",
    "     \n",
    "    x = tf.keras.layers.Concatenate()([z, inpW])\n",
    "            \n",
    "    for i in reversed(list(latent_dims)):\n",
    "        x=tf.keras.layers.Dense(i,activation='relu')(x) \n",
    "    x=tf.keras.layers.Dense(original_dimX,name='X')(x)   \n",
    "       \n",
    "    model = models.Model([inpX,inpW],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inception2D(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_out_size=1):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in,1),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in,1),name=\"W_in\")\n",
    "    \n",
    "\n",
    "    x = tf.concat([x_in,w_in],-2)\n",
    "      \n",
    "    layer_1 = tf.keras.layers.Conv2D(10, (3,3), padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_2 = tf.keras.layers.Conv2D(10, (5,5), padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_3 = tf.keras.layers.MaxPooling2D(3, strides=(1,1), padding='same')(x)\n",
    "    layer_3 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(layer_3)\n",
    "\n",
    "    mid_1 = tf.keras.layers.concatenate([layer_1, layer_2, layer_3], axis = 3)\n",
    "\n",
    "    ### 2nd Module\n",
    "    layer_4 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(mid_1)\n",
    "    layer_4 = tf.keras.layers.Conv2D(10, (3,3), padding='same', activation='relu')(layer_4)\n",
    "\n",
    "    layer_5 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(mid_1)\n",
    "    layer_5 = tf.keras.layers.Conv2D(10, (5,5), padding='same', activation='relu')(layer_5)\n",
    "\n",
    "    layer_6 = tf.keras.layers.MaxPooling2D(1, strides=(1,1), padding='same')(mid_1)\n",
    "    layer_6 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(layer_6)\n",
    "\n",
    "    mid_2 = tf.keras.layers.concatenate([layer_4, layer_5, layer_6], axis = 2)\n",
    "\n",
    "    flat_1 = tf.keras.layers.Flatten()(mid_2)\n",
    "\n",
    "    drop = tf.keras.layers.Dropout(.5)(flat_1)\n",
    "\n",
    "    dense_1 = tf.keras.layers.Dense(256, activation='sigmoid')(drop)\n",
    "    y = tf.keras.layers.Dense(feature_out_size, activation='relu')(dense_1)\n",
    "\n",
    "    model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def inception1D(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_out_size=1):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "\n",
    "    x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    layer_1 = tf.keras.layers.Conv1D(10, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_2 = tf.keras.layers.Conv1D(10, 5, padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_3 = tf.keras.layers.MaxPooling1D(3, strides=1, padding='same')(x)\n",
    "    layer_3 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(layer_3)\n",
    "\n",
    "    mid_1 = tf.keras.layers.concatenate([layer_1, layer_2, layer_3], axis = 2)\n",
    "\n",
    "    ### 2nd Module\n",
    "    layer_4 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(mid_1)\n",
    "    layer_4 = tf.keras.layers.Conv1D(10, 3, padding='same', activation='relu')(layer_4)\n",
    "\n",
    "    layer_5 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(mid_1)\n",
    "    layer_5 = tf.keras.layers.Conv1D(10, 5, padding='same', activation='relu')(layer_5)\n",
    "\n",
    "    layer_6 = tf.keras.layers.MaxPooling1D(1, strides=1, padding='same')(mid_1)\n",
    "    layer_6 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(layer_6)\n",
    "\n",
    "    mid_2 = tf.keras.layers.concatenate([layer_4, layer_5, layer_6], axis = 2)\n",
    "\n",
    "    flat_1 = tf.keras.layers.Flatten()(mid_2)\n",
    "\n",
    "    drop = tf.keras.layers.Dropout(.5)(flat_1)\n",
    "\n",
    "    dense_1 = tf.keras.layers.Dense(256, activation='sigmoid')(drop)\n",
    "    y = tf.keras.layers.Dense(feature_out_size, activation='relu')(dense_1)\n",
    "\n",
    "    model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction\n",
    "\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctRUL(hi, RUL):\n",
    "    if hi == 1:\n",
    "      return -1\n",
    "    else:\n",
    "      return RUL\n",
    "def correctMaxRUL(hi, RUL,MAX_RUL):\n",
    "    if hi == 1:\n",
    "      return MAX_RUL\n",
    "    else:\n",
    "      return RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FC in range(0,FCs):\n",
    "    # Load data DEV\n",
    "    with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_dev'+\".h5\", 'r') as hdf:\n",
    "                # Development set\n",
    "                W_train = np.array(hdf.get('W_dev'), dtype='float16')             # W\n",
    "                X_s_train = np.array(hdf.get('X_s_dev'), dtype='float16')         # X_s\n",
    "                X_v_train = np.array(hdf.get('X_v_dev'), dtype='float16')         # X_v\n",
    "                T_train = np.array(hdf.get('T_dev'), dtype='float16')             # T\n",
    "                Y_train = np.array(hdf.get('Y_dev'), dtype='float16')             # RUL  \n",
    "                A_train = np.array(hdf.get('A_dev'), dtype='float16')\n",
    "                \n",
    "                W_train = W_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "                X_s_train = X_s_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "                X_v_train = X_v_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "                T_train = T_train[::DOWNSAMPLING_STEP[FC],:] \n",
    "                Y_train = Y_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "                A_train = A_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "\n",
    "                # Varnams\n",
    "                W_var = np.array(hdf.get('W_var'))\n",
    "                X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "                X_v_var = np.array(hdf.get('X_v_var')) \n",
    "                T_var = np.array(hdf.get('T_var'))\n",
    "                A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "                    # from np.array to list dtype U4/U5\n",
    "                W_var = list(np.array(W_var, dtype='U20'))\n",
    "                X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "                X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "                T_var = list(np.array(T_var, dtype='U20'))\n",
    "                A_var = list(np.array(A_var, dtype='U20'))\n",
    "\n",
    "    # Load data TEST\n",
    "    mode = '_test'\n",
    "    with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_test'+\".h5\", 'r') as hdf:\n",
    "                # Development set\n",
    "                W_test = np.array(hdf.get('W_test'), dtype='float16')             # W\n",
    "                X_s_test = np.array(hdf.get('X_s_test'), dtype='float16')         # X_s\n",
    "                X_v_test = np.array(hdf.get('X_v_test'), dtype='float16')         # X_v\n",
    "                T_test = np.array(hdf.get('T_test'), dtype='float16')             # T\n",
    "                Y_test = np.array(hdf.get('Y_test'), dtype='float16')             # RUL  \n",
    "                A_test = np.array(hdf.get('A_test'), dtype='float16')\n",
    "                \n",
    "                W_test = W_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                X_s_test = X_s_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                X_v_test = X_v_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                T_test = T_test[::DOWNSAMPLING_STEP[FC],:] \n",
    "                Y_test = Y_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                A_test = A_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                \n",
    "    if FC==0:\n",
    "        W_train_aux = W_train\n",
    "        X_s_train_aux = X_s_train\n",
    "        X_v_train_aux = X_v_train\n",
    "        T_train_aux = T_train\n",
    "        Y_train_aux = Y_train\n",
    "        A_train_aux = A_train\n",
    "        \n",
    "        W_test_aux = W_test\n",
    "        X_s_test_aux = X_s_test\n",
    "        X_v_test_aux = X_v_test\n",
    "        T_test_aux = T_test\n",
    "        Y_test_aux = Y_test\n",
    "        A_test_aux = A_test\n",
    "    if FC!=0:\n",
    "        W_train_aux = np.concatenate((W_train_aux, W_train), axis=0)  \n",
    "        X_s_train_aux = np.concatenate((X_s_train_aux, X_s_train), axis=0)\n",
    "        X_v_train_aux = np.concatenate((X_v_train_aux, X_v_train), axis=0)\n",
    "        T_train_aux = np.concatenate((T_train_aux, T_train), axis=0)\n",
    "        Y_train_aux = np.concatenate((Y_train_aux, Y_train), axis=0) \n",
    "        A_train_aux = np.concatenate((A_train_aux, A_train), axis=0) \n",
    "        \n",
    "        W_test_aux = np.concatenate((W_test_aux, W_test), axis=0)  \n",
    "        X_s_test_aux = np.concatenate((X_s_test_aux, X_s_test), axis=0)\n",
    "        X_v_test_aux = np.concatenate((X_v_test_aux, X_v_test), axis=0)\n",
    "        T_test_aux = np.concatenate((T_test_aux, T_test), axis=0)\n",
    "        Y_test_aux = np.concatenate((Y_test_aux, Y_test), axis=0) \n",
    "        A_test_aux = np.concatenate((A_test_aux, A_test), axis=0) \n",
    "        \n",
    "units_train=A_train_aux[:,0].reshape(-1,1)\n",
    "cycles_train=A_train_aux[:,1].reshape(-1,1)\n",
    "fc_train = A_train_aux[:,2].reshape(-1,1)\n",
    "hi_train = A_train_aux[:,-1]\n",
    "\n",
    "if PIECE_WISE==True:\n",
    "    df_hs_unit_train = DataFrame({'unit': units_train.reshape(-1).astype(int),'RUL': Y_train_aux.reshape(-1), 'hi': hi_train.reshape(-1)})\n",
    "    df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "    pd_aux=DataFrame(df_hs_unit_train.groupby('unit')['RUL'].max()).reset_index()\n",
    "    df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "    Y_train_aux=df_hs_unit_train['RUL'].to_numpy().reshape(len(df_hs_unit_train),1)\n",
    "\n",
    "\n",
    "units_test=A_test_aux[:,0].reshape(-1,1)\n",
    "cycles_test=A_test_aux[:,1].reshape(-1,1)\n",
    "fc_test = A_test_aux[:,2].reshape(-1,1)\n",
    "hi_test = A_test_aux[:,-1]\n",
    "\n",
    "if PIECE_WISE==True:\n",
    "    df_hs_unit_test = DataFrame({'unit': units_test.reshape(-1).astype(int),'RUL': Y_test_aux.reshape(-1), 'hi': hi_test.reshape(-1)})\n",
    "    df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "    pd_aux=DataFrame(df_hs_unit_test.groupby('unit')['RUL'].max()).reset_index()\n",
    "    df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "    Y_test_aux=df_hs_unit_test['RUL'].to_numpy().reshape(len(df_hs_unit_test),1)\n",
    "\n",
    "\n",
    "print(\"XS_train\",X_s_train_aux.shape)\n",
    "print(units_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxMin Scale $X_s$ and $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pcaX = PCA(n_components=1)\n",
    "pcaX.fit(X_s_train_aux)    \n",
    "dump(pcaX, open('data/federation/PCAX.pkl', 'wb'))\n",
    "\n",
    "pcaW = PCA(n_components=1)\n",
    "pcaW.fit(W_train_aux)    \n",
    "dump(pcaW, open('data/federation/PCAW.pkl', 'wb'))\n",
    "\n",
    "# SCALE\n",
    "\n",
    "# scaler_X = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(X_s_train_aux)\n",
    "dump(scaler_X, open('data/federation/ScalerX.pkl', 'wb'))\n",
    "X_s_train = scaler_X.transform(X_s_train_aux)\n",
    "X_s_test = scaler_X.transform(X_s_test_aux)\n",
    "\n",
    "# scaler_W = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_W = StandardScaler()\n",
    "scaler_W.fit(W_train_aux)\n",
    "dump(scaler_X, open('data/federation/ScalerW.pkl', 'wb'))\n",
    "W_train = scaler_W.transform(W_train_aux)\n",
    "W_test = scaler_W.transform(W_test_aux)\n",
    "\n",
    "scaler_Y = MinMaxScaler(feature_range=(0,1))\n",
    "Y_train = scaler_Y.fit_transform(Y_train_aux)\n",
    "Y_test = scaler_Y.transform(Y_test_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsamplig 0.1Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_windows, U_windows, C_windows=sequence_generator_per_unit(X_s_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows,_,_=sequence_generator_per_unit(W_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows,_,_=sequence_generator_per_unit(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = stride)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(X_windows.shape[0])\n",
    "training_idx, val_idx = shuffle_idx[X_windows.shape[0]//10:],shuffle_idx[:X_windows.shape[0]//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAYERS = [64,32]\n",
    "from datetime import datetime\n",
    "past = datetime.now()\n",
    "\n",
    "x_temp = X_windows\n",
    "w_temp = W_windows\n",
    "y_temp = Y_windows\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "t = X_windows.shape[1]\n",
    "#rul_model = predictor(t=t,useH=False)\n",
    "rul_model = inception1D(t=t)\n",
    "OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=LR)\n",
    "#LOSS = tf.keras.losses.MeanAbsoluteError()\n",
    "#LOSS = tf.keras.losses.RootMeanSquaredError()\n",
    "rul_model.compile(optimizer=OPTIMIZER, \n",
    "        loss= rmse, metrics=['mae',rmse])\n",
    "        \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=5,restore_best_weights=True)  \n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(((x_temp[training_idx],w_temp[training_idx]),y_temp[training_idx]))\n",
    "train_ds = train_ds.shuffle(buffer_size=x_temp.shape[0]//10).batch(BATCH_SIZE)     \n",
    "val_ds = tf.data.Dataset.from_tensor_slices(((x_temp[val_idx],w_temp[val_idx]),y_temp[val_idx]))    \n",
    "val_ds = val_ds.shuffle(buffer_size=x_temp.shape[0]//10).batch(BATCH_SIZE) \n",
    "history = rul_model.fit(train_ds,epochs=EPOCHS,validation_data=val_ds,verbose=1,callbacks = [callback])\n",
    "\n",
    "last = datetime.now()\n",
    "time=last-past\n",
    "print(\"Training Time =\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"model.h5\")\n",
    "rul_model.save_weights(\"RUL_MODEL/Data-centralized/model.h5\")\n",
    "#model.save_weights(\"nodes8_main_model_t6.h5\")\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = rul_model.to_json()\n",
    "with open(\"RUL_MODEL/Data-centralized/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"model.h5\")\n",
    "\n",
    "json_file = open(\"RUL_MODEL/Data-centralized/model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "rul_model = model_from_json(loaded_model_json)\n",
    "rul_model.load_weights(\"RUL_MODEL/Data-centralized/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for FC in range(0,FCs):\n",
    "    # Load data TEST\n",
    "    mode = '_test'\n",
    "    with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_test'+\".h5\", 'r') as hdf:\n",
    "                # Development set\n",
    "                W_test = np.array(hdf.get('W_test'), dtype='float16')             # W\n",
    "                X_s_test = np.array(hdf.get('X_s_test'), dtype='float16')         # X_s\n",
    "                X_v_test = np.array(hdf.get('X_v_test'), dtype='float16')         # X_v\n",
    "                T_test = np.array(hdf.get('T_test'), dtype='float16')             # T\n",
    "                Y_test = np.array(hdf.get('Y_test'), dtype='float16')             # RUL  \n",
    "                A_test = np.array(hdf.get('A_test'), dtype='float16')\n",
    "                \n",
    "                W_test = W_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                X_s_test = X_s_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                X_v_test = X_v_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                T_test = T_test[::DOWNSAMPLING_STEP[FC],:] \n",
    "                Y_test = Y_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                A_test = A_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                \n",
    "                if FC==0:\n",
    "                    W_test_aux = W_test\n",
    "                    X_s_test_aux = X_s_test\n",
    "                    Y_test_aux = Y_test\n",
    "                    A_test_aux = A_test\n",
    "                if FC!=0:\n",
    "                    W_test_aux = np.concatenate((W_test_aux, W_test), axis=0)  \n",
    "                    X_s_test_aux = np.concatenate((X_s_test_aux, X_s_test), axis=0)\n",
    "                    Y_test_aux = np.concatenate((Y_test_aux, Y_test), axis=0) \n",
    "                    A_test_aux = np.concatenate((A_test_aux, A_test), axis=0)\n",
    "        \n",
    "\n",
    "units_test=W_test_aux[:,0].reshape(-1,1)\n",
    "cycles_test=A_test_aux[:,1].reshape(-1,1)\n",
    "fc_test = A_test_aux[:,2].reshape(-1,1)\n",
    "hi_test = A_test_aux[:,-1]\n",
    "\n",
    "if PIECE_WISE==True:\n",
    "    df_hs_unit_test = DataFrame({'unit': units_test.reshape(-1).astype(int),'RUL': Y_test_aux.reshape(-1), 'hi': hi_test.reshape(-1)})\n",
    "    df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "    pd_aux=DataFrame(df_hs_unit_test.groupby('unit')['RUL'].max()).reset_index()\n",
    "    df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "    Y_test_aux=df_hs_unit_test['RUL'].to_numpy().reshape(len(df_hs_unit_test),1)\n",
    "\n",
    "\n",
    "# SCALE\n",
    "\n",
    "X_s_test = scaler_X.transform(X_s_test_aux)\n",
    "W_test = scaler_W.transform(W_test_aux)\n",
    "Y_test = scaler_Y.transform(Y_test_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_windows_test, U_windows_test,C_windows_test=sequence_generator_per_unit(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows_test,_,_=sequence_generator_per_unit(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows_test,_,_=sequence_generator_per_unit(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = stride)\n",
    "\n",
    "rul_predicted = rul_model.predict((X_windows_test,W_windows_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted_ = scaler_Y.inverse_transform(rul_predicted)\n",
    "groud_truth = scaler_Y.inverse_transform(Y_windows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math \n",
    "mae=mean_absolute_error(rul_predicted_,groud_truth)\n",
    "mse=mean_squared_error(rul_predicted_,groud_truth)\n",
    "rmse=np.sqrt(mse)\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Useful Life Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_TEST=3\n",
    "for FC in range(FC_TEST-1,FC_TEST):\n",
    "    # Load data TEST\n",
    "    with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_test'+\".h5\", 'r') as hdf:\n",
    "                # Development set\n",
    "                W_test = np.array(hdf.get('W_test'), dtype='float16')             # W\n",
    "                X_s_test = np.array(hdf.get('X_s_test'), dtype='float16')         # X_s\n",
    "                T_test = np.array(hdf.get('T_test'), dtype='float16')             # T\n",
    "                Y_test = np.array(hdf.get('Y_test'), dtype='float16')             # RUL  \n",
    "                A_test = np.array(hdf.get('A_test'), dtype='float16')\n",
    "                \n",
    "                W_test = W_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                X_s_test = X_s_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                T_test = T_test[::DOWNSAMPLING_STEP[FC],:] \n",
    "                Y_test = Y_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                A_test = A_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "\n",
    "units_test=A_test[:,0].reshape(-1,1)\n",
    "cycles_test=A_test[:,1].reshape(-1,1)\n",
    "fc_test = A_test[:,2].reshape(-1,1)\n",
    "hi_test = A_test[:,-1]\n",
    "\n",
    "df_hs_unit_test = DataFrame({'unit': units_test.reshape(-1).astype(int),'RUL': Y_test.reshape(-1), 'hi': hi_test.reshape(-1)})\n",
    "df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "pd_aux=DataFrame(df_hs_unit_test.groupby('unit')['RUL'].max()).reset_index()\n",
    "df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "Y_test=df_hs_unit_test['RUL'].to_numpy().reshape(len(df_hs_unit_test),1)\n",
    "\n",
    "# SCALE\n",
    "X_s_test = scaler_X.transform(X_s_test)\n",
    "W_test = scaler_W.transform(W_test)\n",
    "Y_test = scaler_Y.transform(Y_test)\n",
    "\n",
    "X_windows_test, U_windows_test,C_windows_test=sequence_generator_per_unit(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows_test,_,_=sequence_generator_per_unit(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows_test,_,_=sequence_generator_per_unit(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = stride)\n",
    "\n",
    "rul_predicted = rul_model.predict((X_windows_test,W_windows_test))\n",
    "rul_predicted_ = scaler_Y.inverse_transform(rul_predicted)\n",
    "groud_truth = scaler_Y.inverse_transform(Y_windows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math \n",
    "mae=mean_absolute_error(rul_predicted_,groud_truth)\n",
    "mse=mean_squared_error(rul_predicted_,groud_truth)\n",
    "rmse=np.sqrt(mse)\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC1\n",
    "if FC_TEST==1:\n",
    "    units = [107,109,214,312,314,508,510,608,610,708,710,810,815] \n",
    "# FC2\n",
    "if FC_TEST==2:\n",
    "    units = [108,215,315,407,408,409,507,607,707,811,907,908,909,910] \n",
    "# FC3\n",
    "if FC_TEST==3:\n",
    "    units = [110,211,310,311,313,410,509,609,709,812,813,814]\n",
    "\n",
    "numberUnits=4\n",
    "start=0\n",
    "numberUnits = int(len(units))\n",
    "for x in range(start,len(units),numberUnits):\n",
    "    if (x+2*numberUnits) < len(units):\n",
    "    #if (x+numberUnits) < len(units):\n",
    "        unit_sel = np.array(units[x:x+numberUnits], dtype=int)\n",
    "        plot_predicted_true_rul([rul_predicted_], unit_sel, U_windows_test, C_windows_test[:,0,:], groud_truth)\n",
    "    else:\n",
    "        unit_sel = np.array(units[x:len(np.unique(U_windows_test))], dtype=int)\n",
    "        plot_predicted_true_rul([rul_predicted_], unit_sel, U_windows_test, C_windows_test[:,0,:], groud_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
