{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting a N-CMAPSS Dataset by Fligh Class\n",
    "\n",
    "The new C-MAPSS datasets comprises multiple $DS$ sets varying in the number of run-to-failure trajectories of turbofan units. A single $DS$ set provides degradation trajectories of a determined number of turbofan engines with unknown and different initial health condition for complete flights and two failure modes (HPT efficiency degradation & HPT efficiency degradation combined with LPT efficiency and capacity degradation). \n",
    "\n",
    "$DS$ contains multivariate sensors readings of the complete run-to-failure trajectories. Therefore, the records stop at the cycle/time the engine failed. Particularly, $RUL$ estimation using inception-based CNN network, uses sensor variables $X_s$ and operating conditions $w$ to estimate the $RUL$. \n",
    "\n",
    "This notebook separate data of turbofan units from a determined $DS$ (filename) in development and test. Then, \"Reorganize Data per Flight Class\" Section divides turbofan units of those development and test splits in three different flight classs. At the end we have $DS*_{dev}h5$ and  $DS*_{test}.h5$ files. \n",
    "\n",
    "N-CMAPSS dataset, created by Manuel Arias is better explained in: https://www.mdpi.com/2306-5729/6/1/5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $DS$ sets to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-up - Define file location\n",
    "#filename = 'N-CMAPSS_DS01-005'\n",
    "#filename = 'N-CMAPSS_DS02-006'\n",
    "#filename = 'N-CMAPSS_DS03-012'\n",
    "#filename = 'N-CMAPSS_DS04'\n",
    "#filename = 'N-CMAPSS_DS05'\n",
    "#filename = 'N-CMAPSS_DS06'\n",
    "#filename = 'N-CMAPSS_DS07'\n",
    "#filename = 'N-CMAPSS_DS08a-009'\n",
    "filename = 'N-CMAPSS_DS08c-008'\n",
    "\n",
    "\n",
    "#filename = 'N-CMAPSS_DS08d-010'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time tracking, Operation time (min):  0.003\n",
    "t = time.process_time()  \n",
    "\n",
    "# Load data\n",
    "with h5py.File(filename+\".h5\", 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))           # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "        X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "        A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "        X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "        T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))\n",
    "                          \n",
    "#W = np.concatenate((W_dev, W_test), axis=0)  \n",
    "#X_s = np.concatenate((X_s_dev, X_s_test), axis=0)\n",
    "#X_v = np.concatenate((X_v_dev, X_v_test), axis=0)\n",
    "#T = np.concatenate((T_dev, T_test), axis=0)\n",
    "#Y = np.concatenate((Y_dev, Y_test), axis=0) \n",
    "#A = np.concatenate((A_dev, A_test), axis=0) \n",
    "\n",
    "print('')\n",
    "print(\"Operation time (min): \" , (time.process_time()-t)/60)\n",
    "print('')\n",
    "print (\"W_dev shape: \" + str(W_dev.shape))\n",
    "print (\"X_s_dev shape: \" + str(X_s_dev.shape))\n",
    "print (\"X_v_dev shape: \" + str(X_v_dev.shape))\n",
    "print (\"T_dev shape: \" + str(T_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))\n",
    "print (\"A_dev shape: \" + str(A_dev.shape))\n",
    "\n",
    "print (\"W_test shape: \" + str(W_dev.shape))\n",
    "print (\"X_s_test shape: \" + str(X_s_dev.shape))\n",
    "print (\"X_v_test shape: \" + str(X_v_dev.shape))\n",
    "print (\"T_test shape: \" + str(T_dev.shape))\n",
    "print (\"Y_test shape: \" + str(Y_dev.shape))\n",
    "print (\"A_test shape: \" + str(A_dev.shape))\n",
    "\n",
    "print (\"W_var shape: \" + str(len(W_var)))\n",
    "print (\"X_s_var shape: \" + str(len(X_s_var)))\n",
    "print (\"X_v_var shape: \" + str(len(X_v_var)))\n",
    "print (\"T_var shape: \" + str(len(T_var)))\n",
    "print (\"A_var shape: \" + str(len(A_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling 0.1Hz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Information ($A$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "df_A_dev = DataFrame(data=A_dev, columns=A_var)\n",
    "# TEST\n",
    "df_A_test = DataFrame(data=A_test, columns=A_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight Classes\n",
    "\n",
    "The units are divided into three flight classes depending on whether the unit is operating short-length flights (i.e., flight class 1), medium-length flights (i.e., flight class 2), or long-length flights (i.e., flight class 2). A number of real flight conditions are available within each of the flight classes.\n",
    "\n",
    "| Flight Class   | Flight Length [h]\n",
    "| :-----------:  | :-----------:    \n",
    "| 1              |    1 to 3        \n",
    "| 2              |    3 to 5        \n",
    "| 3              |    5 to 7        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_dev.unit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labelsize = 17\n",
    "plt.plot(df_A_dev.unit, df_A_dev.Fc, 'o')\n",
    "plt.tick_params(axis='x', labelsize=labelsize )\n",
    "plt.tick_params(axis='y', labelsize=labelsize )\n",
    "plt.xlabel('Unit # [-]', fontsize=labelsize)\n",
    "plt.ylabel('Flight Class # [-]', fontsize=labelsize )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_h5_file(dataset, filename, dataType, flightClass):\n",
    "    # Save numpy array \n",
    "    with h5py.File(filename+dataType+\"_FC\"+str(flightClass)+'.h5', 'w') as f:\n",
    "        f.create_dataset(filename+dataType+\"_FC\"+str(flightClass), data=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Hz variable to Auxiliar Information ($A$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(hz):\n",
    "    if hz%10 == 0:\n",
    "        return hz\n",
    "    else:\n",
    "        return np.nan\n",
    "# DEV\n",
    "df_SubA_dev=df_A_dev    \n",
    "df_SubA_dev['Hz'] = df_A_dev.groupby(['unit','cycle']).cumcount().add(1)\n",
    "df_SubA_dev['Hz'] = df_SubA_dev.apply(lambda row: downsampling(row['Hz']), axis=1)\n",
    "\n",
    "# TEST\n",
    "df_SubA_test=df_A_test    \n",
    "df_SubA_test['Hz'] = df_A_test.groupby(['unit','cycle']).cumcount().add(1)\n",
    "df_SubA_test['Hz'] = df_SubA_test.apply(lambda row: downsampling(row['Hz']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsamplig Degradation ($\\theta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "df_T_dev = DataFrame(data=T_dev, columns=T_var) \n",
    "df_T_dev_downsampled = pd.concat([df_SubA_dev, df_T_dev], axis=1)\n",
    "df_T_dev_downsampled = df_T_dev_downsampled.dropna(axis=0)\n",
    "df_T_dev_downsampled = df_T_dev_downsampled.reset_index(drop=True)\n",
    "\n",
    "for FC in df_T_dev_downsampled.Fc.unique():\n",
    "    df = df_T_dev_downsampled.loc[df_T_dev_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'T_dev', int(FC))\n",
    "# TEST    \n",
    "df_T_test = DataFrame(data=T_test, columns=T_var) \n",
    "df_T_test_downsampled = pd.concat([df_SubA_test, df_T_test], axis=1)\n",
    "df_T_test_downsampled = df_T_test_downsampled.dropna(axis=0)\n",
    "df_T_test_downsampled = df_T_test_downsampled.reset_index(drop=True)\n",
    "\n",
    "for FC in df_T_test_downsampled.Fc.unique():\n",
    "    df = df_T_test_downsampled.loc[df_T_test_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'T_test', int(FC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsamplig Ground Truth ($Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "df_Y_dev = DataFrame(data=Y_dev) \n",
    "df_Y_dev_downsampled = pd.concat([df_SubA_dev, df_Y_dev], axis=1)\n",
    "df_Y_dev_downsampled = df_Y_dev_downsampled.dropna(axis=0)\n",
    "df_Y_dev_downsampled = df_Y_dev_downsampled.reset_index(drop=True)\n",
    "\n",
    "for FC in df_Y_dev_downsampled.Fc.unique():\n",
    "    df = df_Y_dev_downsampled.loc[df_Y_dev_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'Y_dev', int(FC))\n",
    "\n",
    "# TEST\n",
    "df_Y_test = DataFrame(data=Y_test) \n",
    "df_Y_test_downsampled = pd.concat([df_SubA_test, df_Y_test], axis=1)\n",
    "df_Y_test_downsampled = df_Y_test_downsampled.dropna(axis=0)\n",
    "df_Y_test_downsampled = df_Y_test_downsampled.reset_index(drop=True)\n",
    "\n",
    "for FC in df_Y_test_downsampled.Fc.unique():\n",
    "    df = df_Y_test_downsampled.loc[df_Y_test_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'Y_test', int(FC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsamplig Operative Conditions ($w$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "df_W_dev = DataFrame(data=W_dev, columns=W_var)\n",
    "df_W_dev['unit'] = df_A_dev['unit'].values\n",
    "df_W_dev_downsampled = pd.concat([df_SubA_dev, df_W_dev], axis=1)\n",
    "df_W_dev_downsampled = df_W_dev_downsampled.dropna(axis=0)\n",
    "df_W_dev_downsampled = df_W_dev_downsampled.reset_index(drop=True)\n",
    "for FC in df_W_dev_downsampled.Fc.unique():\n",
    "    df = df_W_dev_downsampled.loc[df_W_dev_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'W_dev', int(FC))\n",
    "    \n",
    "# TEST\n",
    "df_W_test = DataFrame(data=W_test, columns=W_var)\n",
    "df_W_test['unit'] = df_A_test['unit'].values\n",
    "df_W_test_downsampled = pd.concat([df_SubA_test, df_W_test], axis=1)\n",
    "df_W_test_downsampled = df_W_test_downsampled.dropna(axis=0)\n",
    "df_W_test_downsampled = df_W_test_downsampled.reset_index(drop=True)\n",
    "for FC in df_W_test_downsampled.Fc.unique():\n",
    "    df = df_W_test_downsampled.loc[df_W_test_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'W_test', int(FC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsamplig Sensor readings ($X_s$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "df_X_s_dev = DataFrame(data=X_s_dev, columns=X_s_var)\n",
    "df_X_s_dev_downsampled = pd.concat([df_SubA_dev, df_X_s_dev], axis=1)\n",
    "df_X_s_dev_downsampled = df_X_s_dev_downsampled.dropna(axis=0)\n",
    "df_X_s_dev_downsampled = df_X_s_dev_downsampled.reset_index(drop=True)\n",
    "for FC in df_X_s_dev_downsampled.Fc.unique():\n",
    "    df = df_X_s_dev_downsampled.loc[df_X_s_dev_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'W_s_dev', int(FC))\n",
    "    \n",
    "# TEST\n",
    "df_X_s_test = DataFrame(data=X_s_test, columns=X_s_var)\n",
    "df_X_s_test_downsampled = pd.concat([df_SubA_test, df_X_s_test], axis=1)\n",
    "df_X_s_test_downsampled = df_X_s_test_downsampled.dropna(axis=0)\n",
    "df_X_s_test_downsampled = df_X_s_test_downsampled.reset_index(drop=True)\n",
    "for FC in df_X_s_test_downsampled.Fc.unique():\n",
    "    df = df_X_s_test_downsampled.loc[df_X_s_test_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'W_s_test', int(FC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling Virtual Sensors ($X_v$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DEV \n",
    "df_X_v_dev = DataFrame(data=X_v_dev, columns=X_v_var) \n",
    "df_X_v_dev_downsampled = pd.concat([df_SubA_dev, df_X_v_dev], axis=1)\n",
    "df_X_v_dev_downsampled = df_X_v_dev_downsampled.dropna(axis=0)\n",
    "df_X_v_dev_downsampled = df_X_v_dev_downsampled.reset_index(drop=True)\n",
    "for FC in df_X_v_dev_downsampled.Fc.unique():\n",
    "    df = df_X_v_dev_downsampled.loc[df_X_v_dev_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'W_v_dev', int(FC))\n",
    "    \n",
    "# TEST\n",
    "df_X_v_test = DataFrame(data=X_v_test, columns=X_v_var) \n",
    "df_X_v_test_downsampled = pd.concat([df_SubA_test, df_X_v_test], axis=1)\n",
    "df_X_v_test_downsampled = df_X_v_test_downsampled.dropna(axis=0)\n",
    "df_X_v_test_downsampled = df_X_v_test_downsampled.reset_index(drop=True)\n",
    "for FC in df_X_v_test_downsampled.Fc.unique():\n",
    "    df = df_X_v_test_downsampled.loc[df_X_v_test_downsampled[\"Fc\"]==FC]\n",
    "    save_h5_file(df, filename, 'W_v_test', int(FC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorganize Data per Flight Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_file(resources_path, name):\n",
    "    # Read numpy array \n",
    "    hf = h5py.File(resources_path+name+\".h5\", 'r')\n",
    "    return np.array(hf[name][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'N-CMAPSS_DS01-005'\n",
    "#dataset= 'DS01-005/'\n",
    "\n",
    "#filename = 'N-CMAPSS_DS02-006'\n",
    "#dataset= 'DS02-006/'\n",
    "\n",
    "#filename = 'N-CMAPSS_DS03-012'\n",
    "#dataset= 'DS03-012/'\n",
    "\n",
    "#filename = 'N-CMAPSS_DS04'\n",
    "#dataset= 'DS04/'\n",
    "\n",
    "#filename = 'N-CMAPSS_DS05'\n",
    "#dataset= 'DS05/'\n",
    "\n",
    "#filename = 'N-CMAPSS_DS06'\n",
    "#dataset= 'DS06/'\n",
    "\n",
    "#filename = 'N-CMAPSS_DS07'\n",
    "#dataset= 'DS07/'\n",
    "\n",
    "#filename = 'N-CMAPSS_DS08a-009'\n",
    "#dataset= 'DS08a-009/'\n",
    "\n",
    "filename = 'N-CMAPSS_DS08c-008'\n",
    "dataset= 'DS08c-008/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC = 3\n",
    "resources_path = \"FC\"+str(int(FC))+\"/\"+dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename+\".h5\", 'r') as hdf:\n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        # to save as numpy array\n",
    "        W_var_array  = np.array(W_var)\n",
    "        X_s_var_array = np.array(X_s_var)\n",
    "        X_v_var_array = np.array(X_v_var)\n",
    "        T_var_array = np.array(T_var)\n",
    "        A_var_array = np.array(A_var)\n",
    "        \n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "        X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "        T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_var_original = ['unit',\n",
    " 'cycle',\n",
    " 'Fc',\n",
    " 'hs']\n",
    "A_var.extend(T_var)\n",
    "A_var.extend(['Hz'])\n",
    "W_var.extend(A_var_original)\n",
    "W_var.extend(['Hz','unit'])\n",
    "X_s_var.extend(A_var_original)\n",
    "X_s_var.extend(['Hz'])\n",
    "X_v_var.extend(A_var_original)\n",
    "X_v_var.extend(['Hz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "df_T_dev = DataFrame(data=read_h5_file(resources_path, filename+\"T_dev\"+\"_FC\"+str(int(FC))), columns=A_var)\n",
    "df_T_dev.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_T_dev.drop(column, axis=1, inplace=True)\n",
    "\n",
    "df_Y_dev = DataFrame(data=read_h5_file(resources_path, filename+\"Y_dev\"+\"_FC\"+str(int(FC))))\n",
    "for column in range(0,5):\n",
    "        df_Y_dev.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "df_A_dev = DataFrame(data=read_h5_file(resources_path, filename+\"T_dev\"+\"_FC\"+str(int(FC))), columns=A_var)\n",
    "df_A_dev.drop('Hz', axis=1, inplace=True)\n",
    "for column in T_var:\n",
    "        df_A_dev.drop(column, axis=1, inplace=True)        \n",
    "        \n",
    "\n",
    "df_W_dev = DataFrame(data=read_h5_file(resources_path, filename+\"W_dev\"+\"_FC\"+str(int(FC))), columns=W_var)\n",
    "df_W_dev.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_W_dev.drop(column, axis=1, inplace=True)\n",
    "\n",
    "df_X_s_dev = DataFrame(data=read_h5_file(resources_path, filename+\"W_s_dev\"+\"_FC\"+str(int(FC))), columns=X_s_var)\n",
    "df_X_s_dev.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_X_s_dev.drop(column, axis=1, inplace=True)\n",
    "\n",
    "df_X_v_dev = DataFrame(data=read_h5_file(resources_path, filename+\"W_v_dev\"+\"_FC\"+str(int(FC))), columns=X_v_var)\n",
    "df_X_v_dev.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_X_v_dev.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename+\"_dev\"+'.h5', 'w') as f:\n",
    "    f.create_dataset('T_dev', data=df_T_dev.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('Y_dev', data=df_Y_dev.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('A_dev', data=df_A_dev.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('W_dev', data=df_W_dev.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('X_s_dev', data=df_X_s_dev.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('X_v_dev', data=df_X_v_dev.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('W_var', data=W_var_array)\n",
    "    f.create_dataset('X_s_var', data=X_s_var_array)\n",
    "    f.create_dataset('X_v_var', data=X_v_var_array)\n",
    "    f.create_dataset('T_var', data=T_var_array)\n",
    "    f.create_dataset('A_var', data=A_var_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "df_T_test = DataFrame(data=read_h5_file(resources_path, filename+\"T_test\"+\"_FC\"+str(int(FC))), columns=A_var)\n",
    "df_T_test.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_T_test.drop(column, axis=1, inplace=True)\n",
    "\n",
    "df_Y_test = DataFrame(data=read_h5_file(resources_path, filename+\"Y_test\"+\"_FC\"+str(int(FC))))\n",
    "for column in range(0,5):\n",
    "        df_Y_test.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "df_A_test = DataFrame(data=read_h5_file(resources_path, filename+\"T_test\"+\"_FC\"+str(int(FC))), columns=A_var)\n",
    "df_A_test.drop('Hz', axis=1, inplace=True)\n",
    "for column in T_var:\n",
    "    if column!='Hz':\n",
    "        df_A_test.drop(column, axis=1, inplace=True)        \n",
    "        \n",
    "\n",
    "df_W_test = DataFrame(data=read_h5_file(resources_path, filename+\"W_test\"+\"_FC\"+str(int(FC))), columns=W_var)\n",
    "df_W_test.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_W_test.drop(column, axis=1, inplace=True)\n",
    "\n",
    "df_X_s_test = DataFrame(data=read_h5_file(resources_path, filename+\"W_s_test\"+\"_FC\"+str(int(FC))), columns=X_s_var)\n",
    "df_X_s_test.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_X_s_test.drop(column, axis=1, inplace=True)\n",
    "\n",
    "df_X_v_test = DataFrame(data=read_h5_file(resources_path, filename+\"W_v_test\"+\"_FC\"+str(int(FC))), columns=X_v_var)\n",
    "df_X_v_test.drop('Hz', axis=1, inplace=True)\n",
    "for column in A_var_original:\n",
    "    if column!='Hz':\n",
    "        df_X_v_test.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename+\"_test\"+'.h5', 'w') as f:\n",
    "    f.create_dataset('T_test', data=df_T_test.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('Y_test', data=df_Y_test.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('A_test', data=df_A_test.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('W_test', data=df_W_test.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('X_s_test', data=df_X_s_test.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('X_v_test', data=df_X_v_test.to_numpy(dtype='float32'))\n",
    "    f.create_dataset('W_var', data=W_var_array)\n",
    "    f.create_dataset('X_s_var', data=X_s_var_array)\n",
    "    f.create_dataset('X_v_var', data=X_v_var_array)\n",
    "    f.create_dataset('T_var', data=T_var_array)\n",
    "    f.create_dataset('A_var', data=A_var_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
