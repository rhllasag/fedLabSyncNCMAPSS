{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437c5788",
   "metadata": {},
   "source": [
    "# Collaborative RUL Estimation of Turbofan Engines using NCMAPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "import operator\n",
    "from pandas import DataFrame\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "#Keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "#Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import product\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for pcidev in $(lspci -D|grep 'VGA compatible controller: NVIDIA'|sed -e 's/[[:space:]].*//'); do echo 0 > /sys/bus/pci/devices/${pcidev}/numa_node; done\n",
    "\n",
    "for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(device_type = 'GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63649cda",
   "metadata": {},
   "source": [
    "## Create a strategy to distribute the variables and the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\",\"GPU:3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56108d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NODES = 3\n",
    "WINDOW_LEN = 50\n",
    "STRIDE = 6\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = [64,32]\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Sync Variables\n",
    "M_LABELS_TRAIN=[*range(0,NODES)]\n",
    "M_LABELS_EVAL=[*range(0,NODES)]\n",
    "SHUFFLE_IDX = {}\n",
    "TRAINING_IDX = {}\n",
    "VAL_IDX = {}\n",
    "MAX_DATASET_SIZE_TRAIN=0\n",
    "MAX_DATASET_SIZE_EVAL=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94109e4a",
   "metadata": {},
   "source": [
    "## Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78102e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(input_data, sequence_length, stride = 1, option = None):\n",
    "    \"\"\"\n",
    "     \n",
    "    \"\"\"\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(0,len(input_data),stride):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + sequence_length\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(input_data):\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        if option=='last':\n",
    "            seq_x = input_data[end_ix-1, :]\n",
    "        elif option=='next':\n",
    "            seq_x = input_data[end_ix, :]\n",
    "        else:\n",
    "            seq_x = input_data[i:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    \n",
    "    return np.array(X)\n",
    "def sequence_generator(input_data, units, cycles, sequence_length=10,stride = 1, option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        c_mask = cycles[mask]\n",
    "        x_unit = input_data[mask]\n",
    "        for j in np.unique(c_mask):\n",
    "            mask = np.ravel(c_mask==j)\n",
    "            seq_x_u = split_sequences(x_unit[mask],sequence_length, stride, option)\n",
    "            X.append(seq_x_u)\n",
    "            unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "            c_num.extend(np.ones(len(seq_x_u),dtype = int)*j)\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.array(c_num).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a89c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDistribution(NODES):\n",
    "    \"\"\"\n",
    "        This function is used to distribute the indexes of Training and Eval datasets\n",
    "    \"\"\"\n",
    "    global SHUFFLE_IDX\n",
    "    global TRAINING_IDX\n",
    "    global VAL_IDX\n",
    "    global M_LABELS_TRAIN\n",
    "    global M_LABELS_EVAL\n",
    "    global MAX_DATASET_SIZE_TRAIN\n",
    "    global MAX_DATASET_SIZE_EVAL\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    \n",
    "    for FC in range(0, NODES):\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_dev'+\".h5\", 'r') as hdf:\n",
    "            Y_train = np.array(hdf.get('Y_dev'), dtype='float32')\n",
    "            A_train = np.array(hdf.get('A_dev'), dtype='float32')\n",
    "            \n",
    "            units_train=A_train[:,0].reshape(-1,1)\n",
    "            cycles_train=A_train[:,1].reshape(-1,1)\n",
    "            \n",
    "            # Create Windows for Labels\n",
    "            Y_windows,_,_=sequence_generator(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            np.take(Y_windows,np.random.permutation(Y_windows.shape[0]),axis=0,out=Y_windows)\n",
    "            indexes=[*range(0,len(Y_windows))]\n",
    "            training = indexes[:int(len(indexes)*0.85)] \n",
    "            validation = indexes[int(len(indexes)*0.85):int(len(indexes))]\n",
    "            M_LABELS_TRAIN[FC] = Y_windows[training]\n",
    "            M_LABELS_EVAL[FC] = Y_windows[validation]\n",
    "            if (MAX_DATASET_SIZE_TRAIN<len(Y_windows[training])):\n",
    "                MAX_DATASET_SIZE_TRAIN=len(Y_windows[training])\n",
    "            if (MAX_DATASET_SIZE_EVAL<len(Y_windows[validation])):\n",
    "                MAX_DATASET_SIZE_EVAL=len(Y_windows[validation])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2e806",
   "metadata": {},
   "source": [
    "## Splitting data by Flight Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02443a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Scaler Variables\n",
    "SCALER_X = MinMaxScaler()\n",
    "SCALER_W = MinMaxScaler()\n",
    "SCALER_Y = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe14b2",
   "metadata": {},
   "source": [
    "### Distribute Indexes (Training and Eval) and Labeles per Replica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb62265",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDistribution(NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_TRAIN=np.empty((MAX_DATASET_SIZE_TRAIN,NODES))\n",
    "M_TRAIN[:] = np.nan\n",
    "for x in range(0,NODES):\n",
    "    M_TRAIN[:len(M_LABELS_TRAIN[x]),x]=list(chain.from_iterable(M_LABELS_TRAIN[x]))\n",
    "M_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_EVAL=np.empty((MAX_DATASET_SIZE_EVAL,NODES))\n",
    "M_EVAL[:] = np.nan\n",
    "for x in range(0,NODES):\n",
    "    M_EVAL[:len(M_LABELS_EVAL[x]),x]=list(chain.from_iterable(M_LABELS_EVAL[x]))\n",
    "M_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelSynchronization(M, localLab):\n",
    "    numWorkers = len(list(zip(*M)))\n",
    "    ID=list(np.unique(M[:,0]))\n",
    "    synchronizedLabels=np.zeros(M.shape,dtype=int)\n",
    "    synchronizedLabels[:,localLab]=range(1,len(M)+1);\n",
    "    if(numWorkers>1):\n",
    "        labelsAndCounters=np.zeros((len(ID),numWorkers))\n",
    "        for worker in range(0,numWorkers):\n",
    "            for _id in range(0,len(ID)):\n",
    "                labelsAndCounters[_id,worker]=operator.countOf(M[:,worker].tolist(),ID[_id])\n",
    "        for worker in range(0,numWorkers):\n",
    "            if worker!=localLab:\n",
    "                counterPerLabel=np.zeros((len(ID),numWorkers), dtype=int)\n",
    "                for _id in range(0, len(ID)):\n",
    "                    element=np.where(M[:, worker]== ID[_id])\n",
    "                    for row in range(0, len(M)):\n",
    "                        if (M[row,localLab]== ID[_id]):\n",
    "                            synchronizedLabels[row,worker]=(list(element)[0])[counterPerLabel[_id,numWorkers-1]]+1\n",
    "                            counterPerLabel[_id,numWorkers-1] += 1\n",
    "                            if counterPerLabel[_id,numWorkers-1]>len(element):\n",
    "                                counterPerLabel[_id,numWorkers-1]=1\n",
    "    return synchronizedLabels\n",
    "\n",
    "\n",
    "SYNCHRONIZED_LABELS_TRAIN=labelSynchronization(M_TRAIN, NODES-1)-1\n",
    "SYNCHRONIZED_LABELS_EVAL=labelSynchronization(M_EVAL, NODES-1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNCHRONIZED_LABELS_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNCHRONIZED_LABELS_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_3d_array(arr, batch_size):\n",
    "    \"\"\"\n",
    "    Batch a 3D NumPy array into smaller chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: 3D NumPy array\n",
    "    - batch_size: Size of each batch along the first dimension\n",
    "\n",
    "    Returns:\n",
    "    - NumPy array of batches, where each batch is a 3D NumPy array\n",
    "    \"\"\"\n",
    "    shape = arr.shape\n",
    "    if len(shape) != 3:\n",
    "        raise ValueError(\"Input array must be 3D.\")\n",
    "\n",
    "    num_batches = (shape[0] + batch_size - 1) // batch_size\n",
    "    batches = np.empty((num_batches, batch_size, shape[1], shape[2]), dtype=arr.dtype)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, shape[0])\n",
    "        batch = arr[start:end, :, :]\n",
    "        batches[i, :end - start, :, :] = batch\n",
    "\n",
    "    return batches\n",
    "\n",
    "def batch_2d_array(arr, batch_size):\n",
    "    \"\"\"\n",
    "    Batch a 2D NumPy array into smaller chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: 2D NumPy array\n",
    "    - batch_size: Size of each batch\n",
    "\n",
    "    Returns:\n",
    "    - NumPy array of batches, where each batch is a 2D NumPy array\n",
    "    \"\"\"\n",
    "    shape = arr.shape\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Input array must be 2D.\")\n",
    "\n",
    "    num_batches = (shape[0] + batch_size - 1) // batch_size\n",
    "    batches = np.empty((num_batches, batch_size, shape[1]), dtype=arr.dtype)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, shape[0])\n",
    "        batch = arr[start:end, :]\n",
    "        batches[i, :end - start, :] = batch\n",
    "\n",
    "    return batches\n",
    "def batch_data_2d(test, batch_size):\n",
    "    m,n = test.shape\n",
    "    S = test.itemsize\n",
    "    if not batch_size:\n",
    "        batch_size = m\n",
    "    count_batches = m//batch_size\n",
    "    # Batches which can be covered fully\n",
    "    test_batches = as_strided(test, shape=(count_batches, batch_size, n), strides=(batch_size*n*S,n*S,S)).copy()\n",
    "    covered = count_batches*batch_size\n",
    "    if covered < m:\n",
    "        rest = test[covered:,:]\n",
    "        rm, rn = rest.shape\n",
    "        mismatch = batch_size - rm\n",
    "        last_batch = np.vstack((rest,np.zeros((mismatch,rn)))).reshape(1,-1,n)\n",
    "        return np.vstack((test_batches,last_batch))\n",
    "    return test_batches\n",
    "\n",
    "def batch_data_3d(test, batch_size):\n",
    "    m,n,p = test.shape\n",
    "    S = test.itemsize\n",
    "    if not batch_size:\n",
    "        batch_size = m\n",
    "    count_batches = m//batch_size\n",
    "    # Batches which can be covered fully\n",
    "    test_batches = as_strided(test, shape=(count_batches, batch_size, n, p), strides=(batch_size*n*p*S,n*p*S,p*S,S)).copy()\n",
    "    covered = count_batches*batch_size\n",
    "    if covered < m:\n",
    "        rest = test[covered:,:,:]\n",
    "        rm, rn, rp = rest.shape\n",
    "        mismatch = batch_size - rm\n",
    "        last_batch = np.vstack((rest,np.zeros((mismatch,rn,rp)))).reshape(1,-1,n,p)\n",
    "        return np.vstack((test_batches,last_batch))\n",
    "    return test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fn_data_partition_train(NODES, evaluation=False):\n",
    "    \n",
    "    # Global Variables\n",
    "    global SCALER_X\n",
    "    global SCALER_W\n",
    "    global SCALER_Y\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    global BATCH_SIZE\n",
    "    global TRAINING_IDX\n",
    "    global VAL_IDX\n",
    "    \n",
    "    distributedData=[]\n",
    "    for FC in range(0, NODES):\n",
    "        DATA_REPLICA_ID = []\n",
    "        # Load data DEV\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_dev'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_train = np.array(hdf.get('W_dev'), dtype='float32')             # W\n",
    "            X_s_train = np.array(hdf.get('X_s_dev'), dtype='float32')         # X_s\n",
    "            Y_train = np.array(hdf.get('Y_dev'), dtype='float32')             # RUL                  \n",
    "            A_train = np.array(hdf.get('A_dev'), dtype='float32')\n",
    "\n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "            # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "            \n",
    "\n",
    "            units_train=A_train[:,0].reshape(-1,1)\n",
    "            cycles_train=A_train[:,1].reshape(-1,1)\n",
    "                \n",
    "            X_s_train = SCALER_X.fit_transform(X_s_train)\n",
    "            W_train = SCALER_W.fit_transform(W_train)\n",
    "            Y_train = SCALER_Y.fit_transform(Y_train)\n",
    "            \n",
    "            X_windows, _, _=sequence_generator(X_s_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            W_windows,_,_=sequence_generator(W_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            Y_windows,_,_=sequence_generator(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            \n",
    "            if evaluation:\n",
    "                X_=X_windows[SYNCHRONIZED_LABELS_EVAL[:,FC]]\n",
    "                X_=X_[:,:,:]\n",
    "                \n",
    "                W_=W_windows[SYNCHRONIZED_LABELS_EVAL[:,FC]]\n",
    "                W_=W_[:,:,:]\n",
    "                \n",
    "                y_=Y_windows[SYNCHRONIZED_LABELS_EVAL[:,FC]]\n",
    "                y_=y_[:,:]\n",
    "                \n",
    "            else:\n",
    "                X_=X_windows[SYNCHRONIZED_LABELS_TRAIN[:,FC]]\n",
    "                X_=X_[:,:,:]\n",
    "                \n",
    "                \n",
    "                W_=W_windows[SYNCHRONIZED_LABELS_TRAIN[:,FC]]\n",
    "                W_=W_[:,:,:]\n",
    "                \n",
    "                \n",
    "                y_=Y_windows[SYNCHRONIZED_LABELS_TRAIN[:,FC]]\n",
    "                y_=y_[:,:]\n",
    "                \n",
    "            #X_=batch_3d_array(np.dstack((X_,W_)),BATCH_SIZE)\n",
    "            X_ = batch_data_3d(X_,BATCH_SIZE)\n",
    "            W_ = batch_data_3d(W_,BATCH_SIZE)\n",
    "            y_= batch_data_2d(y_,BATCH_SIZE)\n",
    "        for i in range(0,len(y_)):\n",
    "            DATA_REPLICA_ID.append(((tf.convert_to_tensor(X_[i], dtype=tf.float32),tf.convert_to_tensor(W_[i], dtype=tf.float32)),tf.convert_to_tensor(y_[i], dtype=tf.float32)))\n",
    "        distributedData.append(DATA_REPLICA_ID)\n",
    "    return distributedData\n",
    "\n",
    "def fn_data_partition_test_data_decentralized(NODES):\n",
    "    \n",
    "    # Global Variables\n",
    "    global SCALER_X\n",
    "    global SCALER_W\n",
    "    global SCALER_Y\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    global BATCH_SIZE\n",
    "    \n",
    "    distributedData=[]\n",
    "    for FC in range(0, NODES):\n",
    "        DATA_REPLICA_ID = []\n",
    "        # Load data DEV\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_test'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_test = np.array(hdf.get('W_test'), dtype='float32')             # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'), dtype='float32')         # X_s\n",
    "            Y_test = np.array(hdf.get('Y_test'), dtype='float32')             # RUL                  \n",
    "            A_test = np.array(hdf.get('A_test'), dtype='float32')\n",
    "\n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "            # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "            \n",
    "\n",
    "            units_test=A_test[:,0].reshape(-1,1)\n",
    "            cycles_test=A_test[:,1].reshape(-1,1)\n",
    "                \n",
    "            X_s_test = SCALER_X.fit_transform(X_s_test)\n",
    "            W_test = SCALER_W.fit_transform(W_test)\n",
    "            Y_test = SCALER_Y.fit_transform(Y_test)\n",
    "            \n",
    "            X_windows, _, _=sequence_generator(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            W_windows,_,_=sequence_generator(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            Y_windows,_,_=sequence_generator(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            \n",
    "            X_=X_windows[:,:,:]\n",
    "            W_=W_windows[:,:,:]\n",
    "            y_=Y_windows[:,:]\n",
    "            DATA_REPLICA_ID.append(((tf.convert_to_tensor(X_, dtype=tf.float32),tf.convert_to_tensor(W_, dtype=tf.float32)),tf.convert_to_tensor(y_, dtype=tf.float32)))\n",
    "        distributedData.append(DATA_REPLICA_ID)\n",
    "    return distributedData\n",
    "\n",
    "def fn_data_partition_test_data_centralized(NODES):\n",
    "    \n",
    "    # Global Variables\n",
    "    global SCALER_X\n",
    "    global SCALER_W\n",
    "    global SCALER_Y\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    global BATCH_SIZE\n",
    "    \n",
    "    distributedData=[]\n",
    "    DATA_REPLICA_ID = []\n",
    "    for FC in range(0, NODES):\n",
    "        # Load data DEV\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_test'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_test = np.array(hdf.get('W_test'), dtype='float32')             # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'), dtype='float32')         # X_s\n",
    "            Y_test = np.array(hdf.get('Y_test'), dtype='float32')             # RUL                  \n",
    "            A_test = np.array(hdf.get('A_test'), dtype='float32')\n",
    "\n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "            # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "        \n",
    "        if FC==0:\n",
    "            W_test_aux = W_test\n",
    "            X_s_test_aux = X_s_test\n",
    "            Y_test_aux = Y_test\n",
    "            A_test_aux = A_test\n",
    "        if FC!=0:\n",
    "            W_test_aux = np.concatenate((W_test_aux, W_test), axis=0)  \n",
    "            X_s_test_aux = np.concatenate((X_s_test_aux, X_s_test), axis=0)\n",
    "            Y_test_aux = np.concatenate((Y_test_aux, Y_test), axis=0) \n",
    "            A_test_aux = np.concatenate((A_test_aux, A_test), axis=0) \n",
    "\n",
    "    units_test=A_test_aux[:,0].reshape(-1,1)\n",
    "    cycles_test=A_test_aux[:,1].reshape(-1,1)\n",
    "                \n",
    "    X_s_test = SCALER_X.fit_transform(X_s_test_aux)\n",
    "    W_test = SCALER_W.fit_transform(W_test_aux)\n",
    "    Y_test = SCALER_Y.fit_transform(A_test_aux)\n",
    "            \n",
    "    X_windows, _, _=sequence_generator(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "    W_windows,_,_=sequence_generator(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "    Y_windows,_,_=sequence_generator(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            \n",
    "    X_=X_windows[:,:,:]\n",
    "    W_=W_windows[:,:,:]\n",
    "    y_=Y_windows[:,:]\n",
    "    X_ = batch_data_3d(X_,BATCH_SIZE)\n",
    "    W_ = batch_data_3d(W_,BATCH_SIZE)\n",
    "    y_= batch_data_2d(y_,BATCH_SIZE)\n",
    "    \n",
    "    for i in range(0,len(y_)):\n",
    "        DATA_REPLICA_ID.append(((tf.convert_to_tensor(X_[i], dtype=tf.float32),tf.convert_to_tensor(W_[i], dtype=tf.float32)),tf.convert_to_tensor(y_[i], dtype=tf.float32)))\n",
    "    distributedData.append(DATA_REPLICA_ID)\n",
    "    return distributedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributedDataTrain = fn_data_partition_train(NODES,False)\n",
    "distributedDataEval = fn_data_partition_train(NODES,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_fn_train(ctx):\n",
    "    return distributedDataTrain[ctx.replica_id_in_sync_group]\n",
    "def value_fn_eval(ctx):\n",
    "    return distributedDataEval[ctx.replica_id_in_sync_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_values_train = strategy.experimental_distribute_values_from_function(value_fn_train)\n",
    "distributed_values_eval = strategy.experimental_distribute_values_from_function(value_fn_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ae749",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUL MODEL\n",
    "\n",
    "def predictor(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_H_in=1,\n",
    "      feature_out_size=1,\n",
    "      activation='relu',\n",
    "      filter = [10,10,1],\n",
    "      filter_size = 10,\n",
    "      useH=True):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "    \n",
    "    if useH:\n",
    "      h_in = layers.Input(shape=(t,feature_H_in),name=\"H_in\")\n",
    "      # h_in = layers.Input(shape=(1,1),name=\"H_in\")\n",
    "      x = tf.concat([x_in,w_in, h_in],-1)\n",
    "    else: \n",
    "      x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    for i in filter:\n",
    "      x = layers.Conv1D(i,filter_size,1,padding='same',activation = activation)(x)\n",
    "      # x = layers.BatchNormalization()(x)\n",
    "      \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(50,activation = activation)(x)\n",
    "    y = layers.Dense(feature_out_size,activation = 'linear')(y)\n",
    "\n",
    "    if useH:\n",
    "      model = models.Model([x_in,w_in,h_in], y)\n",
    "    else:\n",
    "      model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca57499",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "Recall that the loss function consists of one or two parts:\n",
    "\n",
    "  * The **prediction loss** measures how far off the model's predictions are from the training labels for a batch of training examples. It is computed for each labeled example and then reduced across the batch by computing the average value.\n",
    "  * Optionally, **regularization loss** terms can be added to the prediction loss, to steer the model away from overfitting the training data. A common choice is L2 regularization, which adds a small fixed multiple of the sum of squares of all model weights, independent of the number of examples. The model above uses L2 regularization to demonstrate its handling in the training loop below.\n",
    "\n",
    "For training on a single machine with a single GPU/CPU, this works as follows:\n",
    "\n",
    "  * The prediction loss is computed for each example in the batch, summed across the batch, and then divided by the batch size.\n",
    "  * The regularization loss is added to the prediction loss.\n",
    "  * The gradient of the total loss is computed w.r.t. each model weight, and the optimizer updates each model weight from the corresponding gradient.\n",
    "\n",
    "With `tf.distribute.Strategy`, the input batch is split between replicas.\n",
    "For example, let's say you have 4 GPUs, each with one replica of the model. One batch of 256 input examples is distributed evenly across the 4 replicas, so each replica gets a batch of size 64: We have `256 = 4*64`, or generally `GLOBAL_BATCH_SIZE = num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`.\n",
    "\n",
    "Each replica computes the loss from the training examples it gets and computes the gradients of the loss w.r.t. each model weight. The optimizer takes care that these **gradients are summed up across replicas** before using them to update the copies of the model weights on each replica.\n",
    "\n",
    "*So, how should the loss be calculated when using a `tf.distribute.Strategy`?*\n",
    "\n",
    "  * Each replica computes the prediction loss for all examples distributed to it, sums up the results and divides them by `num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`, or equivently, `GLOBAL_BATCH_SIZE`.\n",
    "  * Each replica compues the regularization loss(es) and divides them by\n",
    "  `num_replicas_in_sync`.\n",
    "\n",
    "Compared to non-distributed training, all per-replica loss terms are scaled down by a factor of `1/num_replicas_in_sync`. On the other hand, all loss terms -- or rather, their gradients -- are summed across that number of replicas before the optimizer applies them. In effect, the optimizer on each replica uses the same gradients as if a non-distributed computation with `GLOBAL_BATCH_SIZE` had happened. This is consistent with the distributed and undistributed behavior of Keras `Model.fit`. See the [Distributed training with Keras](./keras.ipynb) tutorial on how a larger gloabl batch size enables to scale up the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def compute_loss_batch(labels, predictions, model_losses):\n",
    "        per_example_loss = (labels - predictions)**2  # Sample error\n",
    "        loss = tf.math.sqrt(tf.nn.compute_average_loss(per_example_loss)) # Batch Error\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    eval_mae = tf.keras.metrics.MeanAbsoluteError()\n",
    "    train_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    eval_rmse = tf.keras.metrics.RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "    model = predictor(t=50,useH=False,filter = LAYERS)\n",
    "    optimizer = SGD(learning_rate=LEARNING_RATE)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32a1e5",
   "metadata": {},
   "source": [
    "## Auxiliar Funcions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c294995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_batch(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "        loss = compute_loss_batch(labels, predictions, model.losses) # Batch Error\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_rmse.update_state(labels, predictions)\n",
    "    return loss\n",
    "def train_step_sample(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "    return predictions\n",
    "\n",
    "def compute_loss_fedLabSync(inputs, collaborativePredictions):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "        loss = compute_loss_batch(labels, (collaborativePredictions+predictions)/2, model.losses) # Batch Error\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_rmse.update_state(labels, (collaborativePredictions+predictions)/2)\n",
    "    return loss\n",
    "    \n",
    "def test_step(inputs, collaborativePredictions):\n",
    "    input_signals, labels = inputs\n",
    "    eval_mae.update_state(labels, collaborativePredictions)\n",
    "    eval_rmse.update_state(labels, collaborativePredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step_batch(dataset_inputs):\n",
    "    per_replica_losses = strategy.run(train_step_batch, args=(dataset_inputs,)) \n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "@tf.function\n",
    "def collaborative_predictions(dataset_inputs):\n",
    "    per_replica_predictions= strategy.run(train_step_sample, args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_predictions,  \n",
    "                         axis=None)\n",
    "@tf.function\n",
    "def local_collaborative_loss(collaborative_predictions, dataset_inputs):\n",
    "    return strategy.run(compute_loss_fedLabSync, args=(dataset_inputs, collaborative_predictions,))\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step_batch(dataset_inputs,collaborative_predictions):\n",
    "    return strategy.run(test_step, args=(dataset_inputs,collaborative_predictions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527e630",
   "metadata": {},
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1098f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # TRAIN LOOP\n",
    "    num_batches = 0\n",
    "    for x in distributed_values_train:\n",
    "        #distributed_train_step_batch(x)\n",
    "        colaboratePrediction = collaborative_predictions(x)\n",
    "        total_loss = local_collaborative_loss(colaboratePrediction, x)\n",
    "        num_batches += 1\n",
    "    \n",
    "    for x in distributed_values_eval:\n",
    "        colaboratePrediction = collaborative_predictions(x)\n",
    "        distributed_test_step_batch(x,colaboratePrediction)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "    template = (\"Epoch {}, Train_RMSE: {}, Eval MAE: {}, \"\n",
    "              \"Eval_RMSE: {}\")\n",
    "    print(template.format(epoch + 1, train_rmse.result(), eval_mae.result(),\n",
    "                         eval_rmse.result()))\n",
    "    eval_mae.reset_states()\n",
    "    train_rmse.reset_states()\n",
    "    eval_rmse.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_per_replica(model,FC):\n",
    "  model.save_weights(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"model_federated.h5\")\n",
    "  #model.save_weights(\"nodes8_main_model_t6.h5\")\n",
    "  # serialize model to JSON\n",
    "  model_json = model.to_json()\n",
    "  with open(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"model_federated.json\", \"w\") as json_file:\n",
    "      json_file.write(model_json)\n",
    "    \n",
    "def current_replica(ctx):\n",
    "    return ctx.replica_id_in_sync_group+1\n",
    "\n",
    "with strategy.scope():\n",
    "  replica = strategy.experimental_distribute_values_from_function(current_replica)\n",
    "  strategy.run(save_model_per_replica, args=(model,replica,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_per_replica(FC):\n",
    "  json_file = open(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"model_federated.json\", 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  return model_from_json(loaded_model_json)\n",
    "\n",
    "with strategy.scope():\n",
    "  replica = strategy.experimental_distribute_values_from_function(current_replica)\n",
    "  model= strategy.run(load_model_per_replica, args=(replica,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributedDataTest = fn_data_partition_test_data_centralized(NODES)\n",
    "def value_fn_test(ctx):\n",
    "    return distributedDataTest[0]\n",
    "\n",
    "distributed_values_test_data_decentralized = strategy.experimental_distribute_values_from_function(value_fn_test)\n",
    "\n",
    "for x in distributed_values_test_data_decentralized:\n",
    "        colaboratePrediction = collaborative_predictions(x)\n",
    "        print(len(colaboratePrediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
