{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437c5788",
   "metadata": {},
   "source": [
    "# Collaborative Remaining Useful Life ($\\textit{RUL}$) estimation of turbofan engines\n",
    "\n",
    "This notebook simulates a collaborative prognostics problem, the Remaining Useful Life estimation of turbofan engines located at different parties (airlines). Collaborative problem simulates the participation of three airlines differing in flight lengths (Flight Classes): short-length flights (i.e., flight class 1), medium-length flights (i.e., flight class 2), or long-length flights (i.e., flight class 2).\n",
    "\n",
    "| Flight Class   | Flight Length [h]\n",
    "| :-----------:  | :-----------:    \n",
    "| 1              |    1 to 3        \n",
    "| 2              |    3 to 5        \n",
    "| 3              |    5 to 7  \n",
    "\n",
    "Those flight classes were defined in the **new** C-MAPSS (Commercial Modular Aero-Propulsion System Simulation) dataset from NASA for aircraft engines. More details about the generation process can be found at https://www.mdpi.com/2306-5729/6/1/5.  \n",
    "\n",
    "This notebook reused the inception-based CNN network from (https://doi.org/10.36001/phmconf.2021.v13i1.3109) and its pre-processing precedures to estimate the $\\textit{RUL}$ of engines experiencing a determined flight class (FC). Datasets (FC*_dev.h5 and FC*_test.h5) for each FC are generated by executing:\n",
    "\n",
    "- 1) Spliting a given dataset by Flight Class.ipynb\n",
    "- 2) Concatenating datasets by Flight Class.ipynb\n",
    "\n",
    "The collaborative problem, simulated using the Federated Label Synchornization FedLabSync algorithm, uses \"TensorFlow Parallel Computing\" to distribute the work along multiple GPUs.  To run this code, a cluster of GPUs must be configured. Hyperparameters for distributed Machine Learning experiments are configured in \"Global Variables\" Section, e.g.,  number of nodes (parties participating in the ferated network) and the current node for which the federated model is trained. \n",
    "\n",
    "The sucess of FedLanSync algorithm lies in using a label synchronization strategy to create SYNCHRONIZED_LABELS_* matrices. Multiplie options are defined in \"Label Synchornization\" Section. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "import operator\n",
    "from pandas import DataFrame\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "#Keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "#Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "#sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy import spatial\n",
    "# EVALUATION modules\n",
    "from scipy.stats import spearmanr\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "params = {'legend.fontsize': 8,\n",
    "          'figure.figsize': (9,6),\n",
    "         'axes.labelsize': 20,\n",
    "         'axes.titlesize':20,\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'axes.linewidth' : 2,\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(device_type = 'GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63649cda",
   "metadata": {},
   "source": [
    "## Create a strategy to distribute the variables and the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy([\"GPU:1\", \"GPU:0\",\"GPU:3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56108d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NODES = 3\n",
    "NODE = 2\n",
    "WINDOW_LEN = 30\n",
    "STRIDE = 1\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 900\n",
    "DOWNSAMPLING_STEP = [2,2,2]\n",
    "# EARLY STOPPING\n",
    "PATIENCE=5\n",
    "COLLAB_LOSS_WEIGHT=0.01\n",
    "PIECE_WISE=True\n",
    "CYCLE_CORRECTION=False\n",
    "PCA_CORRECTION = True\n",
    "PCA_W=True\n",
    "#FC1\n",
    "#seed = 5\n",
    "#seed=257\n",
    "#seed=922\n",
    "\n",
    "#FC2\n",
    "#seed=257\n",
    "#seed=571\n",
    "#seed=111\n",
    "#seed=798\n",
    "#seed=345\n",
    "\n",
    "#FC3\n",
    "#seed=571\n",
    "#seed=111\n",
    "#seed=798\n",
    "#seed=345\n",
    "#seed=945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 257\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Sync Variables\n",
    "M_LABELS_TRAIN=[*range(0,NODES)]\n",
    "M_LABELS_EVAL=[*range(0,NODES)]\n",
    "SHUFFLE_IDX = {}\n",
    "TRAINING_IDX = {}\n",
    "VAL_IDX = {}\n",
    "MAX_DATASET_SIZE_TRAIN=0\n",
    "MAX_DATASET_SIZE_EVAL=0\n",
    "if PCA_CORRECTION==True:\n",
    "    if PCA_W==True:\n",
    "        PCA=load(open('data/federation/PCAW.pkl', 'rb'))\n",
    "    else:\n",
    "        PCA=load(open('data/federation/PCAX.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_loss):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            current_loss (float): The validation loss in the current epoch.\n",
    "        Returns:\n",
    "            early_stop (bool): Whether to stop the training.\n",
    "        \"\"\"\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94109e4a",
   "metadata": {},
   "source": [
    "## Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctRUL(hi, RUL):\n",
    "    if hi == 1:\n",
    "      return -1\n",
    "    else:\n",
    "      return RUL\n",
    "def correctMaxRUL(hi, RUL,MAX_RUL):\n",
    "    if hi == 1:\n",
    "      return MAX_RUL\n",
    "    else:\n",
    "      return RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_true_rul(log_y_hat_test, unit_sel, Unit_test, C_test, rul_test):\n",
    "    \"\"\"\n",
    "    Plots the predicted and true remaining useful life (RUL) for a given set of test data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_y_hat_test : numpy.ndarray\n",
    "        Logarithm of the predicted RUL for each cycle of each unit in the test data.\n",
    "    unit_sel : list\n",
    "        List of units to include in the plot.\n",
    "    Unit_test : numpy.ndarray\n",
    "        Array containing the unit numbers for each cycle in the test data.\n",
    "    C_test : numpy.ndarray\n",
    "        Array containing the cycle numbers for each cycle in the test data.\n",
    "    rul_test : numpy.ndarray\n",
    "        Array containing the true RUL for each cycle in the test data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i in range(len(log_y_hat_test)):\n",
    "        fig = plt.figure(figsize=(9, 7))\n",
    "        leg = []\n",
    "        \n",
    "        # Plot predicted RUL\n",
    "        for j in unit_sel:\n",
    "            y_hat_mean, y_hat_max, y_hat_min = [], [], []\n",
    "            unit = Unit_test == j\n",
    "            c_test = np.sort(C_test[unit])-1\n",
    "            idx = np.argsort(C_test[unit])\n",
    "            y_hat_test = log_y_hat_test[i][unit]\n",
    "            y_hat_test_sorted = y_hat_test[idx]\n",
    "            for k in np.unique(c_test):\n",
    "                y_hat_mean.append(np.mean(y_hat_test_sorted[c_test == k]))\n",
    "                y_hat_max.append(np.max(y_hat_test_sorted[c_test == k]))\n",
    "                y_hat_min.append(np.min(y_hat_test_sorted[c_test == k]))\n",
    "            y_hat_mean = np.array(y_hat_mean, dtype=np.float64)\n",
    "            y_hat_max = np.array(y_hat_max, dtype=np.float64)\n",
    "            y_hat_min = np.array(y_hat_min, dtype=np.float64)\n",
    "            plt.plot(np.unique(c_test), y_hat_mean, 'o', alpha=0.7, markersize=5)\n",
    "            plt.fill_between(np.unique(c_test), y_hat_min, y_hat_max, alpha=0.3)\n",
    "        # Plot true RUL\n",
    "        plt.gca().set_prop_cycle(None)\n",
    "        for j in unit_sel:        \n",
    "            unit = Unit_test == j  \n",
    "            c_test_unique = np.unique(np.sort(C_test[unit])-1)\n",
    "            rul_test_unique = np.unique(rul_test[unit])\n",
    "            rul_test_unique=np.append(rul_test_unique,np.zeros(len(c_test_unique)-len(rul_test_unique))+np.max(rul_test[unit]))\n",
    "            plt.plot(c_test_unique, rul_test_unique[::-1], alpha=0.7)\n",
    "            leg.append('Prediction-Traj.' + str(j))           \n",
    "            leg.append('$RUL$-Traj.' + str(j))\n",
    "        plt.legend(leg, loc='upper right')\n",
    "        plt.ylabel(r'$Prediction$ & $RUL$ [cycles]')\n",
    "        plt.xlabel('Time [cycles]')\n",
    "        plt.ylim(top=90)\n",
    "        plt.ylim(bottom=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78102e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(input_data, sequence_length, stride = 1, option = None):\n",
    "    \"\"\"\n",
    "     \n",
    "    \"\"\"\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(0,len(input_data),stride):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + sequence_length\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(input_data):\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        if option=='last':\n",
    "            seq_x = input_data[end_ix-1, :]\n",
    "        elif option=='next':\n",
    "            seq_x = input_data[end_ix, :]\n",
    "        else:\n",
    "            seq_x = input_data[i:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    \n",
    "    return np.array(X)\n",
    "def sequence_generator(input_data, units, cycles, sequence_length=10,stride = 1, option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        c_mask = cycles[mask]\n",
    "        x_unit = input_data[mask]\n",
    "        for j in np.unique(c_mask):\n",
    "            mask = np.ravel(c_mask==j)\n",
    "            seq_x_u = split_sequences(x_unit[mask],sequence_length, stride, option)\n",
    "            X.append(seq_x_u)\n",
    "            unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "            c_num.extend(np.ones(len(seq_x_u),dtype = int)*j)\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.array(c_num).reshape(-1,1)\n",
    "\n",
    "\n",
    "def sequence_generator_per_unit(input_data, units, cycles, sequence_length=10, stride =1,option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        x_unit = input_data[mask]\n",
    "        seq_x_u = split_sequences(x_unit,sequence_length, stride, option)\n",
    "        X.append(seq_x_u)\n",
    "        unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "        c_num.append(split_sequences(cycles[mask],sequence_length, stride, option))\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.vstack(c_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a89c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDistribution(NODES):\n",
    "    \"\"\"\n",
    "        This function is used to distribute the indexes of Training and Eval datasets\n",
    "    \"\"\"\n",
    "    global SHUFFLE_IDX\n",
    "    global TRAINING_IDX\n",
    "    global VAL_IDX\n",
    "    global M_LABELS_TRAIN\n",
    "    global M_LABELS_EVAL\n",
    "    global MAX_DATASET_SIZE_TRAIN\n",
    "    global MAX_DATASET_SIZE_EVAL\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    global PIECE_WISE\n",
    "    global CYCLE_CORRECTION\n",
    "    global PCA\n",
    "    global PCA_CORRECTION\n",
    "    \n",
    "    for FC in range(0, NODES):\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_dev'+\".h5\", 'r') as hdf:\n",
    "            Y_train = np.array(hdf.get('Y_dev'), dtype='float32')\n",
    "            A_train = np.array(hdf.get('A_dev'), dtype='float32')\n",
    "            if PCA_CORRECTION==True:\n",
    "                if PCA_W==True:\n",
    "                    W_train = np.array(hdf.get('W_dev'), dtype='float32')\n",
    "                    W_train = W_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "                else:\n",
    "                    X_train = np.array(hdf.get('X_dev'), dtype='float32')\n",
    "                    X_train = X_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "            \n",
    "            Y_train = Y_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "            A_train = A_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "            \n",
    "            units_train=A_train[:,0].reshape(-1,1)\n",
    "            cycles_train=A_train[:,1].reshape(-1,1)\n",
    "            hi_train = A_train[:,-1]\n",
    "            \n",
    "            if PCA_CORRECTION==True:\n",
    "                if PCA_W==True:\n",
    "                    W_train=PCA.transform(W_train)\n",
    "                    PCA_windows,_,_=sequence_generator_per_unit(W_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "                else:\n",
    "                    X_train=PCA.transform(X_train)\n",
    "                    PCA_windows,_,_=sequence_generator_per_unit(X_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "                \n",
    "            if CYCLE_CORRECTION==True:\n",
    "                df = pd.DataFrame(data={'unit': list(itertools.chain.from_iterable(units_train)), 'cycle': list(itertools.chain.from_iterable(cycles_train))})\n",
    "                gb1 = df.groupby(['unit'])['cycle'].transform(lambda x: np.max(x))\n",
    "                df['max_cycle'] = gb1\n",
    "                df['cycle'] = 100-df['max_cycle']+df['cycle']\n",
    "                noise = np.random.normal(-0.0001, 0.0001, len(df['cycle']))\n",
    "                df['cycle'] = df['cycle']+noise\n",
    "                cycles_train_aux=df['cycle'].to_numpy().reshape(-1, 1)\n",
    "            \n",
    "            if PIECE_WISE==True:\n",
    "                df_hs_unit_train = DataFrame({'unit': units_train.reshape(-1).astype(int),'RUL': Y_train.reshape(-1), 'hi': hi_train.reshape(-1)})\n",
    "                df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "                pd_aux=DataFrame(df_hs_unit_train.groupby('unit')['RUL'].max()).reset_index()\n",
    "                df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "                Y_train=df_hs_unit_train['RUL'].to_numpy().reshape(len(df_hs_unit_train),1)\n",
    "    \n",
    "            # Create Windows for Labels\n",
    "            Y_train=Y_train.reshape(-1)\n",
    "            Y_train=Y_train.reshape(len(Y_train),1)\n",
    "            Y_windows,_,_=sequence_generator_per_unit(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            \n",
    "            if CYCLE_CORRECTION==True:\n",
    "                C_windows,_,_=sequence_generator_per_unit(cycles_train_aux,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "                if PCA_CORRECTION==True:\n",
    "                    Y_windows=np.concatenate((Y_windows, C_windows, PCA_windows), axis=1)\n",
    "                else:\n",
    "                    Y_windows=np.concatenate((Y_windows, C_windows), axis=1)\n",
    "                    \n",
    "            ids = [*range(0,Y_windows.shape[0])]\n",
    "            random.shuffle(ids)\n",
    "            SHUFFLE_IDX[FC] = ids\n",
    "            TRAINING_IDX[FC] = ids[:int(len(ids)*0.85)]\n",
    "            VAL_IDX[FC] = ids[int(len(ids)*0.85)+1:int(len(ids))]\n",
    "            \n",
    "            M_LABELS_TRAIN[FC] = Y_windows[TRAINING_IDX[FC]]\n",
    "            M_LABELS_EVAL[FC] = Y_windows[VAL_IDX[FC]]\n",
    "            \n",
    "            if (MAX_DATASET_SIZE_TRAIN<len(Y_windows[TRAINING_IDX[FC]])):\n",
    "                MAX_DATASET_SIZE_TRAIN=len(Y_windows[TRAINING_IDX[FC]])\n",
    "            if (MAX_DATASET_SIZE_EVAL<len(Y_windows[VAL_IDX[FC]])):\n",
    "                MAX_DATASET_SIZE_EVAL=len(Y_windows[VAL_IDX[FC]])\n",
    "    Y_windows = []\n",
    "    A_train = []\n",
    "    Y_train = []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDistribution(NODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2e806",
   "metadata": {},
   "source": [
    "## Splitting data by Flight Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02443a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Scaler Variables\n",
    "SCALER_X = StandardScaler()\n",
    "SCALER_W = StandardScaler()\n",
    "SCALER_Y = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe14b2",
   "metadata": {},
   "source": [
    "### Distribute Indexes (Training and Eval) and Labeles per Replica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CYCLE_CORRECTION==False:\n",
    "    M_TRAIN=np.empty((NODES,MAX_DATASET_SIZE_TRAIN,1))\n",
    "else:\n",
    "    M_TRAIN=np.empty((NODES,MAX_DATASET_SIZE_TRAIN,2))\n",
    "M_TRAIN[:] = np.nan\n",
    "for x in range(0,NODES):\n",
    "    M_TRAIN[x,:len(M_LABELS_TRAIN[x])]=[list(i) for i in M_LABELS_TRAIN[x] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if CYCLE_CORRECTION==False:\n",
    "    M_EVAL=np.empty((NODES, MAX_DATASET_SIZE_EVAL,1))\n",
    "else:\n",
    "    M_EVAL=np.empty((NODES,MAX_DATASET_SIZE_EVAL,2))\n",
    "    \n",
    "M_EVAL[:] = np.nan\n",
    "for x in range(0,NODES):\n",
    "    M_EVAL[x,:len(M_LABELS_EVAL[x])]=[list(i) for i in M_LABELS_EVAL[x] ]     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0fae5",
   "metadata": {},
   "source": [
    "# Label Synchornization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_nan(arr):\n",
    "    nan_index = np.argmax(np.isnan(arr))\n",
    "    return nan_index if np.isnan(arr[nan_index]) else None\n",
    "\n",
    "def labelSynchronization(M, localLab):\n",
    "    numWorkers = len(list(zip(*M)))                     # Number of columns\n",
    "    ID=list(np.unique(M[:,localLab]))                   # List of unique Labels\n",
    "    synchronizedLabels=np.zeros(M.shape,dtype=int)      \n",
    "    synchronizedLabels[:,localLab]=range(0,len(M))    # Column of localLab with same indexes\n",
    "    if(numWorkers>1):\n",
    "        labelsAndCounters=np.zeros((len(ID),numWorkers))      # Matrix of Labels and Counters with zeros\n",
    "        for worker in range(0,numWorkers):          # for each worker            \n",
    "            for _id in range(0,len(ID)):            # AND for each ID                    \n",
    "                labelsAndCounters[_id,worker]=operator.countOf(M[:,worker].tolist(),ID[_id]) # Count the number of instances\n",
    "        for worker in range(0,numWorkers):          # for each worker different fotm local\n",
    "            if worker!=localLab:                    \n",
    "                counterPerLabel=np.zeros((len(ID),numWorkers), dtype=int) # Counter Per ID Matrix\n",
    "                for _id in range(0, len(ID)):                             # For each ID\n",
    "                    condition = lambda x: x == ID[_id]\n",
    "                    elements = [index for index, element in enumerate(M[:, worker]) if condition(element)]\n",
    "                    for row in range(0, len(M)):\n",
    "                        if (M[row,localLab]== ID[_id]):\n",
    "                            synchronizedLabels[row,worker]=elements[counterPerLabel[_id,numWorkers-1]]\n",
    "                            counterPerLabel[_id,numWorkers-1] += 1\n",
    "                            if counterPerLabel[_id,numWorkers-1]>=len(elements):\n",
    "                                counterPerLabel[_id,numWorkers-1]=0\n",
    "    return synchronizedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    def square(x):\n",
    "        return x**2\n",
    "\n",
    "    def sqrt(x):\n",
    "        return x**0.5\n",
    "    if CYCLE_CORRECTION==True:\n",
    "        return sqrt(sum(square(p1 - p2) for p1, p2 in zip(point1, point2)))\n",
    "    else:\n",
    "        return np.abs(point1-point2)\n",
    "\n",
    "def alternative_closest_index_in_list2(closest_index, min_distance, value1, list2):\n",
    "    for i, value2 in enumerate(list2):\n",
    "        if ~np.isnan(value1).any() and ~np.isnan(value2).any():\n",
    "            distance = euclidean_distance(value1, value2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_index = i            \n",
    "    return closest_index, min_distance\n",
    "    \n",
    "def closest_index_in_list2(count, closest_index, min_distance, closest_indices, j,value1,list2):\n",
    "    for i, value2 in enumerate(list2):\n",
    "            if ~np.isnan(value1).any() and ~np.isnan(value2).any():\n",
    "                distance = euclidean_distance(value1, value2)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_index = i\n",
    "                if CYCLE_CORRECTION==False:\n",
    "                    if i==len(list2)-1:\n",
    "                        try:\n",
    "                            if len(closest_indices)> 0:\n",
    "                                # count = 0\n",
    "                                index = closest_indices.index(closest_index)\n",
    "                                if min_distance >= alternative_closest_index_in_list2(closest_index, min_distance, value1, np.delete(list2, closest_index, axis=0))[1]:\n",
    "                                    list2 = np.delete(list2, closest_index, axis=0)\n",
    "                                    closest_index=closest_index_in_list2(count+1, closest_index, float('inf'), closest_indices, j,value1,list2)[0]+1\n",
    "                        except ValueError:\n",
    "                            1\n",
    "    return closest_index, min_distance\n",
    "\n",
    "def closest_index_for_eachND(list1, list2):\n",
    "    closest_indices = []\n",
    "    min_distance = float('inf')\n",
    "    for j, value1 in enumerate(list1):\n",
    "        closest_index = None\n",
    "        count=0\n",
    "        closest_index,_ = closest_index_in_list2(count,closest_index,min_distance,closest_indices,j,value1,list2)\n",
    "        closest_indices.append(closest_index)\n",
    "\n",
    "    return closest_indices\n",
    "\n",
    "def closest_index_matrix_ND(M_labels,NODE):\n",
    "    for labels_node in range(0,M_labels.shape[0]):\n",
    "        #print(M_labels[:][0])\n",
    "        if labels_node==0:\n",
    "            if labels_node==NODE:\n",
    "                M=np.array(range(0,len(M_labels[:][NODE])))\n",
    "            else:\n",
    "                match = np.array(closest_index_for_eachND(M_labels[:][NODE], M_labels[:][labels_node])) \n",
    "                M = match\n",
    "        if labels_node!=0:\n",
    "            if labels_node == NODE:\n",
    "                M = np.vstack((M,np.array(range(0,len(M_labels[:][NODE])))))\n",
    "            else:\n",
    "                match = np.array(closest_index_for_eachND(M_labels[:][NODE], M_labels[:][labels_node]))\n",
    "                M =  np.vstack((M, match))\n",
    "    return np.transpose(M)\n",
    "    \n",
    "# Example 2D\n",
    "M_labels = np.array([[[0.2], \n",
    "                      [0.6], \n",
    "                      [0.2],\n",
    "                      [0.2]],\n",
    "                     [[0.2],\n",
    "                      [1],\n",
    "                      [0.2], \n",
    "                      [np.nan],],\n",
    "                     [[0.4],\n",
    "                      [0.2],\n",
    "                      [0.2],\n",
    "                      [0.2]]])\n",
    "\n",
    "M = closest_index_matrix_ND(M_labels,NODE)\n",
    "print(M_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c475389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineDistance(a, b):\n",
    "    return spatial.distance.cosine(a, b)\n",
    "def replaceCycle(diff, original):\n",
    "    return [original[0], diff]\n",
    "def find_closest_value(value, column2):\n",
    "    closest_value = column2.iloc[(column2 - value).abs().idxmin()]\n",
    "    return closest_value\n",
    "def find_closest_idx(value, column2):\n",
    "        euclidean_distance = lambda x: np.linalg.norm(np.array(x) - np.array(value))\n",
    "        return column2.iloc[column2.apply(lambda row: euclidean_distance(row)).idxmin()]\n",
    "def closestIndexWithCosineDifference(M_labels, NODE):\n",
    "    data={}\n",
    "    for column in range(0,len(M_labels)):\n",
    "        data[str(column)]= M_labels[column]\n",
    "    df=pd.DataFrame(data)\n",
    "    NODES = len(df.columns)\n",
    "    for column in range(0,len(M_labels)):\n",
    "        df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]= df.apply(lambda row: cosineDistance(row[str(NODE)], row[str(column)]), axis=1)\n",
    "        if column!=NODE:\n",
    "            df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]= df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]    \n",
    "        df[\"aux\"+str(column)] = df[str(column)].str[0]+1j*abs(df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]+ np.random.normal(-0.0001, 0.0001, len(df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)])))\n",
    "        #df[str(column)]= df.apply(lambda row: replaceCycle(row[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)], row[str(column)]), axis=1)\n",
    "        df.drop(\"Cossine_diff\"+str(NODE)+\"_\"+str(column), axis=1, inplace=True)\n",
    "    \n",
    "    for column in range(0,NODES):\n",
    "            if column==NODE:\n",
    "                df['IndexIn'+str(column)] = df.index\n",
    "            else:\n",
    "                df['IndexIn'+str(column)] = df[\"aux\"+str(NODE)].apply(find_closest_value, column2=df[\"aux\"+str(column)]).apply(lambda x: df[\"aux\"+str(column)][df[\"aux\"+str(column)] == x].index[0])\n",
    "    for column in range(0,NODES):\n",
    "        df.drop(str(column), axis=1, inplace=True)\n",
    "        df.drop(\"aux\"+str(column), axis=1, inplace=True)  \n",
    "    return df.to_numpy()\n",
    "\n",
    "def closestIndexWithCosineDifferenceAndPCA(M, NODE):\n",
    "    M_labels=M[:,:,:2].tolist()\n",
    "    M_PCA=M[:,:,2:]*0.01\n",
    "    data={}\n",
    "    for column in range(0,len(M_labels)):\n",
    "        data[str(column)]= M_labels[column]\n",
    "    df=pd.DataFrame(data)\n",
    "    NODES = len(df.columns)\n",
    "    for column in range(0,len(M_labels)):\n",
    "        df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]= df.apply(lambda row: cosineDistance(row[str(NODE)], row[str(column)]), axis=1)\n",
    "        if column!=NODE:\n",
    "            df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]= df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]    \n",
    "        df[\"aux\"+str(column)] = df[str(column)].str[0]+1j*abs(df[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)]+ M_PCA[column].reshape(-1))\n",
    "        #df[str(column)]= df.apply(lambda row: replaceCycle(row[\"Cossine_diff\"+str(NODE)+\"_\"+str(column)], row[str(column)]), axis=1)\n",
    "        df.drop(\"Cossine_diff\"+str(NODE)+\"_\"+str(column), axis=1, inplace=True)\n",
    "    \n",
    "    for column in range(0,NODES):\n",
    "            if column==NODE:\n",
    "                df['IndexIn'+str(column)] = df.index\n",
    "            else:\n",
    "                df['IndexIn'+str(column)] = df[\"aux\"+str(NODE)].apply(find_closest_value, column2=df[\"aux\"+str(column)]).apply(lambda x: df[\"aux\"+str(column)][df[\"aux\"+str(column)] == x].index[0])\n",
    "    for column in range(0,NODES):\n",
    "        df.drop(str(column), axis=1, inplace=True)\n",
    "        df.drop(\"aux\"+str(column), axis=1, inplace=True)  \n",
    "    return df.to_numpy()\n",
    "\n",
    "M_labels=np.array([[[1.0, 2.0], [1, 2], [7, 8]],\n",
    "                  [[10, 10], [1, 2], [16, 17]],\n",
    "                  [[1,2], [1,2], [2,3]]]).tolist()\n",
    "M_labels_PCA=np.array([[[1.0, 2.0, 0.2], [1, 2, 0.3], [7, 8, 0.4]],\n",
    "                  [[10, 10, 0.1], [1, 2, 0.3], [16, 17, 0.5]],\n",
    "                  [[1,2,0.6], [1,2,0.2], [2,3,0.34]]])\n",
    "if PCA_CORRECTION==True:\n",
    "    print(closestIndexWithCosineDifferenceAndPCA(M_labels_PCA, 0))\n",
    "else:\n",
    "    print(closestIndexWithCosineDifference(M_labels, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59873e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstNaN_Train=find_first_nan(M_TRAIN[NODE])\n",
    "#firstNaN_Eval=find_first_nan(M_EVAL[NODE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for J in range(0,NODES):\n",
    "    while (find_first_nan(M_EVAL[J]) and find_first_nan(M_EVAL[J])<M_EVAL.shape[1]):\n",
    "        M_EVAL[J,find_first_nan(M_EVAL[J]):,:]=M_EVAL[J,:M_EVAL.shape[1]-find_first_nan(M_EVAL[J]),:]\n",
    "for J in range(0,NODES):\n",
    "    while (find_first_nan(M_TRAIN[J]) and find_first_nan(M_TRAIN[J])<M_TRAIN.shape[1]):\n",
    "        M_TRAIN[J,find_first_nan(M_TRAIN[J]):,:]=M_TRAIN[J,:M_TRAIN.shape[1]-find_first_nan(M_TRAIN[J]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d07a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M_TRAIN=M_TRAIN[:,:firstNaN_Train,:]\n",
    "#M_EVAL=M_EVAL[:,:firstNaN_Eval,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e67724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def find_closest_value(value, column2):\n",
    "    closest_value = column2.iloc[(column2 - value).abs().idxmin()]\n",
    "    return closest_value\n",
    "\n",
    "def closestIndex(M_labels, NODE):\n",
    "    df = pd.DataFrame()\n",
    "    for column in range(0,M_labels.shape[0]):\n",
    "        df[str(column)]= M_labels[column]\n",
    "    NODES = len(df.columns)\n",
    "    for column in range(0,NODES):\n",
    "        if column==NODE:\n",
    "            df['IndexIn'+str(column)] = df.index\n",
    "        else:\n",
    "            df['IndexIn'+str(column)] = df[str(NODE)].apply(find_closest_value, column2=df[str(column)]).apply(lambda x: df[str(column)][df[str(column)] == x].index[0])\n",
    "    for column in range(0,NODES):\n",
    "        df.drop(str(column), axis=1, inplace=True)\n",
    "    return df.to_numpy()\n",
    "\n",
    "# Example usage:\n",
    "def euclidean_distance(point1, point2):\n",
    "    def square(x):\n",
    "        return x**2\n",
    "\n",
    "    def sqrt(x):\n",
    "        return x**0.5\n",
    "    \n",
    "    return sqrt(sum(square(p1 - p2) for p1, p2 in zip(point1, point2)))\n",
    "\n",
    "\n",
    "def alternative_closest_index_in_list2(closest_index, min_distance, value1, list2):\n",
    "    for i, value2 in enumerate(list2):\n",
    "        if ~np.isnan(value1).any() and ~np.isnan(value2).any():\n",
    "            distance = euclidean_distance(value1, value2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_index = i            \n",
    "    return closest_index, min_distance\n",
    "    \n",
    "def closest_index_in_list2(count, closest_index, min_distance, closest_indices, j,value1,list2):\n",
    "    for i, value2 in enumerate(list2):\n",
    "            if ~np.isnan(value1).any() and ~np.isnan(value2).any():\n",
    "                distance = euclidean_distance(value1, value2)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_index = i\n",
    "                if CYCLE_CORRECTION==False:\n",
    "                    if i==len(list2)-1:\n",
    "                        try:\n",
    "                            if len(closest_indices)> 0:\n",
    "                                # count = 0\n",
    "                                index = closest_indices.index(closest_index)\n",
    "                                if min_distance >= alternative_closest_index_in_list2(closest_index, min_distance, value1, np.delete(list2, closest_index, axis=0))[1]:\n",
    "                                    list2 = np.delete(list2, closest_index, axis=0)\n",
    "                                    closest_index=closest_index_in_list2(count+1, closest_index, float('inf'), closest_indices, j,value1,list2)[0]+1\n",
    "                        except ValueError:\n",
    "                            1\n",
    "    return closest_index, min_distance\n",
    "\n",
    "def closest_index_for_eachND(list1, list2):\n",
    "    closest_indices = []\n",
    "    min_distance = float('inf')\n",
    "    for j, value1 in enumerate(list1):\n",
    "        closest_index = None\n",
    "        count=0\n",
    "        closest_index,_ = closest_index_in_list2(count,closest_index,min_distance,closest_indices,j,value1,list2)\n",
    "        closest_indices.append(closest_index)\n",
    "\n",
    "    return closest_indices\n",
    "\n",
    "def closest_index_matrix_ND(M_labels,NODE):\n",
    "    for labels_node in range(0,M_labels.shape[0]):\n",
    "        #print(M_labels[:][0])\n",
    "        if labels_node==0:\n",
    "            if labels_node==NODE:\n",
    "                M=np.array(range(0,len(M_labels[:][NODE])))\n",
    "            else:\n",
    "                print(\"0\")\n",
    "                match = np.array(closest_index_for_eachND(M_labels[:][NODE], M_labels[:][labels_node])) \n",
    "                M = match\n",
    "        if labels_node!=0:\n",
    "            if labels_node == NODE:\n",
    "                M = np.vstack((M,np.array(range(0,len(M_labels[:][NODE])))))\n",
    "            else:\n",
    "                print(\"!0\")\n",
    "                match = np.array(closest_index_for_eachND(M_labels[:][NODE], M_labels[:][labels_node]))\n",
    "                M =  np.vstack((M, match))\n",
    "    return np.transpose(M)\n",
    "    \n",
    "# Example 2D\n",
    "M_labels = np.array([[2,6,2,2],[2,1,2,2],[4,2,3,6]])\n",
    "M = closestIndex(M_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CYCLE_CORRECTION==False:\n",
    "    M_EVAL=M_EVAL.reshape(M_EVAL.shape[0],M_EVAL.shape[1])\n",
    "    SYNCHRONIZED_LABELS_EVAL=closestIndex(M_EVAL,NODE)\n",
    "else:\n",
    "    if PCA_CORRECTION==True:\n",
    "        SYNCHRONIZED_LABELS_EVAL=closestIndexWithCosineDifferenceAndPCA(M_EVAL,NODE)\n",
    "    else:\n",
    "        SYNCHRONIZED_LABELS_EVAL=closestIndexWithCosineDifference(M_EVAL.tolist(),NODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CYCLE_CORRECTION==False:\n",
    "    M_TRAIN=M_TRAIN.reshape(M_TRAIN.shape[0],M_TRAIN.shape[1])\n",
    "    SYNCHRONIZED_LABELS_TRAIN=closestIndex(M_TRAIN,NODE)\n",
    "else:\n",
    "    if PCA_CORRECTION==True:\n",
    "        SYNCHRONIZED_LABELS_TRAIN=closestIndexWithCosineDifferenceAndPCA(M_TRAIN,NODE)\n",
    "    else:\n",
    "        SYNCHRONIZED_LABELS_TRAIN=closestIndexWithCosineDifference(M_TRAIN.tolist(),NODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_3d_array(arr, batch_size):\n",
    "    \"\"\"\n",
    "    Batch a 3D NumPy array into smaller chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: 3D NumPy array\n",
    "    - batch_size: Size of each batch along the first dimension\n",
    "\n",
    "    Returns:\n",
    "    - NumPy array of batches, where each batch is a 3D NumPy array\n",
    "    \"\"\"\n",
    "    shape = arr.shape\n",
    "    if len(shape) != 3:\n",
    "        raise ValueError(\"Input array must be 3D.\")\n",
    "\n",
    "    num_batches = (shape[0] + batch_size - 1) // batch_size\n",
    "    batches = np.empty((num_batches, batch_size, shape[1], shape[2]), dtype=arr.dtype)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, shape[0])\n",
    "        batch = arr[start:end, :, :]\n",
    "        batches[i, :end - start, :, :] = batch\n",
    "\n",
    "    return batches\n",
    "\n",
    "def batch_2d_array(arr, batch_size):\n",
    "    \"\"\"\n",
    "    Batch a 2D NumPy array into smaller chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: 2D NumPy array\n",
    "    - batch_size: Size of each batch\n",
    "\n",
    "    Returns:\n",
    "    - NumPy array of batches, where each batch is a 2D NumPy array\n",
    "    \"\"\"\n",
    "    shape = arr.shape\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Input array must be 2D.\")\n",
    "\n",
    "    num_batches = (shape[0] + batch_size - 1) // batch_size\n",
    "    batches = np.empty((num_batches, batch_size, shape[1]), dtype=arr.dtype)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, shape[0])\n",
    "        batch = arr[start:end, :]\n",
    "        batches[i, :end - start, :] = batch\n",
    "\n",
    "    return batches\n",
    "def batch_data_2d(test, batch_size):\n",
    "    m,n = test.shape\n",
    "    S = test.itemsize\n",
    "    if not batch_size:\n",
    "        batch_size = m\n",
    "    count_batches = m//batch_size\n",
    "    # Batches which can be covered fully\n",
    "    test_batches = as_strided(test, shape=(count_batches, batch_size, n), strides=(batch_size*n*S,n*S,S)).copy()\n",
    "    covered = count_batches*batch_size\n",
    "    if covered < m:\n",
    "        rest = test[covered:,:]\n",
    "        rm, rn = rest.shape\n",
    "        mismatch = batch_size - rm\n",
    "        last_batch = np.vstack((rest,np.zeros((mismatch,rn)))).reshape(1,-1,n)\n",
    "        return np.vstack((test_batches,last_batch))\n",
    "    return test_batches\n",
    "\n",
    "def batch_data_3d(test, batch_size):\n",
    "    m,n,p = test.shape\n",
    "    S = test.itemsize\n",
    "    if not batch_size:\n",
    "        batch_size = m\n",
    "    count_batches = m//batch_size\n",
    "    # Batches which can be covered fully\n",
    "    test_batches = as_strided(test, shape=(count_batches, batch_size, n, p), strides=(batch_size*n*p*S,n*p*S,p*S,S)).copy()\n",
    "    covered = count_batches*batch_size\n",
    "    if covered < m:\n",
    "        rest = test[covered:,:,:]\n",
    "        rm, rn, rp = rest.shape\n",
    "        mismatch = batch_size - rm\n",
    "        last_batch = np.vstack((rest,np.zeros((mismatch,rn,rp)))).reshape(1,-1,n,p)\n",
    "        return np.vstack((test_batches,last_batch))\n",
    "    return test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_remove_first_batch(my_array, axis=0):\n",
    "    extracted_batch = my_array[0]\n",
    "\n",
    "    # Remove the first batch from the original array along the specified axis\n",
    "    my_array = np.delete(my_array,0, axis=axis)\n",
    "\n",
    "    return extracted_batch, my_array\n",
    "\n",
    "def fn_data_partition_train(NODES, evaluation):\n",
    "    \n",
    "    # Global Variables\n",
    "    global SCALER_X\n",
    "    global SCALER_W\n",
    "    global SCALER_Y\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    global BATCH_SIZE\n",
    "    global TRAINING_IDX\n",
    "    global VAL_IDX\n",
    "    global SHUFFLE_IDX\n",
    "    \n",
    "    distributedData=[]\n",
    "    for FC in range(0, NODES):\n",
    "        DATA_REPLICA_ID = []\n",
    "        # Load data DEV\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_dev'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_train = np.array(hdf.get('W_dev'), dtype='float32')             # W\n",
    "            X_s_train = np.array(hdf.get('X_s_dev'), dtype='float32')         # X_s\n",
    "            Y_train = np.array(hdf.get('Y_dev'), dtype='float32')             # RUL                  \n",
    "            A_train = np.array(hdf.get('A_dev'), dtype='float32')\n",
    "            \n",
    "            W_train = W_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "            X_s_train = X_s_train[::DOWNSAMPLING_STEP[FC],:] \n",
    "            Y_train = Y_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "            A_train = A_train[::DOWNSAMPLING_STEP[FC],:]\n",
    "\n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "            # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "            \n",
    "\n",
    "            units_train=A_train[:,0].reshape(-1,1)\n",
    "            cycles_train=A_train[:,1].reshape(-1,1)\n",
    "            hi_train = A_train[:,-1]\n",
    "                \n",
    "            X_s_train = SCALER_X.fit_transform(X_s_train)\n",
    "            W_train = SCALER_W.fit_transform(W_train)\n",
    "            Y_train = SCALER_Y.fit_transform(Y_train)\n",
    "            \n",
    "            X_windows, _, _=sequence_generator_per_unit(X_s_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            X_s_train=[]\n",
    "            X_windows=X_windows[SHUFFLE_IDX[FC]]\n",
    "            if evaluation==True:\n",
    "                #X_=X_windows[int(len(X_windows)*0.85)+1:int(len(X_windows))-1]\n",
    "                X_=X_windows[SYNCHRONIZED_LABELS_EVAL[:,FC]]\n",
    "                X_windows=[]\n",
    "                X_=X_[:,:,:]\n",
    "            else:\n",
    "                #X_=X_windows[0:int(len(X_windows)*0.85)]\n",
    "                X_=X_windows[SYNCHRONIZED_LABELS_TRAIN[:,FC]]\n",
    "                X_windows=[]\n",
    "                X_=X_[:,:,:]\n",
    "                \n",
    "            W_windows,_,_=sequence_generator_per_unit(W_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            W_train=[]\n",
    "            W_windows=W_windows[SHUFFLE_IDX[FC]]\n",
    "            if evaluation==True:\n",
    "                #W_=W_windows[int(len(W_windows)*0.85)+1:int(len(W_windows))-1]\n",
    "                W_=W_windows[SYNCHRONIZED_LABELS_EVAL[:,FC]]\n",
    "                W_windows=[]\n",
    "                W_=W_[:,:,:]\n",
    "            else:\n",
    "                #W_=W_windows[0:int(len(W_windows)*0.85)]\n",
    "                W_=W_windows[SYNCHRONIZED_LABELS_TRAIN[:,FC]]\n",
    "                W_windows=[]\n",
    "                W_=W_[:,:,:]\n",
    "            \n",
    "            if PIECE_WISE==True:\n",
    "                df_hs_unit_train = DataFrame({'unit': units_train.reshape(-1).astype(int),'RUL': Y_train.reshape(-1), 'hi': hi_train.reshape(-1)})\n",
    "                df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "                pd_aux=DataFrame(df_hs_unit_train.groupby('unit')['RUL'].max()).reset_index()\n",
    "                df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "                Y_train=df_hs_unit_train['RUL'].to_numpy().reshape(len(df_hs_unit_train),1)\n",
    "                \n",
    "            Y_windows,_,_=sequence_generator_per_unit(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            Y_train=[]\n",
    "            units_train=[]\n",
    "            cycles_train=[]\n",
    "            Y_windows=Y_windows[SHUFFLE_IDX[FC]]\n",
    "            \n",
    "            if evaluation==True:\n",
    "                #y_=Y_windows[int(len(Y_windows)*0.85)+1:int(len(Y_windows))-1]\n",
    "                y_=Y_windows[SYNCHRONIZED_LABELS_EVAL[:,FC]]\n",
    "                Y_windows=[]\n",
    "                y_=y_[:]\n",
    "            else:\n",
    "                #y_=Y_windows[0:int(len(Y_windows)*0.85)]\n",
    "                y_=Y_windows[SYNCHRONIZED_LABELS_TRAIN[:,FC]]\n",
    "                Y_windows=[]\n",
    "                y_=y_[:]\n",
    "                \n",
    "            X_ = batch_data_3d(X_,BATCH_SIZE)\n",
    "            W_ = batch_data_3d(W_,BATCH_SIZE)\n",
    "            y_= batch_data_2d(y_,BATCH_SIZE)\n",
    "        for i in range(0,len(y_)):\n",
    "            DATA_REPLICA_ID.append(((tf.convert_to_tensor(X_[i], dtype=tf.float32),tf.convert_to_tensor(W_[i], dtype=tf.float32)),tf.convert_to_tensor(y_[i], dtype=tf.float32)))\n",
    "        distributedData.append(DATA_REPLICA_ID)\n",
    "    return distributedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5da2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributedDataTrain = fn_data_partition_train(NODES,False)\n",
    "SYNCHRONIZED_LABELS_TRAIN=[]\n",
    "M_TRAIN=[]\n",
    "TRAINING_IDX = {}\n",
    "def value_fn_train(ctx):\n",
    "    return distributedDataTrain[ctx.replica_id_in_sync_group]\n",
    "distributed_values_train = strategy.experimental_distribute_values_from_function(value_fn_train)\n",
    "distributedDataTrain=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributedDataEval = fn_data_partition_train(NODES,True)\n",
    "SYNCHRONIZED_LABELS_EVAL=[]\n",
    "M_EVAL=[]\n",
    "VAL_IDX = {}\n",
    "def value_fn_eval(ctx):\n",
    "    return distributedDataEval[ctx.replica_id_in_sync_group]\n",
    "distributed_values_eval = strategy.experimental_distribute_values_from_function(value_fn_eval)\n",
    "distributedDataEval=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ae749",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUL MODEL\n",
    "\n",
    "def predictor(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_H_in=1,\n",
    "      feature_out_size=1,\n",
    "      activation='relu',\n",
    "      filter = [10,10,1],\n",
    "      filter_size = 10,\n",
    "      useH=True):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "    \n",
    "    if useH:\n",
    "      h_in = layers.Input(shape=(t,feature_H_in),name=\"H_in\")\n",
    "      # h_in = layers.Input(shape=(1,1),name=\"H_in\")\n",
    "      x = tf.concat([x_in,w_in, h_in],-1)\n",
    "    else: \n",
    "      x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    for i in filter:\n",
    "      x = layers.Conv1D(i,filter_size,1,padding='same',activation = activation)(x)\n",
    "      # x = layers.BatchNormalization()(x)\n",
    "      \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(50,activation = activation)(x)\n",
    "    y = layers.Dense(feature_out_size,activation = 'linear',name=\"predictions\")(y)\n",
    "\n",
    "    if useH:\n",
    "      model = models.Model([x_in,w_in,h_in], y)\n",
    "    else:\n",
    "      model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inception2D(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_out_size=1):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in,1),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in,1),name=\"W_in\")\n",
    "    \n",
    "\n",
    "    x = tf.concat([x_in,w_in],-2)\n",
    "      \n",
    "    layer_1 = tf.keras.layers.Conv2D(10, (3,3), padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_2 = tf.keras.layers.Conv2D(10, (5,5), padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_3 = tf.keras.layers.MaxPooling2D(3, strides=(1,1), padding='same')(x)\n",
    "    layer_3 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(layer_3)\n",
    "\n",
    "    mid_1 = tf.keras.layers.concatenate([layer_1, layer_2, layer_3], axis = 3)\n",
    "\n",
    "    ### 2nd Module\n",
    "    layer_4 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(mid_1)\n",
    "    layer_4 = tf.keras.layers.Conv2D(10, (3,3), padding='same', activation='relu')(layer_4)\n",
    "\n",
    "    layer_5 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(mid_1)\n",
    "    layer_5 = tf.keras.layers.Conv2D(10, (5,5), padding='same', activation='relu')(layer_5)\n",
    "\n",
    "    layer_6 = tf.keras.layers.MaxPooling2D(1, strides=(1,1), padding='same')(mid_1)\n",
    "    layer_6 = tf.keras.layers.Conv2D(10, (1,1), padding='same', activation='relu')(layer_6)\n",
    "\n",
    "    mid_2 = tf.keras.layers.concatenate([layer_4, layer_5, layer_6], axis = 2)\n",
    "\n",
    "    flat_1 = tf.keras.layers.Flatten()(mid_2)\n",
    "\n",
    "    drop = tf.keras.layers.Dropout(.5)(flat_1)\n",
    "\n",
    "    dense_1 = tf.keras.layers.Dense(256, activation='sigmoid')(drop)\n",
    "    y = tf.keras.layers.Dense(feature_out_size, activation='relu')(dense_1)\n",
    "\n",
    "    model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def inception1D(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_out_size=1):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "\n",
    "    x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    layer_1 = tf.keras.layers.Conv1D(10, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_2 = tf.keras.layers.Conv1D(10, 5, padding='same', activation='relu')(x)\n",
    "\n",
    "    layer_3 = tf.keras.layers.MaxPooling1D(3, strides=1, padding='same')(x)\n",
    "    layer_3 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(layer_3)\n",
    "\n",
    "    mid_1 = tf.keras.layers.concatenate([layer_1, layer_2, layer_3], axis = 2)\n",
    "\n",
    "    ### 2nd Module\n",
    "    layer_4 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(mid_1)\n",
    "    layer_4 = tf.keras.layers.Conv1D(10, 3, padding='same', activation='relu')(layer_4)\n",
    "\n",
    "    layer_5 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(mid_1)\n",
    "    layer_5 = tf.keras.layers.Conv1D(10, 5, padding='same', activation='relu')(layer_5)\n",
    "\n",
    "    layer_6 = tf.keras.layers.MaxPooling1D(1, strides=1, padding='same')(mid_1)\n",
    "    layer_6 = tf.keras.layers.Conv1D(10, 1, padding='same', activation='relu')(layer_6)\n",
    "\n",
    "    mid_2 = tf.keras.layers.concatenate([layer_4, layer_5, layer_6], axis = 2)\n",
    "\n",
    "    flat_1 = tf.keras.layers.Flatten()(mid_2)\n",
    "\n",
    "    drop = tf.keras.layers.Dropout(.5)(flat_1)\n",
    "\n",
    "    dense_1 = tf.keras.layers.Dense(256, activation='sigmoid')(drop)\n",
    "    y = tf.keras.layers.Dense(feature_out_size, activation='relu')(dense_1)\n",
    "\n",
    "    model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca57499",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "Recall that the loss function consists of one or two parts:\n",
    "\n",
    "  * The **prediction loss** measures how far off the model's predictions are from the training labels for a batch of training examples. It is computed for each labeled example and then reduced across the batch by computing the average value.\n",
    "  * Optionally, **regularization loss** terms can be added to the prediction loss, to steer the model away from overfitting the training data. A common choice is L2 regularization, which adds a small fixed multiple of the sum of squares of all model weights, independent of the number of examples. The model above uses L2 regularization to demonstrate its handling in the training loop below.\n",
    "\n",
    "For training on a single machine with a single GPU/CPU, this works as follows:\n",
    "\n",
    "  * The prediction loss is computed for each example in the batch, summed across the batch, and then divided by the batch size.\n",
    "  * The regularization loss is added to the prediction loss.\n",
    "  * The gradient of the total loss is computed w.r.t. each model weight, and the optimizer updates each model weight from the corresponding gradient.\n",
    "\n",
    "With `tf.distribute.Strategy`, the input batch is split between replicas.\n",
    "For example, let's say you have 4 GPUs, each with one replica of the model. One batch of 256 input examples is distributed evenly across the 4 replicas, so each replica gets a batch of size 64: We have `256 = 4*64`, or generally `GLOBAL_BATCH_SIZE = num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`.\n",
    "\n",
    "Each replica computes the loss from the training examples it gets and computes the gradients of the loss w.r.t. each model weight. The optimizer takes care that these **gradients are summed up across replicas** before using them to update the copies of the model weights on each replica.\n",
    "\n",
    "*So, how should the loss be calculated when using a `tf.distribute.Strategy`?*\n",
    "\n",
    "  * Each replica computes the prediction loss for all examples distributed to it, sums up the results and divides them by `num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`, or equivently, `GLOBAL_BATCH_SIZE`.\n",
    "  * Each replica compues the regularization loss(es) and divides them by\n",
    "  `num_replicas_in_sync`.\n",
    "\n",
    "Compared to non-distributed training, all per-replica loss terms are scaled down by a factor of `1/num_replicas_in_sync`. On the other hand, all loss terms -- or rather, their gradients -- are summed across that number of replicas before the optimizer applies them. In effect, the optimizer on each replica uses the same gradients as if a non-distributed computation with `GLOBAL_BATCH_SIZE` had happened. This is consistent with the distributed and undistributed behavior of Keras `Model.fit`. See the [Distributed training with Keras](./keras.ipynb) tutorial on how a larger gloabl batch size enables to scale up the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def compute_loss_batch(labels, predictions, model_losses):\n",
    "        per_example_loss = (labels - predictions)**2  # Sample error\n",
    "        loss = tf.math.sqrt(tf.nn.compute_average_loss(per_example_loss)) # Batch Error\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    eval_mae = tf.keras.metrics.MeanAbsoluteError()\n",
    "    train_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    eval_rmse = tf.keras.metrics.RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "    #model = predictor(t=50,useH=False)\n",
    "    model = inception1D(t=WINDOW_LEN)\n",
    "    optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=LEARNING_RATE)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32a1e5",
   "metadata": {},
   "source": [
    "## Auxiliar Funcions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c294995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_batch(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "        loss = compute_loss_batch(labels, predictions, model.losses) # Batch Error\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables),skip_gradients_aggregation=True)\n",
    "    train_rmse.update_state(labels, predictions)\n",
    "    return loss\n",
    "def train_step_sample(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "    return predictions\n",
    "\n",
    "def compute_loss_fedLabSync(inputs, collaborativePredictions):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "        loss = compute_loss_batch(labels, (collaborativePredictions*(COLLAB_LOSS_WEIGHT)+predictions*(1-COLLAB_LOSS_WEIGHT)), model.losses) # Batch Error\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_rmse.update_state(labels, (collaborativePredictions*(COLLAB_LOSS_WEIGHT)+predictions*(1-COLLAB_LOSS_WEIGHT)))\n",
    "    return loss\n",
    "    \n",
    "def test_step_batch(inputs, collaborativePredictions):\n",
    "    input_signals, labels = inputs\n",
    "    eval_mae.update_state(labels, collaborativePredictions)\n",
    "    eval_rmse.update_state(labels, collaborativePredictions)\n",
    "def test_step(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    predictions = model(input_signals, training=False)\n",
    "    eval_mae.update_state(labels, predictions)\n",
    "    eval_rmse.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step_batch(dataset_inputs):\n",
    "    per_replica_losses = strategy.run(train_step_batch, args=(dataset_inputs,)) \n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "@tf.function\n",
    "def collaborative_predictions(dataset_inputs):\n",
    "    per_replica_predictions= strategy.run(train_step_sample, args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_predictions,  \n",
    "                         axis=None)\n",
    "@tf.function\n",
    "def local_collaborative_loss(collaborative_predictions, dataset_inputs):\n",
    "    return strategy.run(compute_loss_fedLabSync, args=(dataset_inputs, collaborative_predictions,))\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step_batch(dataset_inputs,collaborative_predictions):\n",
    "    return strategy.run(test_step_batch, args=(dataset_inputs,collaborative_predictions,))\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "    return strategy.run(test_step, args=(dataset_inputs,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527e630",
   "metadata": {},
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1098f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "past = datetime.now()\n",
    "early_stopping = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # TRAIN LOOP\n",
    "    num_batches = 0\n",
    "    for x in distributed_values_train:\n",
    "        distributed_train_step_batch(x)\n",
    "        #colaboratePrediction = collaborative_predictions(x)\n",
    "        #total_loss = local_collaborative_loss(colaboratePrediction, x)\n",
    "        #num_batches += 1\n",
    "    \n",
    "    for x in distributed_values_eval:\n",
    "        distributed_test_step(x)\n",
    "        #colaboratePrediction = collaborative_predictions(x)\n",
    "        #distributed_test_step_batch(x,colaboratePrediction)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "    template = (\"Epoch {}, Train_RMSE: {}, Eval MAE: {}, \"\n",
    "              \"Eval_RMSE: {}\")\n",
    "    print(template.format(epoch + 1, train_rmse.result(), eval_mae.result(),\n",
    "                         eval_rmse.result()))\n",
    "    if early_stopping(eval_rmse.result()):\n",
    "        break\n",
    "    eval_mae.reset_states()\n",
    "    train_rmse.reset_states()\n",
    "    eval_rmse.reset_states()\n",
    "    \n",
    "last = datetime.now()\n",
    "time=last-past\n",
    "print(\"Training Time =\", time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39480bf1",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" with strategy.scope():\n",
    "    saved_model_path = \"RUL_MODEL/FC\"+str(NODE+1)+\"/federated\"\n",
    "    tf.saved_model.save(model, saved_model_path) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" with strategy.scope():\n",
    "    saved_model_path = \"RUL_MODEL/FC\"+str(NODE+1)+\"/federated\"\n",
    "    loaded = tf.saved_model.load(saved_model_path)\n",
    "    infer = loaded.signatures[\"serving_default\"]\n",
    "    print(infer) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE=1\n",
    "for FC in range(NODE,NODE+1):\n",
    "        # Load data DEV\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_test'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_test = np.array(hdf.get('W_test'), dtype='float32')             # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'), dtype='float32')         # X_s\n",
    "            Y_test = np.array(hdf.get('Y_test'), dtype='float32')             # RUL                  \n",
    "            A_test = np.array(hdf.get('A_test'), dtype='float32')\n",
    "            \n",
    "            W_test = W_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "            X_s_test = X_s_test[::DOWNSAMPLING_STEP[FC],:] \n",
    "            Y_test = Y_test[::DOWNSAMPLING_STEP[FC]]\n",
    "            A_test = A_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "\n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "            # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "        \n",
    "        #if FC==0:\n",
    "            W_test_aux = W_test\n",
    "            X_s_test_aux = X_s_test\n",
    "            Y_test_aux = Y_test\n",
    "            A_test_aux = A_test\n",
    "        #if FC!=0:\n",
    "        #    W_test_aux = np.concatenate((W_test_aux, W_test), axis=0)  \n",
    "        #    X_s_test_aux = np.concatenate((X_s_test_aux, X_s_test), axis=0)\n",
    "        #    Y_test_aux = np.concatenate((Y_test_aux, Y_test), axis=0) \n",
    "        #    A_test_aux = np.concatenate((A_test_aux, A_test), axis=0)\n",
    "            \n",
    "units_test=A_test_aux[:,0].reshape(-1,1)\n",
    "cycles_test=A_test_aux[:,1].reshape(-1,1)\n",
    "hi_test = A_test_aux[:,-1]\n",
    "\n",
    "if PIECE_WISE==True:\n",
    "    df_hs_unit_train = DataFrame({'unit': units_test.reshape(-1).astype(int),'RUL': Y_test_aux.reshape(-1), 'hi': hi_test.reshape(-1)})\n",
    "    df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "    pd_aux=DataFrame(df_hs_unit_train.groupby('unit')['RUL'].max()).reset_index()\n",
    "    df_hs_unit_train['RUL']=df_hs_unit_train.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "    Y_test_aux=df_hs_unit_train['RUL'].to_numpy().reshape(len(df_hs_unit_train),1)\n",
    "    \n",
    "    \n",
    "                \n",
    "X_s_test = SCALER_X.fit_transform(X_s_test_aux)\n",
    "W_test = SCALER_W.fit_transform(W_test_aux)\n",
    "Y_test = SCALER_Y.fit_transform(Y_test_aux)\n",
    "            \n",
    "X_windows_test, U_windows_test,C_windows_test=sequence_generator_per_unit(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "W_windows_test,_,_=sequence_generator_per_unit(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "Y_windows_test,_,_=sequence_generator_per_unit(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "                \n",
    "X_=X_windows_test[:,:,:]\n",
    "W_=W_windows_test[:,:,:]\n",
    "y_=Y_windows_test[:]\n",
    "\n",
    "#predictions=np.array(loaded.signatures['serving_default'](W_in=tf.convert_to_tensor(X_, dtype=tf.float32),X_in=tf.convert_to_tensor(W_, dtype=tf.float32))['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict((tf.convert_to_tensor(X_, dtype=tf.float32),tf.convert_to_tensor(W_, dtype=tf.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_prediction= np.mean( np.array([ predictions1, predictions2, predictions3 ]), axis=0 )\n",
    "rul_predicted_ = SCALER_Y.inverse_transform(predictions)\n",
    "groud_truth = SCALER_Y.inverse_transform(Y_windows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mae=mean_absolute_error(rul_predicted_,groud_truth)\n",
    "mse=mean_squared_error(rul_predicted_,groud_truth)\n",
    "rmse=np.sqrt(mse)\n",
    "print(\"GAUSSIAN KALMAN\")\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC1\n",
    "\n",
    "if NODE==0:\n",
    "    units = [107,109,214,312,314,508,510,608,610,708,710,810,815] \n",
    "# FC2\n",
    "if NODE==1:\n",
    "    units = [108,215,315,507,607,707,811,907,908,909,910] \n",
    "# FC3\n",
    "if NODE==2:\n",
    "    units = [110,211,310,311,313,509,609,709,812,813,814]\n",
    "numberUnits=4\n",
    "start=0\n",
    "numberUnits = int(len(units))\n",
    "for x in range(start,len(units),numberUnits):\n",
    "    if (x+2*numberUnits) < len(units):\n",
    "    #if (x+numberUnits) < len(units):\n",
    "        unit_sel = np.array(units[x:x+numberUnits], dtype=int)\n",
    "        plot_predicted_true_rul([rul_predicted_], unit_sel, U_windows_test, C_windows_test[:,0,:], groud_truth)\n",
    "    else:\n",
    "        unit_sel = np.array(units[x:len(np.unique(U_windows_test))], dtype=int)\n",
    "        plot_predicted_true_rul([rul_predicted_], unit_sel, U_windows_test, C_windows_test[:,0,:], groud_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd17947a",
   "metadata": {},
   "source": [
    "# Test All trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2af4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_data_partition_test(NODES):\n",
    "    # Global Variables\n",
    "    global SCALER_X\n",
    "    global SCALER_W\n",
    "    global SCALER_Y\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    distributedData=[]\n",
    "    DATA_REPLICA_ID=[]\n",
    "    for FC in range(0,NODES):\n",
    "        # Load data TEST\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_test'+\".h5\", 'r') as hdf:\n",
    "                    # Development set\n",
    "                    W_test = np.array(hdf.get('W_test'), dtype='float16')             # W\n",
    "                    X_s_test = np.array(hdf.get('X_s_test'), dtype='float16')         # X_s\n",
    "                    X_v_test = np.array(hdf.get('X_v_test'), dtype='float16')         # X_v\n",
    "                    T_test = np.array(hdf.get('T_test'), dtype='float16')             # T\n",
    "                    Y_test = np.array(hdf.get('Y_test'), dtype='float16')             # RUL  \n",
    "                    A_test = np.array(hdf.get('A_test'), dtype='float16')\n",
    "                    \n",
    "                    W_test = W_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                    X_s_test = X_s_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                    X_v_test = X_v_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                    T_test = T_test[::DOWNSAMPLING_STEP[FC],:] \n",
    "                    Y_test = Y_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                    A_test = A_test[::DOWNSAMPLING_STEP[FC],:]\n",
    "                    \n",
    "                    if FC==0:\n",
    "                        W_test_aux = W_test\n",
    "                        X_s_test_aux = X_s_test\n",
    "                        Y_test_aux = Y_test\n",
    "                        A_test_aux = A_test\n",
    "                    if FC!=0:\n",
    "                        W_test_aux = np.concatenate((W_test_aux, W_test), axis=0)  \n",
    "                        X_s_test_aux = np.concatenate((X_s_test_aux, X_s_test), axis=0)\n",
    "                        Y_test_aux = np.concatenate((Y_test_aux, Y_test), axis=0) \n",
    "                        A_test_aux = np.concatenate((A_test_aux, A_test), axis=0)\n",
    "            \n",
    "\n",
    "    units_test=W_test_aux[:,0].reshape(-1,1)\n",
    "    cycles_test=A_test_aux[:,1].reshape(-1,1)\n",
    "    fc_test = A_test_aux[:,2].reshape(-1,1)\n",
    "    hi_test = A_test_aux[:,-1]\n",
    "\n",
    "    if PIECE_WISE==True:\n",
    "        df_hs_unit_test = DataFrame({'unit': units_test.reshape(-1).astype(int),'RUL': Y_test_aux.reshape(-1), 'hi': hi_test.reshape(-1)})\n",
    "        df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctRUL(row['hi'],row['RUL']), axis=1)\n",
    "\n",
    "        pd_aux=DataFrame(df_hs_unit_test.groupby('unit')['RUL'].max()).reset_index()\n",
    "        df_hs_unit_test['RUL']=df_hs_unit_test.apply(lambda row: correctMaxRUL(row['hi'],row['RUL'], float(pd_aux.iloc[pd_aux.index[pd_aux['unit'] == row['unit']]]['RUL'])), axis=1)\n",
    "        Y_test_aux=df_hs_unit_test['RUL'].to_numpy().reshape(len(df_hs_unit_test),1)\n",
    "\n",
    "\n",
    "    # SCALE\n",
    "\n",
    "    X_s_test = SCALER_X.transform(X_s_test_aux)\n",
    "    W_test = SCALER_W.transform(W_test_aux)\n",
    "    Y_test = SCALER_Y.transform(Y_test_aux)\n",
    "\n",
    "    X_windows_test, U_windows_test,C_windows_test=sequence_generator_per_unit(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "    W_windows_test,_,_=sequence_generator_per_unit(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "    Y_windows_test,_,_=sequence_generator_per_unit(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "    \n",
    "    X_ = batch_data_3d(X_windows_test,BATCH_SIZE)\n",
    "    W_ = batch_data_3d(W_windows_test,BATCH_SIZE)\n",
    "    y_= batch_data_2d(Y_windows_test,BATCH_SIZE)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributedDataTest = fn_data_partition_test(NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2707eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_fn_test(ctx):\n",
    "    return distributedDataTest[ctx.replica_id_in_sync_group]\n",
    "distributed_values_test = strategy.experimental_distribute_values_from_function(value_fn_test)\n",
    "for x in distributed_values_test:\n",
    "    colaboratePrediction = collaborative_predictions(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colaboratePrediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
