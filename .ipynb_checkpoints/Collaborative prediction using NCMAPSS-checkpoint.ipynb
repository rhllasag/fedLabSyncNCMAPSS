{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e10806e",
   "metadata": {},
   "source": [
    "# Collaborative RUL Estimation of Turbofan Engines using NCMAPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5eb0826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "#Keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import  keras\n",
    "import keras\n",
    "import keras.backend as K\n",
    "#Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d2c5d",
   "metadata": {},
   "source": [
    "## Create a strategy to distribute the variables and the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e852e",
   "metadata": {},
   "source": [
    "## Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(input_data, sequence_length, stride = 1, option = None):\n",
    "    \"\"\"\n",
    "     \n",
    "    \"\"\"\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(0,len(input_data),stride):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + sequence_length\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(input_data):\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        if option=='last':\n",
    "            seq_x = input_data[end_ix-1, :]\n",
    "        elif option=='next':\n",
    "            seq_x = input_data[end_ix, :]\n",
    "        else:\n",
    "            seq_x = input_data[i:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    \n",
    "    return np.array(X)\n",
    "def sequence_generator(input_data, units, cycles, sequence_length=10,stride = 1, option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        c_mask = cycles[mask]\n",
    "        x_unit = input_data[mask]\n",
    "        for j in np.unique(c_mask):\n",
    "            mask = np.ravel(c_mask==j)\n",
    "            seq_x_u = split_sequences(x_unit[mask],sequence_length, stride, option)\n",
    "            X.append(seq_x_u)\n",
    "            unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "            c_num.extend(np.ones(len(seq_x_u),dtype = int)*j)\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.array(c_num).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDistribution(NODES):\n",
    "    \"\"\"\n",
    "        This function is used to distribute the indexes of Training and Eval datasets\n",
    "    \"\"\"\n",
    "    global SHUFFLE_IDX\n",
    "    global TRAINING_IDX\n",
    "    global VAL_IDX\n",
    "    global M_LABELS\n",
    "    global MAX_DATASET_SIZE\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    \n",
    "    for FC in range(0, NODES):\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_dev'+\".h5\", 'r') as hdf:\n",
    "            Y_train = np.array(hdf.get('Y_dev'), dtype='float16')\n",
    "            A_train = np.array(hdf.get('A_dev'), dtype='float16')\n",
    "            \n",
    "            units_train=A_train[:,0].reshape(-1,1)\n",
    "            cycles_train=A_train[:,1].reshape(-1,1)\n",
    "            \n",
    "            # Create Windows for Labels\n",
    "            Y_windows,_,_=sequence_generator(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            \n",
    "            shuffle_idx = np.random.permutation(Y_windows.shape[0]) \n",
    "            TRAINING_IDX[FC], VAL_IDX[FC] = shuffle_idx[Y_windows.shape[0]//10:],shuffle_idx[:Y_windows.shape[0]//10]\n",
    "            M_LABELS[FC] = Y_train[TRAINING_IDX[FC]]\n",
    "            if (MAX_DATASET_SIZE<Y_train[TRAINING_IDX[FC]].size):\n",
    "                MAX_DATASET_SIZE=Y_train[TRAINING_IDX[FC]].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15478c26",
   "metadata": {},
   "source": [
    "## Splitting data by Flight Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9446c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NODES = 3\n",
    "WINDOW_LEN = 50\n",
    "STRIDE = 5\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc15b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [64,32]\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0b68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Scaler Variables\n",
    "SCALER_X = MinMaxScaler()\n",
    "SCALER_W = MinMaxScaler()\n",
    "SCALER_Y = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73950ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Sync Variables\n",
    "M_LABELS=[*range(0,NODES)]\n",
    "SHUFFLE_IDX = {}\n",
    "TRAINING_IDX = {}\n",
    "VAL_IDX = {}\n",
    "MAX_DATASET_SIZE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ffea2",
   "metadata": {},
   "source": [
    "### Distribute Indexes (Training and Eval) and Labeles per Replica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69416cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDistribution(NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a57c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13., 22., 56.],\n",
       "       [94., 27., 12.],\n",
       "       [31., 36.,  6.],\n",
       "       ...,\n",
       "       [nan, nan, 58.],\n",
       "       [nan, nan, 64.],\n",
       "       [nan, nan, 66.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=np.empty((MAX_DATASET_SIZE,NODES))\n",
    "M[:] = np.nan\n",
    "for x in range(0,NODES):\n",
    "    M[:len(M_LABELS[x]),x]=list(chain.from_iterable(M_LABELS[x]))\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff0254f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_data_partition(NODES, evaluation=False):\n",
    "    \n",
    "    # Global Variables\n",
    "    global SCALER_X\n",
    "    global SCALER_W\n",
    "    global SCALER_Y\n",
    "    global WINDOW_LEN\n",
    "    global STRIDE\n",
    "    global BATCH_SIZE\n",
    "    global TRAINING_IDX\n",
    "    global VAL_IDX\n",
    "    \n",
    "    distributedData=[]\n",
    "    for FC in range(0, NODES):\n",
    "        DATA_REPLICA_ID = []\n",
    "        # Load data DEV\n",
    "        with h5py.File(\"FC\"+str(FC+1)+\"/FC\"+str(FC+1)+'_dev'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_train = np.array(hdf.get('W_dev'), dtype='float16')             # W\n",
    "            X_s_train = np.array(hdf.get('X_s_dev'), dtype='float16')         # X_s\n",
    "            X_v_train = np.array(hdf.get('X_v_dev'), dtype='float16')         # X_v\n",
    "            T_train = np.array(hdf.get('T_dev'), dtype='float16')             # T\n",
    "            Y_train = np.array(hdf.get('Y_dev'), dtype='float16')             # RUL                  \n",
    "            A_train = np.array(hdf.get('A_dev'), dtype='float16')\n",
    "\n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "            # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "\n",
    "            units_train=A_train[:,0].reshape(-1,1)\n",
    "            cycles_train=A_train[:,1].reshape(-1,1)\n",
    "            fc_train = A_train[:,2].reshape(-1,1)\n",
    "            hi_train = A_train[:,-1]\n",
    "                \n",
    "            X_s_train = SCALER_X.fit_transform(X_s_train)\n",
    "            W_train = SCALER_W.fit_transform(W_train)\n",
    "            Y_train = SCALER_Y.fit_transform(Y_train)\n",
    "                \n",
    "            X_windows, U_windows, C_windows=sequence_generator(X_s_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            W_windows,_,_=sequence_generator(W_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = STRIDE)\n",
    "            Y_windows,_,_=sequence_generator(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = STRIDE)\n",
    "            \n",
    "            if evaluation:\n",
    "                DATA_REPLICA_ID = tf.data.Dataset.from_tensor_slices(((X_windows[VAL_IDX[FC]],W_windows[VAL_IDX[FC]]),Y_windows[VAL_IDX[FC]]))\n",
    "                DATA_REPLICA_ID = DATA_REPLICA_ID.batch(BATCH_SIZE)\n",
    "            else: \n",
    "                DATA_REPLICA_ID = tf.data.Dataset.from_tensor_slices(((X_windows[TRAINING_IDX[FC]],W_windows[TRAINING_IDX[FC]]),Y_windows[TRAINING_IDX[FC]]))\n",
    "                DATA_REPLICA_ID = DATA_REPLICA_ID.batch(BATCH_SIZE) \n",
    "        \n",
    "        distributedData.append(DATA_REPLICA_ID)\n",
    "    return distributedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14bdd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributedDataTrain = fn_data_partition(NODES,False)\n",
    "distributedDataTest = fn_data_partition(NODES,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f397d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_fn_train(ctx):\n",
    "    return distributedDataTrain[ctx.replica_id_in_sync_group]\n",
    "def value_fn_test(ctx):\n",
    "    return distributedDataTest[ctx.replica_id_in_sync_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75387904",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_values_train = strategy.experimental_distribute_values_from_function(value_fn_train)\n",
    "distributed_values_test = strategy.experimental_distribute_values_from_function(value_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd21798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_result_train = strategy.experimental_local_results(distributed_values_train)[0]\n",
    "local_result_test = strategy.experimental_local_results(distributed_values_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d3a49",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13e91c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUL MODEL\n",
    "def predictor(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_H_in=1,\n",
    "      feature_out_size=1,\n",
    "      activation='relu',\n",
    "      filter = [10,10,1],\n",
    "      filter_size = 10,\n",
    "      useH=True):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "    \n",
    "    if useH:\n",
    "      h_in = layers.Input(shape=(t,feature_H_in),name=\"H_in\")\n",
    "      # h_in = layers.Input(shape=(1,1),name=\"H_in\")\n",
    "      x = tf.concat([x_in,w_in, h_in],-1)\n",
    "    else: \n",
    "      x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    for i in filter:\n",
    "      x = layers.Conv1D(i,filter_size,1,padding='same',activation = activation)(x)\n",
    "      # x = layers.BatchNormalization()(x)\n",
    "      \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(50,activation = activation)(x)\n",
    "    y = layers.Dense(feature_out_size,activation = 'linear')(y)\n",
    "\n",
    "    if useH:\n",
    "      model = models.Model([x_in,w_in,h_in], y)\n",
    "    else:\n",
    "      model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48beca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6568b5",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "Recall that the loss function consists of one or two parts:\n",
    "\n",
    "  * The **prediction loss** measures how far off the model's predictions are from the training labels for a batch of training examples. It is computed for each labeled example and then reduced across the batch by computing the average value.\n",
    "  * Optionally, **regularization loss** terms can be added to the prediction loss, to steer the model away from overfitting the training data. A common choice is L2 regularization, which adds a small fixed multiple of the sum of squares of all model weights, independent of the number of examples. The model above uses L2 regularization to demonstrate its handling in the training loop below.\n",
    "\n",
    "For training on a single machine with a single GPU/CPU, this works as follows:\n",
    "\n",
    "  * The prediction loss is computed for each example in the batch, summed across the batch, and then divided by the batch size.\n",
    "  * The regularization loss is added to the prediction loss.\n",
    "  * The gradient of the total loss is computed w.r.t. each model weight, and the optimizer updates each model weight from the corresponding gradient.\n",
    "\n",
    "With `tf.distribute.Strategy`, the input batch is split between replicas.\n",
    "For example, let's say you have 4 GPUs, each with one replica of the model. One batch of 256 input examples is distributed evenly across the 4 replicas, so each replica gets a batch of size 64: We have `256 = 4*64`, or generally `GLOBAL_BATCH_SIZE = num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`.\n",
    "\n",
    "Each replica computes the loss from the training examples it gets and computes the gradients of the loss w.r.t. each model weight. The optimizer takes care that these **gradients are summed up across replicas** before using them to update the copies of the model weights on each replica.\n",
    "\n",
    "*So, how should the loss be calculated when using a `tf.distribute.Strategy`?*\n",
    "\n",
    "  * Each replica computes the prediction loss for all examples distributed to it, sums up the results and divides them by `num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`, or equivently, `GLOBAL_BATCH_SIZE`.\n",
    "  * Each replica compues the regularization loss(es) and divides them by\n",
    "  `num_replicas_in_sync`.\n",
    "\n",
    "Compared to non-distributed training, all per-replica loss terms are scaled down by a factor of `1/num_replicas_in_sync`. On the other hand, all loss terms -- or rather, their gradients -- are summed across that number of replicas before the optimizer applies them. In effect, the optimizer on each replica uses the same gradients as if a non-distributed computation with `GLOBAL_BATCH_SIZE` had happened. This is consistent with the distributed and undistributed behavior of Keras `Model.fit`. See the [Distributed training with Keras](./keras.ipynb) tutorial on how a larger gloabl batch size enables to scale up the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62113915",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def compute_loss_batch(labels, predictions, model_losses):\n",
    "        per_example_loss = (labels - predictions)**2  # Sample error\n",
    "        loss = tf.math.sqrt(tf.nn.compute_average_loss(per_example_loss)) # Batch Error\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45ca6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    test_mae = tf.keras.metrics.MeanAbsoluteError()\n",
    "    train_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    test_rmse = tf.keras.metrics.RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "106050cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\.conda\\envs\\fedLabSync\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "    model = predictor(t=14,useH=False,filter = LAYERS)\n",
    "    optimizer = SGD(lr=LEARNING_RATE)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a4fea",
   "metadata": {},
   "source": [
    "## Auxiliar Funcions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc7d23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_batch(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "        loss = compute_loss_batch(labels, predictions, model.losses) # Batch Error\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_rmse.update_state(labels, predictions)\n",
    "    return loss\n",
    "def train_step_sample(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "    return predictions\n",
    "\n",
    "def compute_loss_fedLabSync(inputs, collaborativePredictions):\n",
    "    input_signals, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_signals, training=True)\n",
    "        loss = compute_loss_batch(labels, (collaborativePredictions+predictions)/2, model.losses) # Batch Error\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_rmse.update_state(labels, (collaborativePredictions+predictions)/2)\n",
    "    return loss\n",
    "    \n",
    "def test_step(inputs):\n",
    "    input_signals, labels = inputs\n",
    "    predictions = model(input_signals, training=False)\n",
    "    \n",
    "    t_loss = tf.math.abs(labels-predictions)\n",
    "    test_mae.update_state(labels, predictions)\n",
    "    test_rmse.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4696f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step_batch(dataset_inputs):\n",
    "    per_replica_losses = strategy.run(train_step_batch, args=(dataset_inputs,)) \n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "@tf.function\n",
    "def collaborative_predictions(dataset_inputs):\n",
    "    per_replica_predictions= strategy.run(train_step_sample, args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_predictions,  \n",
    "                         axis=None)\n",
    "@tf.function\n",
    "def local_collaborative_loss(collaborative_predictions, dataset_inputs):\n",
    "    return strategy.run(compute_loss_fedLabSync, args=(dataset_inputs, collaborative_predictions,))\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step_batch(dataset_inputs):\n",
    "    return strategy.run(test_step, args=(dataset_inputs,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4325ae",
   "metadata": {},
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52da7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # TRAIN LOOP\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in local_result_train:\n",
    "        colaboratePrediction = collaborative_predictions(x)\n",
    "        total_loss += local_collaborative_loss(colaboratePrediction, x)\n",
    "        #total_loss += distributed_train_step_batch(x)\n",
    "        num_batches += 1\n",
    "    train_loss = total_loss / num_batches\n",
    "    \n",
    "    # TEST LOOP\n",
    "    for x in local_result_test:\n",
    "        distributed_test_step_batch(x)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "    template = (\"Epoch {}, Train_RMSE: {}, Test MAE: {}, \"\n",
    "              \"Test_RMSE: {}\")\n",
    "    print(template.format(epoch + 1, train_rmse.result(), test_mae.result(),\n",
    "                         test_rmse.result()))\n",
    "    test_mae.reset_states()\n",
    "    train_rmse.reset_states()\n",
    "    test_rmse.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
