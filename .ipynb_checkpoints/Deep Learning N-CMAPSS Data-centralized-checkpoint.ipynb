{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import random\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from sklearn.utils import shuffle\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# EVALUATION modules\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "params = {'legend.fontsize': 20,\n",
    "          'figure.figsize': (9,6),\n",
    "         'axes.labelsize': 20,\n",
    "         'axes.titlesize':20,\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'axes.linewidth' : 2,\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "\n",
    "plt.rcParams.update(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mape error\n",
    "def mape(real,predicted):\n",
    "    return np.mean(np.abs((real - predicted) / real)) * 100\n",
    "\n",
    "def mae(real,predicted):\n",
    "    return np.mean(np.abs(real - predicted))\n",
    "\n",
    "def rmse(real,predicted):\n",
    "    return np.sqrt(np.mean((real - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x, lb, ub, max_v=1.0, min_v=-1.0):\n",
    "\n",
    "    # Set-up\n",
    "    if (lb == []) & (ub == []):\n",
    "        # OPTION 1:\n",
    "        ub = x.max(0)\n",
    "        lb = x.min(0)\n",
    "    \n",
    "        # OPTION 2:\n",
    "        #ub = np.percentile(x, 99.9, axis=0, keepdims=True)\n",
    "        #lb = np.percentile(x, 0.1, axis=0, keepdims=True)\n",
    "    \n",
    "    ub.shape = (1,-1)\n",
    "    lb.shape = (1,-1)           \n",
    "    max_min = max_v - min_v\n",
    "    delta = ub-lb\n",
    "\n",
    "    # Compute\n",
    "    x_n = max_min * (x - lb) / delta + min_v\n",
    "    if 0 in delta:\n",
    "        idx = np.ravel(delta == 0)\n",
    "        x_n[:,idx] = x[:,idx] - lb[:, idx]\n",
    "\n",
    "    return x_n, lb, ub \n",
    "\n",
    "\n",
    "def split_dataset(dataset, split=4): \n",
    "    '''\n",
    "    Split 'dataset' in tree pieces: w, x_s, theta\n",
    "    '''\n",
    "    w = dataset[:,:split]\n",
    "    x_s = dataset[:,split:20]\n",
    "    theta = dataset[:,-10:]\n",
    "    tmp = theta[:,6]\n",
    "    tmp.shape = (-1,1)\n",
    "    \n",
    "    return w, x_s, tmp\n",
    "\n",
    "def extract_units_ds(id_en, ds, units):\n",
    "    '''\n",
    "    Creates a subset with only id_en units for ds\n",
    "    '''\n",
    "    \n",
    "    # Set-up\n",
    "    ds_sub = []\n",
    "    units_unique = np.unique(units)\n",
    "\n",
    "    # Process\n",
    "    for i in units_unique:\n",
    "        if i in id_en:\n",
    "            idx = np.ravel(units==i)\n",
    "            ds_sub.append(ds[idx,:])           \n",
    "    \n",
    "    return np.concatenate(ds_sub, axis=0)\n",
    "\n",
    "def create_lag_data(w, x_s, theta, y, Units, Cycles, stride=1):\n",
    "    # Set-up\n",
    "    W, X, T, Y, U, C, _W, _X = [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # Loop over units and then within the units\n",
    "    units = np.unique(Units)\n",
    "    for k in units:\n",
    "        unit = np.ravel(Units == k)\n",
    "        w_unit = w[unit,:]\n",
    "        x_s_unit = x_s[unit,:]\n",
    "        T_unit = theta[unit,:]\n",
    "        Y_unit = y[unit,:]\n",
    "        U_unit = Units[unit,:]\n",
    "        C_unit = Cycles[unit,:]\n",
    "        dim = w_unit.shape[0]\n",
    "        for i in range(dim-1): \n",
    "            X.append(x_s_unit[i + stride, :])          # X  or X_\n",
    "            W.append(w_unit[i + stride, :])            # W  or W_\n",
    "            T.append(T_unit[i + stride, :])            # T  or T_\n",
    "            Y.append(Y_unit[i + stride, :])            # T  or T_\n",
    "            U.append(U_unit[i + stride, :])            # U  or U_\n",
    "            C.append(C_unit[i + stride, :])            # C  or C_\n",
    "            _X.append(x_s_unit[i, :])                  # _X or X\n",
    "            _W.append(w_unit[i, :])                  # _X or X\n",
    "            \n",
    "    return np.array(W), np.array(X), np.array(T), np.array(Y), np.array(U), np.array(C), np.array(_X), np.array(_W)\n",
    "\n",
    "\n",
    "\n",
    "# np.array(X_out), np.array(W_out), np.array(Y_out), np.array(T_out), np.array(U_out), np.array(C_out), np.array(HI_out)\n",
    "\n",
    "def split_sequences(input_data, sequence_length, stride = 1, option = None):\n",
    "    \"\"\"\n",
    "     \n",
    "    \"\"\"\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(0,len(input_data),stride):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + sequence_length\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(input_data):\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        if option=='last':\n",
    "            seq_x = input_data[end_ix-1, :]\n",
    "        elif option=='next':\n",
    "            seq_x = input_data[end_ix, :]\n",
    "        else:\n",
    "            seq_x = input_data[i:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "# def sequence_generator(input_data, units, sequence_length=10, option=None):\n",
    "#     \"\"\"\n",
    "#      # Generates dataset with windows of sequence_length      \n",
    "#     \"\"\"  \n",
    "#     X = list()\n",
    "#     unit_num=[]\n",
    "#     for i, elem_u in enumerate(list(np.unique(units))):\n",
    "#         mask = np.ravel(units==elem_u)\n",
    "#         seq_x_u = split_sequences(input_data[mask],sequence_length, option)\n",
    "#         X.append(seq_x_u)\n",
    "#         unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "    \n",
    "#     return np.vstack(X),unit_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensor_signal(x_real,x_pred,var_names,num=1000,figsize=10):\n",
    "    input_dim = len(var_names)\n",
    "    cols = min(np.floor(input_dim**0.5).astype(int),4)\n",
    "    rows = (np.ceil(input_dim / cols)).astype(int)\n",
    "    gs   = gridspec.GridSpec(rows, cols)\n",
    "    fig  = plt.figure(figsize=(figsize, max(figsize, rows*2)))   \n",
    "    for i in range(input_dim):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        # ax.plot(np.arange(0,num),x_real[-num:,i],label=\"True\",marker='.',markeredgewidth=0.25, markersize=8)\n",
    "        # ax.plot(np.arange(0,num),x_pred[-num:,i],label=\"Pred\",marker='.',markeredgewidth=0.25, markersize=8)\n",
    "        ax.scatter(np.arange(0,num),x_real[-num:,i],label=\"True\",marker='.')\n",
    "        ax.scatter(np.arange(0,num),x_pred[-num:,i],label=\"Pred\",marker='.')\n",
    "        plt.title(var_names[i])\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sequence Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generator(input_data, units, cycles, sequence_length=10,stride = 1, option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        c_mask = cycles[mask]\n",
    "        x_unit = input_data[mask]\n",
    "        for j in np.unique(c_mask):\n",
    "            mask = np.ravel(c_mask==j)\n",
    "            seq_x_u = split_sequences(x_unit[mask],sequence_length, stride, option)\n",
    "            X.append(seq_x_u)\n",
    "            unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "            c_num.extend(np.ones(len(seq_x_u),dtype = int)*j)\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.array(c_num).reshape(-1,1)\n",
    "\n",
    "\n",
    "def sequence_generator_per_unit(input_data, units, cycles, sequence_length=10, stride =1,option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        x_unit = input_data[mask]\n",
    "        seq_x_u = split_sequences(x_unit,sequence_length, stride, option)\n",
    "        X.append(seq_x_u)\n",
    "        unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "        c_num.append(split_sequences(cycles[mask],sequence_length, stride, option))\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.vstack(c_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(xs,w,y,t,units,cycles,hi,freq,mean = False):\n",
    "    \n",
    "    X_out,W_out,Y_out,T_out,U_out,C_out,HI_out= [], [], [], [], [], [] , []\n",
    "    \n",
    "    # Number of Trajectories (Units)\n",
    "    uniq_units=np.unique(units)\n",
    "    \n",
    "    for k in uniq_units:\n",
    "        unit = np.ravel(units == k)\n",
    "        u_unit=units[unit,:]\n",
    "        w_unit = w[unit,:]\n",
    "        x_s_unit = xs[unit,:]\n",
    "        y_unit= y[unit,:]\n",
    "        t_unit= t[unit,:]\n",
    "        c_unit= cycles[unit,:]\n",
    "        h_unit = hi[unit]\n",
    "        \n",
    "        if mean:\n",
    "            for i in range(0,x_s_unit.shape[0],freq):\n",
    "                X_out.append(np.mean(x_s_unit[i:i+freq,:],axis=0))\n",
    "                W_out.append(np.mean(w_unit[i:i+freq,:],axis=0))\n",
    "                Y_out.append(np.mean(y_unit[i:i+freq,:],axis=0))\n",
    "                T_out.append(np.mean(t_unit[i:i+freq,:],axis=0))\n",
    "                HI_out.append(np.mean(h_unit[i:i+freq,:],axis=0))\n",
    "            \n",
    "            C_out.extend(c_unit[::freq])\n",
    "            U_out.extend(u_unit[::freq])\n",
    "        else:\n",
    "        \n",
    "            X_out.extend(x_s_unit[::freq,])\n",
    "            W_out.extend(w_unit[::freq,])\n",
    "            Y_out.extend(y_unit[::freq])\n",
    "            U_out.extend(u_unit[::freq])\n",
    "            T_out.extend(t_unit[::freq])\n",
    "            C_out.extend(c_unit[::freq])\n",
    "            HI_out.extend(h_unit[::freq])\n",
    "            \n",
    "    return  np.array(X_out).astype(np.float16),\\\n",
    "            np.array(W_out).astype(np.float16),\\\n",
    "            np.array(Y_out).astype(np.float16),\\\n",
    "            np.array(T_out).astype(np.float16),\\\n",
    "            np.array(U_out).astype(np.float16),\\\n",
    "            np.array(C_out).astype(np.float16),\\\n",
    "            np.array(HI_out).astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_healthy(data,units,cycles,cycle_len,sample_len=1):\n",
    "    X,U,C=[],[],[]\n",
    "    uniq_units=np.unique(units)\n",
    "    for i in uniq_units:\n",
    "        idx=np.ravel(units==i)\n",
    "        cy=cycles[idx]\n",
    "        x=data[idx,:]\n",
    "        u=units[idx]\n",
    "        cyidx=np.ravel(cy<=cycle_len)\n",
    "        x=x[cyidx,:]\n",
    "        u=u[cyidx]\n",
    "        cyy = cy[cyidx]\n",
    "        X.extend(x[::sample_len,:])\n",
    "        U.extend(u[::sample_len])\n",
    "        C.extend(cyy[::sample_len])\n",
    "        \n",
    "    return np.array(X), np.array(U), np.array(C)\n",
    "\n",
    "def sample_healthy_mixed(data,units,cycles,starts,cycle_len,full=False):\n",
    "    X,U,C=[],[],[]\n",
    "    uniq_units=np.unique(units)\n",
    "    for i in range(len(uniq_units)):\n",
    "        idx=np.ravel(units==uniq_units[i])\n",
    "        cy=cycles[idx]\n",
    "        x=data[idx,:]\n",
    "        u=units[idx]\n",
    "        if full:\n",
    "            cyidx=np.ravel((cy>=starts[i]))\n",
    "        else:\n",
    "            cyidx=np.ravel((cy>=starts[i])&(cy<=cycle_len+starts[i]))\n",
    "        x=x[cyidx,:]\n",
    "        u=u[cyidx]\n",
    "        cyy = cy[cyidx]\n",
    "        X.extend(x)\n",
    "        U.extend(u)\n",
    "        C.extend(cyy)\n",
    "        \n",
    "    return np.array(X), np.array(U), np.array(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def smooth_z_cycle(HI,T,units):\n",
    "    results = []\n",
    "    stds = []\n",
    "    cycle_num =[]\n",
    "    unit_num =[]\n",
    "    for j in np.unique(units):\n",
    "        idx = np.ravel(units==j)\n",
    "        HI_unit = HI[idx]\n",
    "        T_unit = T[idx].astype('float64')\n",
    "        for jj in np.unique(T_unit):\n",
    "            idxT = np.ravel(T_unit==jj)\n",
    "            HI_mu_cycle = np.mean(HI_unit[idxT])\n",
    "            std_cycle = np.std(HI_unit[idxT])\n",
    "            results.append(HI_mu_cycle)\n",
    "            stds.append(std_cycle)\n",
    "            cycle_num.append(jj)\n",
    "            unit_num.append(j)\n",
    "    return np.array(results), np.array(stds),np.array(cycle_num),np.array(unit_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUL MODEL\n",
    "\n",
    "def predictor(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_H_in=1,\n",
    "      feature_out_size=1,\n",
    "      activation='relu',\n",
    "      filter = [10,10,1],\n",
    "      filter_size = 10,\n",
    "      useH=True):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "    \n",
    "    if useH:\n",
    "      h_in = layers.Input(shape=(t,feature_H_in),name=\"H_in\")\n",
    "      # h_in = layers.Input(shape=(1,1),name=\"H_in\")\n",
    "      x = tf.concat([x_in,w_in, h_in],-1)\n",
    "    else: \n",
    "      x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    for i in filter:\n",
    "      x = layers.Conv1D(i,filter_size,1,padding='same',activation = activation)(x)\n",
    "      # x = layers.BatchNormalization()(x)\n",
    "      \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(50,activation = activation)(x)\n",
    "    y = layers.Dense(feature_out_size,activation = 'linear')(y)\n",
    "\n",
    "    if useH:\n",
    "      model = models.Model([x_in,w_in,h_in], y)\n",
    "    else:\n",
    "      model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AE MODEL FCN\n",
    "def AE(original_dimX,original_dimW,latent_dims,z_size = 1):\n",
    "    \n",
    "    inpX = layers.Input(shape=(original_dimX,))\n",
    "    inpW = layers.Input(shape=(original_dimW,))\n",
    "    x = tf.concat([inpX,inpW],axis = -1)\n",
    "        \n",
    "\n",
    "    for i in latent_dims:\n",
    "        x=tf.keras.layers.Dense(i,activation='relu')(x)\n",
    "        \n",
    "    z = tf.keras.layers.Dense(z_size,name=\"Z\")(x)\n",
    "     \n",
    "    x = tf.keras.layers.Concatenate()([z, inpW])\n",
    "            \n",
    "    for i in reversed(list(latent_dims)):\n",
    "        x=tf.keras.layers.Dense(i,activation='relu')(x) \n",
    "    x=tf.keras.layers.Dense(original_dimX,name='X')(x)   \n",
    "       \n",
    "    model = models.Model([inpX,inpW],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction\n",
    "\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XS_train (2854517, 14)\n",
      "(4914552, 1)\n"
     ]
    }
   ],
   "source": [
    "for FC in range(1,FCs+1):\n",
    "    # Load data DEV\n",
    "    with h5py.File(\"FC\"+str(FC)+\"/FC\"+str(FC)+'_dev'+\".h5\", 'r') as hdf:\n",
    "                # Development set\n",
    "                W_train = np.array(hdf.get('W_dev'), dtype='float16')             # W\n",
    "                X_s_train = np.array(hdf.get('X_s_dev'), dtype='float16')         # X_s\n",
    "                X_v_train = np.array(hdf.get('X_v_dev'), dtype='float16')         # X_v\n",
    "                T_train = np.array(hdf.get('T_dev'), dtype='float16')             # T\n",
    "                Y_train = np.array(hdf.get('Y_dev'), dtype='float16')             # RUL  \n",
    "                A_train = np.array(hdf.get('A_dev'), dtype='float16')\n",
    "\n",
    "                # Varnams\n",
    "                W_var = np.array(hdf.get('W_var'))\n",
    "                X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "                X_v_var = np.array(hdf.get('X_v_var')) \n",
    "                T_var = np.array(hdf.get('T_var'))\n",
    "                A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "                    # from np.array to list dtype U4/U5\n",
    "                W_var = list(np.array(W_var, dtype='U20'))\n",
    "                X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "                X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "                T_var = list(np.array(T_var, dtype='U20'))\n",
    "                A_var = list(np.array(A_var, dtype='U20'))\n",
    "\n",
    "    # Load data TEST\n",
    "    mode = '_test'\n",
    "    with h5py.File(\"FC\"+str(FC)+\"/FC\"+str(FC)+'_test'+\".h5\", 'r') as hdf:\n",
    "                # Development set\n",
    "                W_test = np.array(hdf.get('W_test'), dtype='float16')             # W\n",
    "                X_s_test = np.array(hdf.get('X_s_test'), dtype='float16')         # X_s\n",
    "                X_v_test = np.array(hdf.get('X_v_test'), dtype='float16')         # X_v\n",
    "                T_test = np.array(hdf.get('T_test'), dtype='float16')             # T\n",
    "                Y_test = np.array(hdf.get('Y_test'), dtype='float16')             # RUL  \n",
    "                A_test = np.array(hdf.get('A_test'), dtype='float16')\n",
    "    if FC==1:\n",
    "        W_train_aux = W_train\n",
    "        X_s_train_aux = X_s_train\n",
    "        X_v_train_aux = X_v_train\n",
    "        T_train_aux = T_train\n",
    "        Y_train_aux = Y_train\n",
    "        A_train_aux = A_train\n",
    "        \n",
    "        W_test_aux = W_test\n",
    "        X_s_test_aux = X_s_test\n",
    "        X_v_test_aux = X_v_test\n",
    "        T_test_aux = T_test\n",
    "        Y_test_aux = Y_test\n",
    "        A_test_aux = A_test\n",
    "    if FC!=1:\n",
    "        W_train_aux = np.concatenate((W_train_aux, W_train), axis=0)  \n",
    "        X_s_train_aux = np.concatenate((X_s_train_aux, X_s_train), axis=0)\n",
    "        X_v_train_aux = np.concatenate((X_v_train_aux, X_v_train), axis=0)\n",
    "        T_train_aux = np.concatenate((T_train_aux, T_train), axis=0)\n",
    "        Y_train_aux = np.concatenate((Y_train_aux, Y_train), axis=0) \n",
    "        A_train_aux = np.concatenate((A_train_aux, A_train), axis=0) \n",
    "        \n",
    "        W_test_aux = np.concatenate((W_test_aux, W_test), axis=0)  \n",
    "        X_s_test_aux = np.concatenate((X_s_test_aux, X_s_test), axis=0)\n",
    "        X_v_test_aux = np.concatenate((X_v_test_aux, X_v_test), axis=0)\n",
    "        T_test_aux = np.concatenate((T_test_aux, T_test), axis=0)\n",
    "        Y_test_aux = np.concatenate((Y_test_aux, Y_test), axis=0) \n",
    "        A_test_aux = np.concatenate((A_test_aux, A_test), axis=0) \n",
    "        \n",
    "units_train=A_train_aux[:,0].reshape(-1,1)\n",
    "cycles_train=A_train_aux[:,1].reshape(-1,1)\n",
    "fc_train = A_train_aux[:,2].reshape(-1,1)\n",
    "hi_train = A_train_aux[:,-1]\n",
    "\n",
    "units_test=A_test_aux[:,0].reshape(-1,1)\n",
    "cycles_test=A_test_aux[:,1].reshape(-1,1)\n",
    "fc_test = A_test_aux[:,2].reshape(-1,1)\n",
    "hi_test = A_test_aux[:,-1]\n",
    "\n",
    "print(\"XS_train\",X_s_train_aux.shape)\n",
    "print(units_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxMin Scale $X_s$ and $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE\n",
    "\n",
    "# scaler_X = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_X = MinMaxScaler()\n",
    "X_s_train = scaler_X.fit_transform(X_s_train_aux)\n",
    "X_s_test = scaler_X.transform(X_s_test_aux)\n",
    "\n",
    "\n",
    "# scaler_W = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_W = MinMaxScaler()\n",
    "W_train = scaler_W.fit_transform(W_train_aux)\n",
    "W_test = scaler_W.transform(W_test_aux)\n",
    "\n",
    "scaler_Y = MinMaxScaler(feature_range=(0,1))\n",
    "Y_train = scaler_Y.fit_transform(Y_train_aux)\n",
    "Y_test = scaler_Y.transform(Y_test_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsamplig 0.1Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LEN = 50\n",
    "stride = 1\n",
    "\n",
    "X_windows, U_windows, C_windows=sequence_generator(X_s_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows,_,_=sequence_generator(W_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows,_,_=sequence_generator(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = stride)\n",
    "\n",
    "X_windows_test, U_windows_test,C_windows_test=sequence_generator(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows_test,_,_=sequence_generator(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows_test,_,_=sequence_generator(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = stride)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 5\n",
    "seed=229\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(X_windows.shape[0])\n",
    "training_idx, val_idx = shuffle_idx[X_windows.shape[0]//10:],shuffle_idx[:X_windows.shape[0]//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 11:45:02.479546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.486741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.487256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.488131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-19 11:45:02.489066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.489477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.489870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.768272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.768679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.769041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-19 11:45:02.769383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7719 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 11:45:06.837003: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "   1/9847 [..............................] - ETA: 6:05:45 - loss: 1.0134 - mae: 0.9627 - rmse: 1.0134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 11:45:07.809953: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9847/9847 [==============================] - 59s 6ms/step - loss: 0.1072 - mae: 0.0830 - rmse: 0.1072 - val_loss: 0.0972 - val_mae: 0.0712 - val_rmse: 0.0971\n",
      "Epoch 2/100\n",
      "9847/9847 [==============================] - 57s 6ms/step - loss: 0.0947 - mae: 0.0685 - rmse: 0.0947 - val_loss: 0.0914 - val_mae: 0.0666 - val_rmse: 0.0914\n",
      "Epoch 3/100\n",
      "9847/9847 [==============================] - 57s 6ms/step - loss: 0.0893 - mae: 0.0628 - rmse: 0.0893 - val_loss: 0.0827 - val_mae: 0.0568 - val_rmse: 0.0826\n",
      "Epoch 4/100\n",
      "9847/9847 [==============================] - 57s 6ms/step - loss: 0.0851 - mae: 0.0587 - rmse: 0.0851 - val_loss: 0.0799 - val_mae: 0.0547 - val_rmse: 0.0798\n",
      "Epoch 5/100\n",
      "9847/9847 [==============================] - 57s 6ms/step - loss: 0.0818 - mae: 0.0557 - rmse: 0.0818 - val_loss: 0.0838 - val_mae: 0.0551 - val_rmse: 0.0837\n",
      "Epoch 6/100\n",
      "9847/9847 [==============================] - 57s 6ms/step - loss: 0.0783 - mae: 0.0527 - rmse: 0.0783 - val_loss: 0.0776 - val_mae: 0.0531 - val_rmse: 0.0776\n",
      "Epoch 7/100\n",
      "9847/9847 [==============================] - 55s 6ms/step - loss: 0.0752 - mae: 0.0501 - rmse: 0.0752 - val_loss: 0.0767 - val_mae: 0.0500 - val_rmse: 0.0767\n",
      "Epoch 8/100\n",
      "1565/9847 [===>..........................] - ETA: 44s - loss: 0.0737 - mae: 0.0490 - rmse: 0.0737"
     ]
    }
   ],
   "source": [
    "LAYERS = [64,32]\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "x_temp = X_windows\n",
    "w_temp = W_windows\n",
    "y_temp = Y_windows\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "t = X_windows.shape[1]\n",
    "rul_model = predictor(t=t,useH=False,filter = LAYERS)\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LR,amsgrad=True)\n",
    "#LOSS = tf.keras.losses.MeanAbsoluteError()\n",
    "#LOSS = tf.keras.losses.RootMeanSquaredError()\n",
    "rul_model.compile(optimizer=OPTIMIZER, \n",
    "        loss= rmse, metrics=['mae',rmse])\n",
    "        \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=5,restore_best_weights=True)  \n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(((x_temp[training_idx],w_temp[training_idx]),y_temp[training_idx]))\n",
    "train_ds = train_ds.shuffle(buffer_size=x_temp.shape[0]//10).batch(BATCH_SIZE)     \n",
    "val_ds = tf.data.Dataset.from_tensor_slices(((x_temp[val_idx],w_temp[val_idx]),y_temp[val_idx]))    \n",
    "val_ds = val_ds.shuffle(buffer_size=x_temp.shape[0]//10).batch(BATCH_SIZE) \n",
    "history = rul_model.fit(train_ds,epochs=EPOCHS,validation_data=val_ds,verbose=1,callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"model.h5\")\n",
    "rul_model.save_weights(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.h5\")\n",
    "#model.save_weights(\"nodes8_main_model_t6.h5\")\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = rul_model.to_json()\n",
    "with open(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"model.h5\")\n",
    "\n",
    "json_file = open(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "rul_model = model_from_json(loaded_model_json)\n",
    "rul_model.load_weights(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted = rul_model.predict((X_windows_test,W_windows_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted_ = scaler_Y.inverse_transform(rul_predicted)\n",
    "groud_truth = scaler_Y.inverse_transform(Y_windows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math \n",
    "mae=mean_absolute_error(rul_predicted_,groud_truth)\n",
    "mse=mean_squared_error(rul_predicted_,groud_truth)\n",
    "rmse=np.sqrt(mse)\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_model = AE(14,4,[128,64,16],z_size=1)\n",
    "LOSS = tf.keras.losses.MeanAbsoluteError()\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "fcn_model.compile(optimizer=OPTIMIZER, \n",
    "          loss= LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(((X_s_train,W_train),X_s_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=10000).batch(248)\n",
    "history = fcn_model.fit(train_ds,epochs = 10,verbose=1)\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"model.h5\")\n",
    "fcn_model.save_weights(\"FCN_MODEL/model.h5\")\n",
    "#model.save_weights(\"nodes8_main_model_t6.h5\")\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = fcn_model.to_json()\n",
    "with open(\"FCN_MODEL/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"model.h5\")\n",
    "\n",
    "json_file = open('RUL_MODEL/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "fcn_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted = fcn_model.predict([X_s_test,W_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math \n",
    "mae=mean_absolute_error(rul_predicted,X_s_test)\n",
    "mse=mean_squared_error(rul_predicted,X_s_test)\n",
    "rmse=np.sqrt(mse)\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
