{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 19:05:28.024093: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 19:05:28.024134: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 19:05:28.024158: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 19:05:28.030437: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from sklearn.utils import shuffle\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# EVALUATION modules\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "params = {'legend.fontsize': 20,\n",
    "          'figure.figsize': (9,6),\n",
    "         'axes.labelsize': 20,\n",
    "         'axes.titlesize':20,\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'axes.linewidth' : 2,\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "\n",
    "plt.rcParams.update(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mape error\n",
    "def mape(real,predicted):\n",
    "    return np.mean(np.abs((real - predicted) / real)) * 100\n",
    "\n",
    "def mae(real,predicted):\n",
    "    return np.mean(np.abs(real - predicted))\n",
    "\n",
    "def rmse(real,predicted):\n",
    "    return np.sqrt(np.mean((real - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x, lb, ub, max_v=1.0, min_v=-1.0):\n",
    "\n",
    "    # Set-up\n",
    "    if (lb == []) & (ub == []):\n",
    "        # OPTION 1:\n",
    "        ub = x.max(0)\n",
    "        lb = x.min(0)\n",
    "    \n",
    "        # OPTION 2:\n",
    "        #ub = np.percentile(x, 99.9, axis=0, keepdims=True)\n",
    "        #lb = np.percentile(x, 0.1, axis=0, keepdims=True)\n",
    "    \n",
    "    ub.shape = (1,-1)\n",
    "    lb.shape = (1,-1)           \n",
    "    max_min = max_v - min_v\n",
    "    delta = ub-lb\n",
    "\n",
    "    # Compute\n",
    "    x_n = max_min * (x - lb) / delta + min_v\n",
    "    if 0 in delta:\n",
    "        idx = np.ravel(delta == 0)\n",
    "        x_n[:,idx] = x[:,idx] - lb[:, idx]\n",
    "\n",
    "    return x_n, lb, ub \n",
    "\n",
    "\n",
    "def split_dataset(dataset, split=4): \n",
    "    '''\n",
    "    Split 'dataset' in tree pieces: w, x_s, theta\n",
    "    '''\n",
    "    w = dataset[:,:split]\n",
    "    x_s = dataset[:,split:20]\n",
    "    theta = dataset[:,-10:]\n",
    "    tmp = theta[:,6]\n",
    "    tmp.shape = (-1,1)\n",
    "    \n",
    "    return w, x_s, tmp\n",
    "\n",
    "def extract_units_ds(id_en, ds, units):\n",
    "    '''\n",
    "    Creates a subset with only id_en units for ds\n",
    "    '''\n",
    "    \n",
    "    # Set-up\n",
    "    ds_sub = []\n",
    "    units_unique = np.unique(units)\n",
    "\n",
    "    # Process\n",
    "    for i in units_unique:\n",
    "        if i in id_en:\n",
    "            idx = np.ravel(units==i)\n",
    "            ds_sub.append(ds[idx,:])           \n",
    "    \n",
    "    return np.concatenate(ds_sub, axis=0)\n",
    "\n",
    "def create_lag_data(w, x_s, theta, y, Units, Cycles, stride=1):\n",
    "    # Set-up\n",
    "    W, X, T, Y, U, C, _W, _X = [], [], [], [], [], [], [], []\n",
    "    \n",
    "    # Loop over units and then within the units\n",
    "    units = np.unique(Units)\n",
    "    for k in units:\n",
    "        unit = np.ravel(Units == k)\n",
    "        w_unit = w[unit,:]\n",
    "        x_s_unit = x_s[unit,:]\n",
    "        T_unit = theta[unit,:]\n",
    "        Y_unit = y[unit,:]\n",
    "        U_unit = Units[unit,:]\n",
    "        C_unit = Cycles[unit,:]\n",
    "        dim = w_unit.shape[0]\n",
    "        for i in range(dim-1): \n",
    "            X.append(x_s_unit[i + stride, :])          # X  or X_\n",
    "            W.append(w_unit[i + stride, :])            # W  or W_\n",
    "            T.append(T_unit[i + stride, :])            # T  or T_\n",
    "            Y.append(Y_unit[i + stride, :])            # T  or T_\n",
    "            U.append(U_unit[i + stride, :])            # U  or U_\n",
    "            C.append(C_unit[i + stride, :])            # C  or C_\n",
    "            _X.append(x_s_unit[i, :])                  # _X or X\n",
    "            _W.append(w_unit[i, :])                  # _X or X\n",
    "            \n",
    "    return np.array(W), np.array(X), np.array(T), np.array(Y), np.array(U), np.array(C), np.array(_X), np.array(_W)\n",
    "\n",
    "\n",
    "\n",
    "# np.array(X_out), np.array(W_out), np.array(Y_out), np.array(T_out), np.array(U_out), np.array(C_out), np.array(HI_out)\n",
    "\n",
    "def split_sequences(input_data, sequence_length, stride = 1, option = None):\n",
    "    \"\"\"\n",
    "     \n",
    "    \"\"\"\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(0,len(input_data),stride):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + sequence_length\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(input_data):\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        if option=='last':\n",
    "            seq_x = input_data[end_ix-1, :]\n",
    "        elif option=='next':\n",
    "            seq_x = input_data[end_ix, :]\n",
    "        else:\n",
    "            seq_x = input_data[i:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "# def sequence_generator(input_data, units, sequence_length=10, option=None):\n",
    "#     \"\"\"\n",
    "#      # Generates dataset with windows of sequence_length      \n",
    "#     \"\"\"  \n",
    "#     X = list()\n",
    "#     unit_num=[]\n",
    "#     for i, elem_u in enumerate(list(np.unique(units))):\n",
    "#         mask = np.ravel(units==elem_u)\n",
    "#         seq_x_u = split_sequences(input_data[mask],sequence_length, option)\n",
    "#         X.append(seq_x_u)\n",
    "#         unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "    \n",
    "#     return np.vstack(X),unit_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensor_signal(x_real,x_pred,var_names,num=1000,figsize=10):\n",
    "    input_dim = len(var_names)\n",
    "    cols = min(np.floor(input_dim**0.5).astype(int),4)\n",
    "    rows = (np.ceil(input_dim / cols)).astype(int)\n",
    "    gs   = gridspec.GridSpec(rows, cols)\n",
    "    fig  = plt.figure(figsize=(figsize, max(figsize, rows*2)))   \n",
    "    for i in range(input_dim):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        # ax.plot(np.arange(0,num),x_real[-num:,i],label=\"True\",marker='.',markeredgewidth=0.25, markersize=8)\n",
    "        # ax.plot(np.arange(0,num),x_pred[-num:,i],label=\"Pred\",marker='.',markeredgewidth=0.25, markersize=8)\n",
    "        ax.scatter(np.arange(0,num),x_real[-num:,i],label=\"True\",marker='.')\n",
    "        ax.scatter(np.arange(0,num),x_pred[-num:,i],label=\"Pred\",marker='.')\n",
    "        plt.title(var_names[i])\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sequence Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generator(input_data, units, cycles, sequence_length=10,stride = 1, option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        c_mask = cycles[mask]\n",
    "        x_unit = input_data[mask]\n",
    "        for j in np.unique(c_mask):\n",
    "            mask = np.ravel(c_mask==j)\n",
    "            seq_x_u = split_sequences(x_unit[mask],sequence_length, stride, option)\n",
    "            X.append(seq_x_u)\n",
    "            unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "            c_num.extend(np.ones(len(seq_x_u),dtype = int)*j)\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.array(c_num).reshape(-1,1)\n",
    "\n",
    "\n",
    "def sequence_generator_per_unit(input_data, units, cycles, sequence_length=10, stride =1,option=None):\n",
    "    \"\"\"\n",
    "     # Generates dataset with windows of sequence_length      \n",
    "    \"\"\"  \n",
    "    X = list()\n",
    "    unit_num=[]\n",
    "    c_num =[]\n",
    "    for i, elem_u in enumerate(list(np.unique(units))):\n",
    "        mask = np.ravel(units==elem_u)\n",
    "        x_unit = input_data[mask]\n",
    "        seq_x_u = split_sequences(x_unit,sequence_length, stride, option)\n",
    "        X.append(seq_x_u)\n",
    "        unit_num.extend(np.ones(len(seq_x_u),dtype = int)*elem_u)\n",
    "        c_num.append(split_sequences(cycles[mask],sequence_length, stride, option))\n",
    "    \n",
    "    return np.vstack(X),np.array(unit_num).reshape(-1,1),np.vstack(c_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(xs,w,y,t,units,cycles,hi,freq,mean = False):\n",
    "    \n",
    "    X_out,W_out,Y_out,T_out,U_out,C_out,HI_out= [], [], [], [], [], [] , []\n",
    "    \n",
    "    # Number of Trajectories (Units)\n",
    "    uniq_units=np.unique(units)\n",
    "    \n",
    "    for k in uniq_units:\n",
    "        unit = np.ravel(units == k)\n",
    "        u_unit=units[unit,:]\n",
    "        w_unit = w[unit,:]\n",
    "        x_s_unit = xs[unit,:]\n",
    "        y_unit= y[unit,:]\n",
    "        t_unit= t[unit,:]\n",
    "        c_unit= cycles[unit,:]\n",
    "        h_unit = hi[unit]\n",
    "        \n",
    "        if mean:\n",
    "            for i in range(0,x_s_unit.shape[0],freq):\n",
    "                X_out.append(np.mean(x_s_unit[i:i+freq,:],axis=0))\n",
    "                W_out.append(np.mean(w_unit[i:i+freq,:],axis=0))\n",
    "                Y_out.append(np.mean(y_unit[i:i+freq,:],axis=0))\n",
    "                T_out.append(np.mean(t_unit[i:i+freq,:],axis=0))\n",
    "                HI_out.append(np.mean(h_unit[i:i+freq,:],axis=0))\n",
    "            \n",
    "            C_out.extend(c_unit[::freq])\n",
    "            U_out.extend(u_unit[::freq])\n",
    "        else:\n",
    "        \n",
    "            X_out.extend(x_s_unit[::freq,])\n",
    "            W_out.extend(w_unit[::freq,])\n",
    "            Y_out.extend(y_unit[::freq])\n",
    "            U_out.extend(u_unit[::freq])\n",
    "            T_out.extend(t_unit[::freq])\n",
    "            C_out.extend(c_unit[::freq])\n",
    "            HI_out.extend(h_unit[::freq])\n",
    "            \n",
    "    return  np.array(X_out).astype(np.float16),\\\n",
    "            np.array(W_out).astype(np.float16),\\\n",
    "            np.array(Y_out).astype(np.float16),\\\n",
    "            np.array(T_out).astype(np.float16),\\\n",
    "            np.array(U_out).astype(np.float16),\\\n",
    "            np.array(C_out).astype(np.float16),\\\n",
    "            np.array(HI_out).astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_healthy(data,units,cycles,cycle_len,sample_len=1):\n",
    "    X,U,C=[],[],[]\n",
    "    uniq_units=np.unique(units)\n",
    "    for i in uniq_units:\n",
    "        idx=np.ravel(units==i)\n",
    "        cy=cycles[idx]\n",
    "        x=data[idx,:]\n",
    "        u=units[idx]\n",
    "        cyidx=np.ravel(cy<=cycle_len)\n",
    "        x=x[cyidx,:]\n",
    "        u=u[cyidx]\n",
    "        cyy = cy[cyidx]\n",
    "        X.extend(x[::sample_len,:])\n",
    "        U.extend(u[::sample_len])\n",
    "        C.extend(cyy[::sample_len])\n",
    "        \n",
    "    return np.array(X), np.array(U), np.array(C)\n",
    "\n",
    "def sample_healthy_mixed(data,units,cycles,starts,cycle_len,full=False):\n",
    "    X,U,C=[],[],[]\n",
    "    uniq_units=np.unique(units)\n",
    "    for i in range(len(uniq_units)):\n",
    "        idx=np.ravel(units==uniq_units[i])\n",
    "        cy=cycles[idx]\n",
    "        x=data[idx,:]\n",
    "        u=units[idx]\n",
    "        if full:\n",
    "            cyidx=np.ravel((cy>=starts[i]))\n",
    "        else:\n",
    "            cyidx=np.ravel((cy>=starts[i])&(cy<=cycle_len+starts[i]))\n",
    "        x=x[cyidx,:]\n",
    "        u=u[cyidx]\n",
    "        cyy = cy[cyidx]\n",
    "        X.extend(x)\n",
    "        U.extend(u)\n",
    "        C.extend(cyy)\n",
    "        \n",
    "    return np.array(X), np.array(U), np.array(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_z_cycle(HI,T,units):\n",
    "    results = []\n",
    "    stds = []\n",
    "    cycle_num =[]\n",
    "    unit_num =[]\n",
    "    for j in np.unique(units):\n",
    "        idx = np.ravel(units==j)\n",
    "        HI_unit = HI[idx]\n",
    "        T_unit = T[idx].astype('float64')\n",
    "        for jj in np.unique(T_unit):\n",
    "            idxT = np.ravel(T_unit==jj)\n",
    "            HI_mu_cycle = np.mean(HI_unit[idxT])\n",
    "            std_cycle = np.std(HI_unit[idxT])\n",
    "            results.append(HI_mu_cycle)\n",
    "            stds.append(std_cycle)\n",
    "            cycle_num.append(jj)\n",
    "            unit_num.append(j)\n",
    "    return np.array(results), np.array(stds),np.array(cycle_num),np.array(unit_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUL MODEL\n",
    "\n",
    "def predictor(t=64,\n",
    "      feature_X_in=14,\n",
    "      feature_W_in=4,\n",
    "      feature_H_in=1,\n",
    "      feature_out_size=1,\n",
    "      activation='relu',\n",
    "      filter = [10,10,1],\n",
    "      filter_size = 10,\n",
    "      useH=True):\n",
    "    \n",
    "    '''\n",
    "    useH: if True, use H as input\n",
    "        [X,W,H] -> Y \n",
    "    else:\n",
    "        [X,W] -> Y\n",
    "    '''\n",
    "\n",
    "    x_in=layers.Input(shape=(t,feature_X_in),name=\"X_in\")\n",
    "    w_in = layers.Input(shape=(t,feature_W_in),name=\"W_in\")\n",
    "    \n",
    "    \n",
    "    if useH:\n",
    "      h_in = layers.Input(shape=(t,feature_H_in),name=\"H_in\")\n",
    "      # h_in = layers.Input(shape=(1,1),name=\"H_in\")\n",
    "      x = tf.concat([x_in,w_in, h_in],-1)\n",
    "    else: \n",
    "      x = tf.concat([x_in,w_in],-1)\n",
    "      \n",
    "    for i in filter:\n",
    "      x = layers.Conv1D(i,filter_size,1,padding='same',activation = activation)(x)\n",
    "      # x = layers.BatchNormalization()(x)\n",
    "      \n",
    "    x = layers.Flatten()(x)\n",
    "    y = layers.Dense(50,activation = activation)(x)\n",
    "    y = layers.Dense(feature_out_size,activation = 'linear')(y)\n",
    "\n",
    "    if useH:\n",
    "      model = models.Model([x_in,w_in,h_in], y)\n",
    "    else:\n",
    "      model = models.Model([x_in,w_in], y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AE MODEL FCN\n",
    "def AE(original_dimX,original_dimW,latent_dims,z_size = 1):\n",
    "    \n",
    "    inpX = layers.Input(shape=(original_dimX,))\n",
    "    inpW = layers.Input(shape=(original_dimW,))\n",
    "    x = tf.concat([inpX,inpW],axis = -1)\n",
    "        \n",
    "\n",
    "    for i in latent_dims:\n",
    "        x=tf.keras.layers.Dense(i,activation='relu')(x)\n",
    "        \n",
    "    z = tf.keras.layers.Dense(z_size,name=\"Z\")(x)\n",
    "     \n",
    "    x = tf.keras.layers.Concatenate()([z, inpW])\n",
    "            \n",
    "    for i in reversed(list(latent_dims)):\n",
    "        x=tf.keras.layers.Dense(i,activation='relu')(x) \n",
    "    x=tf.keras.layers.Dense(original_dimX,name='X')(x)   \n",
    "       \n",
    "    model = models.Model([inpX,inpW],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction\n",
    "\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flight Class FC\n",
    "FC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XS_train (1431066, 14)\n",
      "(1431066, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data DEV\n",
    "with h5py.File(\"FC\"+str(FC)+\"/FC\"+str(FC)+'_dev'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_train = np.array(hdf.get('W_dev'), dtype='float16')             # W\n",
    "            X_s_train = np.array(hdf.get('X_s_dev'), dtype='float16')         # X_s\n",
    "            X_v_train = np.array(hdf.get('X_v_dev'), dtype='float16')         # X_v\n",
    "            T_train = np.array(hdf.get('T_dev'), dtype='float16')             # T\n",
    "            Y_train = np.array(hdf.get('Y_dev'), dtype='float16')             # RUL  \n",
    "            A_train = np.array(hdf.get('A_dev'), dtype='float16')\n",
    "            \n",
    "            # Varnams\n",
    "            W_var = np.array(hdf.get('W_var'))\n",
    "            X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "            X_v_var = np.array(hdf.get('X_v_var')) \n",
    "            T_var = np.array(hdf.get('T_var'))\n",
    "            A_var = np.array(hdf.get('A_var'))\n",
    "            \n",
    "                # from np.array to list dtype U4/U5\n",
    "            W_var = list(np.array(W_var, dtype='U20'))\n",
    "            X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "            X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "            T_var = list(np.array(T_var, dtype='U20'))\n",
    "            A_var = list(np.array(A_var, dtype='U20'))\n",
    "            \n",
    "# Load data TEST\n",
    "mode = '_test'\n",
    "with h5py.File(\"FC\"+str(FC)+\"/FC\"+str(FC)+'_test'+\".h5\", 'r') as hdf:\n",
    "            # Development set\n",
    "            W_test = np.array(hdf.get('W_test'), dtype='float16')             # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'), dtype='float16')         # X_s\n",
    "            X_v_test = np.array(hdf.get('X_v_test'), dtype='float16')         # X_v\n",
    "            T_test = np.array(hdf.get('T_test'), dtype='float16')             # T\n",
    "            Y_test = np.array(hdf.get('Y_test'), dtype='float16')             # RUL  \n",
    "            A_test = np.array(hdf.get('A_test'), dtype='float16')\n",
    "\n",
    "units_train=A_train[:,0].reshape(-1,1)\n",
    "cycles_train=A_train[:,1].reshape(-1,1)\n",
    "fc_train = A_train[:,2].reshape(-1,1)\n",
    "hi_train = A_train[:,-1]\n",
    "\n",
    "units_test=A_test[:,0].reshape(-1,1)\n",
    "cycles_test=A_test[:,1].reshape(-1,1)\n",
    "fc_test = A_test[:,2].reshape(-1,1)\n",
    "hi_test = A_test[:,-1]\n",
    "\n",
    "print(\"XS_train\",X_s_train.shape)\n",
    "print(units_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxMin Scale $X_s$ and $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE\n",
    "\n",
    "# scaler_X = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_X = MinMaxScaler()\n",
    "X_s_train = scaler_X.fit_transform(X_s_train)\n",
    "X_s_test = scaler_X.transform(X_s_test)\n",
    "\n",
    "\n",
    "# scaler_W = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_W = MinMaxScaler()\n",
    "W_train = scaler_W.fit_transform(W_train)\n",
    "W_test = scaler_W.transform(W_test)\n",
    "\n",
    "scaler_Y = MinMaxScaler(feature_range=(0,1))\n",
    "Y_train = scaler_Y.fit_transform(Y_train)\n",
    "Y_test = scaler_Y.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsamplig 0.1Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LEN = 50\n",
    "stride = 1\n",
    "\n",
    "X_windows, U_windows, C_windows=sequence_generator(X_s_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows,_,_=sequence_generator(W_train,units_train,cycles_train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows,_,_=sequence_generator(Y_train,units_train,cycles_train,sequence_length=WINDOW_LEN,option='last',stride = stride)\n",
    "\n",
    "X_windows_test, U_windows_test,C_windows_test=sequence_generator(X_s_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows_test,_,_=sequence_generator(W_test,units_test,cycles_test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows_test,_,_=sequence_generator(Y_test,units_test,cycles_test,sequence_length=WINDOW_LEN,option='last',stride = stride)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 5\n",
    "seed=229\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(X_windows.shape[0])\n",
    "training_idx, val_idx = shuffle_idx[X_windows.shape[0]//10:],shuffle_idx[:X_windows.shape[0]//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 19:05:47.991445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.991703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.991931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.992160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.995732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.995987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.996210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.996432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.996655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.996873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.997091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-05 19:05:47.997308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Bad StatusOr access: INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 2: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 25438126080",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/zhaw-raul/fedLabSyncNCMAPSS/Deep Learning N-CMAPSS by Flight Class.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m K\u001b[39m.\u001b[39msqrt(K\u001b[39m.\u001b[39mmean(K\u001b[39m.\u001b[39msquare(y_pred \u001b[39m-\u001b[39m y_true))) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m t \u001b[39m=\u001b[39m X_windows\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m rul_model \u001b[39m=\u001b[39m predictor(t\u001b[39m=\u001b[39;49mt,useH\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\u001b[39mfilter\u001b[39;49m \u001b[39m=\u001b[39;49m LAYERS)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m OPTIMIZER \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mLR,amsgrad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#LOSS = tf.keras.losses.MeanAbsoluteError()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#LOSS = tf.keras.losses.RootMeanSquaredError()\u001b[39;00m\n",
      "\u001b[1;32m/home/zhaw-raul/fedLabSyncNCMAPSS/Deep Learning N-CMAPSS by Flight Class.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m   x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([x_in,w_in, h_in],\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m   x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconcat([x_in,w_in],\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bthanos.mlwin.ch/home/zhaw-raul/fedLabSyncNCMAPSS/Deep%20Learning%20N-CMAPSS%20by%20Flight%20Class.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m   x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv1D(i,filter_size,\u001b[39m1\u001b[39m,padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m,activation \u001b[39m=\u001b[39m activation)(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/fedLabSync/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/fedLabSync/lib/python3.10/site-packages/tensorflow/python/eager/context.py:598\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    595\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextOptionsSetRunEagerOpAsFunction(opts, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    596\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextOptionsSetJitCompileRewrite(\n\u001b[1;32m    597\u001b[0m       opts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile_rewrite)\n\u001b[0;32m--> 598\u001b[0m   context_handle \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_NewContext(opts)\n\u001b[1;32m    599\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_DeleteContextOptions(opts)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 2: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 25438126080"
     ]
    }
   ],
   "source": [
    "LAYERS = [64,32]\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "x_temp = X_windows\n",
    "w_temp = W_windows\n",
    "y_temp = Y_windows\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "t = X_windows.shape[1]\n",
    "rul_model = predictor(t=t,useH=False,filter = LAYERS)\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LR,amsgrad=True)\n",
    "#LOSS = tf.keras.losses.MeanAbsoluteError()\n",
    "#LOSS = tf.keras.losses.RootMeanSquaredError()\n",
    "rul_model.compile(optimizer=OPTIMIZER, \n",
    "        loss= rmse, metrics=['mae',rmse])\n",
    "        \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=5,restore_best_weights=True)  \n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(((x_temp[training_idx],w_temp[training_idx]),y_temp[training_idx]))\n",
    "train_ds = train_ds.shuffle(buffer_size=x_temp.shape[0]//10).batch(BATCH_SIZE)     \n",
    "val_ds = tf.data.Dataset.from_tensor_slices(((x_temp[val_idx],w_temp[val_idx]),y_temp[val_idx]))    \n",
    "val_ds = val_ds.shuffle(buffer_size=x_temp.shape[0]//10).batch(BATCH_SIZE) \n",
    "history = rul_model.fit(train_ds,epochs=EPOCHS,validation_data=val_ds,verbose=1,callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #model.save_weights(\"model.h5\")\n",
    "rul_model.save_weights(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.h5\")\n",
    "#model.save_weights(\"nodes8_main_model_t6.h5\")\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = rul_model.to_json()\n",
    "with open(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"model.h5\")\n",
    "\"\"\" \n",
    "json_file = open(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "rul_model = model_from_json(loaded_model_json)\n",
    "rul_model.load_weights(\"RUL_MODEL/FC\"+str(FC)+\"/\"+\"/model.h5\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted = rul_model.predict((X_windows_test,W_windows_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted_ = scaler_Y.inverse_transform(rul_predicted)\n",
    "groud_truth = scaler_Y.inverse_transform(Y_windows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math \n",
    "mae=mean_absolute_error(rul_predicted_,groud_truth)\n",
    "mse=mean_squared_error(rul_predicted_,groud_truth)\n",
    "rmse=np.sqrt(mse)\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" fcn_model = AE(14,4,[128,64,16],z_size=1)\n",
    "LOSS = tf.keras.losses.MeanAbsoluteError()\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "fcn_model.compile(optimizer=OPTIMIZER, \n",
    "          loss= LOSS) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train_ds = tf.data.Dataset.from_tensor_slices(((X_s_train,W_train),X_s_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=10000).batch(248)\n",
    "history = fcn_model.fit(train_ds,epochs = 10,verbose=1)\n",
    "plt.plot(history.history['loss']) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #model.save_weights(\"model.h5\")\n",
    "fcn_model.save_weights(\"FCN_MODEL/model.h5\")\n",
    "#model.save_weights(\"nodes8_main_model_t6.h5\")\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = fcn_model.to_json()\n",
    "with open(\"FCN_MODEL/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"model.h5\")\n",
    "\n",
    "\"\"\" json_file = open('RUL_MODEL/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "fcn_model = model_from_json(loaded_model_json) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" rul_predicted = fcn_model.predict([X_s_test,W_test]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math \n",
    "mae=mean_absolute_error(rul_predicted,X_s_test)\n",
    "mse=mean_squared_error(rul_predicted,X_s_test)\n",
    "rmse=np.sqrt(mse)\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
